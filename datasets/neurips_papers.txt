cogltx applying bert to long texts 	CogLTX: Applying BERT to Long Texts Ming Ding Tsinghua University Zhou Alibaba Group Hongxia Yang Alibaba Group Tang Tsinghua University Abstract BERT is incapable of processing long texts due to its quadratically increasing memory and time consumption. The most natural ways to address this problem, such as slicing the text by a sliding window or simplifying transformers, suffer from insufﬁcient long-range attentions or need customized CUDA kernels. The maximum length limit in BERT reminds us the limited capacity (5 ∼9 chunks) of theworking memory of humans –— then how do human beings Cognize Long TeXts? Founded on the cognitive theory stemming from Baddeley [ 2], the pro- posed identiﬁes key sentences by training a judge model, concatenates them for reasoning, and enables multi-step reasoning via rehearsal anddecay . Since relevance annotations are usually unavailable, we propose to use interventions to create supervision. As a general algorithm, CogLTX outperforms or gets comparable results to SOTA models on various downstream tasks with memory overheads independent of the length of text. 1 Introduction Figure 1: An example from HotpotQA (dis- tractor setting, concatenated). The key sen- tences to answer the question are the ﬁrst and last ones, more than 512 tokens away from each other. They never appear in the same BERT input window in the sliding window method, hence we fail to answer the language models, pioneered by BERT [ 12], have emerged as silver bullets for many NLP tasks, such as question answering [ 38] and text classiﬁca- tion [ 22]. Researchers and engineers breezily build applications following the standard ﬁnetuning paradigm, while might end up in disap- pointment to ﬁnd some texts longer than the length limit of BERT (usually 512 tokens). This situation may be rare for normalized benchmarks, for example SQuAD [ 38] and GLUE [ 47], but very common for more complex tasks [53] or real-world textual data. A solution for long texts is sliding window [50], processing continuous 512-token spans by BERT. This method sacriﬁces the possibility that the distant tokens “pay attention” to each other, which becomes the bottleneck for BERT to show its efﬁcacy in complex tasks (for example Figure 1). Since the problem roots in the high O(L2)time and space com- plexity in transformers [ 46] (Lis the length of the 1Codes are available at . 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. BERT (reasoner)[CLS] Q yes no [SEP] zStart/End Span Q: Who is the director of the 2003 film which has scenes in it filmed at the Quality Cafe in Los Angeles? Long text x: The Quality Cafe (aka. Quality Diner) is a now-defunct diner … as a location featured in a num-ber of Hollywood films, including “Training Day”, “Old School”… Old School is a 2003 American comedy film released by DreamWorks and directed by Todd ], x) BERT (reasoner)[CLS] [SEP] zMLP Long text x: LOS ANGELES -- The pilot flying Kobe Bryant and seven others to a youth basketball tour-nament did not have alcohol or drugs in his system, and all nine sustained immediately fatal injuries when their heli-copter slammed into a hillside outside Los Angeles in January, according to autopsies released Friday. …MemRecall([], x) 0.6 0.3 …Probabilty for each class BERT (reasoner)[CLS] x[i] [SEP] zLong text x MemRecall([x[i]], x) NN IN DTToken-wise result of x[i] … in the pound is widely expected to take another sharp dive if trade figures for Sep-tember, due for release tomorrow, x[1]:fail to show a substantial improvement from July and August's near-record deficits” is into sub-sequences x[0]… x[n]z}|{ <latexit the results of all x[i] (a) Span Extraction Tasks e.g. question answering(b) Sequence-level Tasks e.g. text ) Token-wise Tasks e.g. POS of relevant sentences to train “ 2: The CogLTX inference for main genres of BERT tasks. MemRecall is the process to extract key text blocks zfrom the long text x. Then zis sent to the BERT, termed reasoner , to fulﬁll the speciﬁc task. A (c) task is converted to multiple (b) tasks. The BERT input w.r.t. zis denoted by z+. text), another line of research attempts to simplify the structure of transformers [ 20,37,8,42], but currently few of them have been successfully applied to BERT [35, 4]. The maximum length limit in BERT naturally reminds us the limited capacity of Working Memory [2], a human cognitive system storing information for logical reasoning and . Experi- ments [ 27,9,31] already showed that the working memory could only hold 5 ∼9 items/words during reading, so how do humans actually understand long texts? “The central executive – the core of the (working memory) system that is responsible for coordinating (multi-modal) information”, and “functions like a attentional system capable of selecting and operating control processes and strategies”, as Baddeley [2]pointed out in his 1992 classic. Later research detailed that the contents in the working memory decay over time [ 5], unless are kept via rehearsal [3], i.e. paying attention to and refreshing the information in the mind. Then the overlooked information is constantly updated with relevant items from long-term memory by retrieval competition [52], collecting sufﬁcient information for reasoning in the working memory. The analogy between BERT and working memory inspires us with the CogLTX framework to Cognize LongTeXts like human. The basic philosophy behind CogLTX is rather concise — reasoning over the concatenation of key sentences (Figure 2) — while compact designs are demanded to bridge the gap between the reasoning processes of machine and human. The critical step in CogLTX is MemRecall, the process to identify relevant text blocks by treating the blocks as episodic memories . MemRecall imitates the working memory on retrieval competition, rehearsal and decay, facilitating multi-step reasoning. Another BERT, termed judge , is introduced to score the relevance of blocks and trained jointly with the original BERT reasoner . Moreover, CogLTX can transform task-oriented labels to relevance annotations by interventions to train judge . Our experiments demonstrate that CogLTX outperforms or achieves comparable performance with the results on four tasks, including NewsQA [ 44], HotpotQA [ 53], 20NewsGroups [ 22] and Alibaba, with constant memory consumption regardless of the length of text. 2 Background Challenge of long texts. The direct and superﬁcial obstacle for long texts is that the pretrained max position embedding is usually 512 in BERT [ 12]. However, even if the embeddings for larger positions are provided, the memory consumption is unaffordable because all the activations are stored for during training. For instance, a 1,500-token text needs about 14.6GB memory to run BERT-large even with batch size of 1, exceeding the capacity of common GPUs (e.g. 11GB for RTX 2080ti). Moreover, the O(L2)space complexity implies a fast increase with the text length L. Related works. As mentioned in Figure 1, the sliding window method suffers from the lack of long-distance attention. Previous works [ 49,33] tried to aggregate results from each window by mean-pooling, max-pooling, or an additional MLP or LSTM over them; but these methods are still weak at long-distance interaction and need O(5122·L/512) =O(512L)space, which in practice is still too large to train a BERT-large on a 2,500-token text on RTX 2080ti with batch size of 1. Besides, these methods mainly optimizes classiﬁcation, while other tasks, e.g., span extraction , haveLBERT outputs, need O( L2) space for self-attention aggregation. 2 Q x0 x2 x8 x13 x14 x25 x31Q: Who is the director of the 2003 film which has scenes in it filmed at the Quality Cafe in Los Angeles? x0: Quality Cafe is the name of two different former loca-tions in Downtown Los Angeles, California. …x8: "The Quality Cafe (aka. Quality Diner) is a now-defunct diner …but has appeared as a location featured in a number of Hollywood films, including "Training Day", "Old School"ŏx40: Old School is a 2003 American comedy film released by DreamWorks Pictures … and directed by Todd Phillips. ZGet scores by respectivelyŏ BERT 1.0 0.84 0.71 0.91 0.64 0.78 0.32 0.48Select highest Scoring blocks Q x0 x2 x8 x13 x14 x25 x31 Q x0 x8 Next-step Reasoningnew zŏŏŏJudge 12 3Select highest scoring blocks45Forget other (initial z = [Q], long text x = [x0 … x40])Q+++ Retrieval 3: The MemRecall illustration for question answering. The long text xis broken into blocks [x0...x40]. In the ﬁrst step, x0andx8are kept in zafter rehearsal. The “Old School” in x8will contribute to retrieve the answer block x40in the next step. See Appendix for details. In the line of researches to adapt transformers for long texts, many of them just compress or reuse the results of former steps and cannot be applied to BERT, e.g., Transformer-XL [ 8] and Compressive Transformer [ 37]. Reformer uses hashing for content-based group attention, but it is not friendly to GPU and still needs veriﬁcation for BERT usage. BlockBERT [ 35] cuts off unimportant attention heads to scale up BERT from 512 token to 1,024. The recent milestone longformer [ 4], customizes CUDA kernels to support window attention and global attention on special tokens. However, the efﬁcacy of the latter are insufﬁciently investigated because the datasets are mostly in 4×the window size of longformer. The direction of “lightweight BERTs” is promising butorthogonal to CogLTX, meaning that they can combine CogLTX to handle longer texts, so they will not be compared or discussed anymore in this paper. A detailed survey can be found in [25]. 3 Method 3.1 The CogLTX methodology This basic assumption of CogLTX is that “for most NLP tasks, a few key sentences in the text store sufﬁcient andnecessary information to fulﬁll the task”. More speciﬁcally, we assume there exists a short text zcomposed by some sentences from the long text x, satisfying reasoner (x+)≈reasoner (z+), (1) where x+andz+are inputs for the reasoner BERT w.r.t. the texts xandzas illustrated in Figure 2. We split each long text xinto blocks [x0...xT−1]by dynamic programming (see the Appendix), which restricts the block length to a maximum of B, in our implementation B= 63 if the BERT length limitL= 512 . The key short text zshould be composed by some blocks in x, i.e.z= [xz0...xzn−1], satisfying n−1. We denote xzibyzi. All blocks in zare automatically sorted to maintain the original relative ordering in x. The key-blocks assumption is strongly related to latent variable models , which are usually solved by EM [ 11] or variational bayes [ 19]. However, these methods estimate the distribution of zand require , thus not efﬁcient enough for BERTs. We take the essence of them into the design of CogLTX, and discuss the connections in § 3.3. Two ingredients are essential in CogLTX, MemRecall and the joint training of two BERTs. As demonstrated in Figure 2, MemRecall is the algorithm utilizing the judge model to retrieve key blocks, which are fed into the reasoner to accomplish the task during inference. 3 3.2 MemRecall As the brain recalls past episodes relevant to current information in working memory, the MemRecall aims to extract key blocks zfrom the long text x(see Figure 3). Input. Although the aim is to extract key blocks, speciﬁc settings differ in the three types of tasks. In Figure 2 (a) (c), the question Qor sub-sequence x[i]serves as query to retrieve relevant blocks. However, queries are absent in (b), and the relevance is only implicitly deﬁned by the training data. For instance, sentences containing “Donald Trump” or “basketball” are more relevant for news topic classiﬁcation than time reporting sentences. So how to seamlessly unify the cases? MemRecall answers by accepting an initial z+as an additional input besides x.z+is the short “key text” maintained during MemRecall to simulate working memory. The query in tasks (a)(c) becomes the initial information in z+to provoke recalling. Then a judge model learns to predict task-speciﬁc relevance with the help of z+. Model. The only model used by MemRecall is the judge mentioned above, a BERT to score the relevance for each token. Suppose z+= [[CLS]Q[SEP] z0[SEP]...zn−1], judge (z+) =sigmoid (MLP(BERT (+). (2) The score of a block zi, denoted as judge (z+)[zi], is the average of the scores of tokens in the block. Procedure. MemRecall begins with a retrieval competition . Each block xiis assigned a coarse relevance score judge ([z+[SEP] xi])[xi]. The “winner” blocks with the highest scores are inserted intozas much as len(z+)≤L. The superiority over vector space models [40] lies in that xifully interacts with current z+via transformers, avoiding information loss during embedding. The following period assigns each score judge (z+)[zi]. Only the highest scored blocks are then kept in z+, just like the phenomenon in working memory. The motivation of ﬁne scores is that the relative sizes of coarse scores are not accurate enough without interaction and comparison between blocks, similar to the motivation of reranking [ 7]. MemRecall in nature enables multi-step reasoning by repeating the procedure with new z+. The importance of iterative retrieval is highlighted by CogQA [ 13], as the answer sentence fails to be directly retrieved by the question in multi-hop reading comprehension. It is worth noting that blocks reserved from last step can also decay , if they are proved not relevant enough (with low scores) by more information from new blocks in z+, which is neglected by previous multi-step reasoning methods [13, 1, 10]. 3.3 Training The diversity of downstreaming tasks pose challenges for training (ﬁnetuning) the BERTs in CogLTX. The solutions under different settings are summarized in Algorithm 1. Supervised training for judge .The span extraction tasks (Figure 2(a)) in nature suggest the answer block as relevant . Even multi-hop datasets, e.g. HotpotQA [ 53], usually annotate supporting sentences. In these cases, the judge is naturally trained in a supervised way: lossjudge(z) =CrossEntropy( judge (z+),relv_label (z+)) , (3) relv_label (z+) = [1,1, ..., 1 for query,0,0, ..., 0 , ..., 1 z1isrelevant, ... ]∈[0,1]len(z+), (4) where the training sample zis either a sequence of continous blocks zrand sampled from x(corre- sponding to the data distribution of retrieval competition), or a mixture of all relevant and randomly selected irrelevant blocks the data distribution of rehearsal). Supervised training for reasoner .The challenge for reasoner is to keep the consistency of data distributions during training and inference, which is a cardinal principle of supervised learning. Ideally, the inputs of reasoner should also be generated by MemRecall during training, but not all relevant blocks are guaranteed to be retrieved. For instance in question answering, if the answer block 4 Algorithm 1: The Training Algorithm of CogLTX Input: Traing setD= [()], : num_epoch, mode, tup,tdown . 1ifmode is unsupervised then 2Initialize the relevance labels in DbyBm25 orGlove if possible. // see Appendix for details. 3forepoch from 1 to num_epoch do 4forx,yinDdo 5 Extract a short ( len≤L) span from xat random as zrand. 6 (judge(z+ (z+ rand)). 7 Aggregate all relevant blocks and some randomly chosen irrelevant blocks as zrelv(len≤L). 8 (judge(z+ (z+ relv)). 9 Update judge by ). //φis the parameters of judge . 10 forx,yinDdo 11 Aggregate all relevant blocks in xasz. 12 forirrelevant block xiinxdo 13 ([z xi]+)[xi] // can be replaced by cached scores during training judge . 14 Fillzup to lengthLwith highest scoring blocks. // corresponding to the zfrom MemRecall. 15 (reasoner (z+),y). 16 Update reasoner by . //θis the parameters of reasoner . 17 ifmode is unsupervised and epoch >1then 18 forblock ziinzdo 19 (reasoner (z+ −zi),y).// gradient-free, much faster than Line 15. 20 Label ziasrelevant ; 21 then Label ziasirrelevant ; is missed by MemRecall, the training cannot proceed. Finally, an approximation is made to send all relevant blocks and the “winner” blocks in the retrieval competition to train the reasoner . Unsupervised training for judge .Unfortunately, many tasks (Figure 2 (b)(c)) do not provide relevance labels. Since CogLTX assumes all relevant blocks necessary , we infer the relevance labels by interventions: test whether a block is indispensable by removing it from z. Suppose that zis the “oracle relevant blocks”, according to our assumption, lossreasoner ( () (5) lossreasoner ([z () (6) where z−ziis the result of removing zifrom z, andtis a threshold. After every iteration in training reasoner , we ablate each block in z, adjust its relevance label according to the increase of loss. Insigniﬁcant increase reveals the block as irrelevant, which will probably not “win the retrieval competition” again to train the reasoner in the next epoch, because it will be labeled as irrelevant to train the judge in the next epoch. Then real relevant blocks might enter znext epoch and be detected. In practice, we split , leaving a buffer zone to prevent frequent changes of labels. We exhibit an example of unsupervised training on the 20News text classiﬁcation dataset in Figure 4. Connections to latent variable models. Unsupervised CogLTX can be viewed as a generalization of (conditional) latent variable model ). EM [ 11] infers the distribution ofzas posterior p(z|y,x;θ)in E-step, while variational bayes methods [ 19,39] use an estimation- ). However, in CogLTX zhas a discrete distribution with up to Cm npossible values, wheren,m are the number of blocks and the capacity of zrespectively. In some cases, sampling for hundreds of times to train BERTs might be required [ 19], whose expensive time consumption force us turn to point estimation forz,2e.g. our method. The intervention solution, maintains an zestimation for each x, and is essentially a local search speciﬁc to CogLTX. zis optimized by comparing nearby values(results after replacing irrelevant blocks) rather than Bayesian rules. The judge ﬁts an inductive discriminative model to help infer z. 2analogous to K-means, which can be seen as EM for Gaussian mixture model with inﬁnitesimal variances. Then the posterior of z, mixture belonging, degenerates into the nearest cluster (the deterministic MLE value). 5 Harrassed at work, could use some prayers =CSE Dept., U.C. San…Yesterday I counted and realized that on seven diﬀerent occasions…If he/she does not seem to take any action, keep going up higher ..If you feel you can not discuss this with your boss, perhaps your …It is unclear from your letter if you have done this or not. It is not …If the company indeed does seem to want to ignore the entire…People in oﬃces tend to be more insensitive while working than … They are doing it because they are still the playground bully …In MY day, we had to make do with 5 bytes of swap... Then they will come back and wonder why I didn't want to go …No one could be bothered to call me at the other building, even …Moderator allows me this latest indulgence. Well, if you can't turn … ground truth label: is, someone that is supportive, comforting, etc. … healing… scoring blocks by judgeMarked as as relevantFigure 4: An example about unsupervised training of CogLTX on 20News dataset. All blocks are initialized as “irrelevant” by BM25 (no common words with the label ). In the ﬁrst epoch, the judge is nearly untrained and selects some blocks at random. Among them, (7) contributes most to the correct classiﬁcation, thus is marked “relevant”. In the second epoch, trained judge ﬁnds (1) with strong evidence “prayers” and (1) is marked as “relevant” at once. Then in the next epoch, (7) becomes not essential for classiﬁcation and is marked as “irrelevant”. 4 Experiments Figure 5: The boxplot of the text length dis- tribution in the datasets.We conducted experiments on four long-text datasets with different tasks. The token-wise (Figure 2 (c)) tasks are not included because they mostly barely need information from adjacent sentences, and are ﬁnally transformed into multiple sequence-level sam- ples. The boxplot in Figure 5 illustrates the statistics of the text length in the datasets. In all experiments, the judge andreasoner are ﬁne- tuned by Adam [ 18] with learning rate 4×10−5 . The learning rates warmup over the ﬁrst 10% steps, and then linearly decay to 1/10of the max learning rates. The common hyper- parameters are batch size = 32 ,strides = [3,5],tup= 0.2andtdown =−0.05. In this section, we separately introduce each task with related results, analysis and ablation studies. 4.1 Reading comprehension Dataset and settings. Given a question and a paragraph, the task is to predict the answer span in the paragraph. We evaluate the performance of CogLTX on NewsQA [ 44], which contains 119,633 questions posed on 12,744 long news previous SOTA [ 43] is not BERT based (due to long texts) in NewsQA, to keep the similar scale of parameters for fair comparison, we ﬁnetune the base version of RoBERTa [26] for 4 epochs in CogLTX. Table 1: NewsQA results (%). Model EM F1 Match-LSTM [48] 34.9 50.0 BiDAF [41] 37.1 52.3 FastQAExt [51] 42.8 56.1 AMANDA [21] 48.4 63.7 MINIMAL [28] 50.1 63.2 DECAPROP [43] 53.1 66.3 RoBERTa-large [26] (sliding window) 49.6 66.3 CogLTX 55.2 70.1Results. Table 1 show that CogLTX-base outperforms QA mod- els, for example BiDAF [ 41] (+17.8% F1), previous SOTA DECAPROP [ 43], which incorporates elaborate self-attention and RNN mechanisms (+4.8% F1), and even RoBERTa-large with sliding window (+4.8%F1). We hypothesize that the ﬁrst sentence (the lead) and the last sentence (the conclusion) are usually the most infor- mative parts in news articles. CogLTX can aggregate them for reasoning while sliding window cannot. 3We use the original version instead of the simpliﬁed version in MRQA [15], which removed long texts. 6 4.2 Multi-hop question answering Dataset and settings. In complex scenarios, the answer is based on multiple paragraphs. Previous methods usually leverage the graph structure between key entities across the paragraphs [ 13,36]. However, if we can handle long texts with CogLTX, the problem can be elegantly solved by concate- nating all the paragraphs as the input of BERTs. HotpotQA [ 53] is a multi-hop QA dataset of 112,779 questions, whose distractor setting provides 2 necessary paragraphs and 8 distractor paragraphs for each question. Both answers and supporting facts are required for evaluation. We treat each sentence as a block in CogLTX, and directly output the 2 blocks with the highest ﬁne scores as supporting facts. Table 2: Results on HotpotQA distractor (dev). (+hyperlink) means usage of extra hyperlink data in Wikipedia. Models beginning with “ −” are ablation studies without the corresponding design. Model Ans EM Ans F1Sup EM Sup F1Joint EM Joint F1 Baseline [53] 45.60 59.02 20.32 64.49 10.83 40.16 DecompRC [29] 55.20 69.63 N/A N/A N/A N/A QFE [30] 53.86 68.06 57.75 84.49 34.63 59.61 DFGN [36] 56.31 69.69 51.50 81.62 33.62 59.82 SAE [45] 60.36 73.58 56.93 84.63 38.81 64.96 SAE-large 66.92 79.62 61.53 86.86 45.36 71.45 HGN [14] (+hyperlink) 66.07 79.36 60.33 87.33 43.57 71.03 HGN-large (+hyperlink) 69.22 82.19 62.76 88.47 47.11 74.21 BERT (sliding window) variants BERT Plus 55.84 69.76 42.88 80.74 27.13 58.23 LQR-net + BERT 57.20 70.66 50.20 82.42 31.18 59.99 GRN + BERT 55.12 68.98 52.55 84.06 32.88 60.31 EPS + BERT 60.13 73.31 52.55 83.20 35.40 63.41 LQR-net 2 + BERT 60.20 73.78 56.21 84.09 36.56 63.68 P-BERT 61.18 74.16 51.38 82.76 35.42 63.79 EPS + BERT(large) 63.29 76.36 58.25 85.60 41.39 67.92 CogLTX 65.09 78.72 56.15 85.78 39.12 69.21 −multi-step reasoning 62.00 75.39 51.74 83.10 35.85 65.35 −rehearsal & decay 61.44 74.99 7.74 47.37 5.36 37.74 −train-test matching 63.20 77.21 52.57 84.21 36.11 66.90 Results. Table 2 shows that CogLTX outperforms most of previous methods and all 7BERT variants solutions on the solutions basically follow the framework of aggregating the results from sliding windows by extra neural networks, leading to bounded performances attributed to insufﬁcient interaction across paragraphs. The SOTA model HGN [ 14] leverages extra hyperlink data in Wikipedia, based on which the dataset is constructed. The thought of SAE [ 45] is similar to CogLTX but less general. It scores paragraphs by an attention layer over BERTs, selects the highest scoring 2 paragraphs and feeds them into BERT together. The supporting facts are determined by another elaborate graph attention model. With the well-directed designs, SAE ﬁts HotpotQA better than CogLTX (2.2% Joint F1), but does not solve the memory problem for longer paragraphs. CogLTX directly solves the multi-hop QA problem as ordinary QA, gets results and explains the supporting facts without extra efforts. Ablation studies. We also summarize the ablation studies in Table 2, indicating that (1)multi- step reasoning does work (+3.9% Joint F1) but not essential, probably because many questions themselves in HotpotQA are relevant enough with the second-hop sentences to retrieve them. (2) The metrics on supporting facts drop dramatically (-35.7% Sup F1) without rehearsal for ﬁne scores, because the relevance scores of top sentences are not comparable without attending to each other. 7 (3)As mentioned in § 3.3, the discrepancy of data distribution during training and test impairs the performance (-2.3% Joint F1) ifreasoner is trained by randomly selected blocks. 4.3 Text classiﬁcation Dataset and settings. As one of the most general tasks in NLP, text classiﬁcation is essential to analyze the topic, sentiment, intent, etc. We conduct experiments on the classic 20NewsGroups [ 22], which contains 18,846 documents from 20 classes. We ﬁnetune RoBERTa for 6 epochs in CogLTX. Table 3: 20NewsGroups results (%). Model Accuracy BoW + SVM 63.0 Bi-LSTM 73.2 fastText [16] 79.4 MS-CNN [32] 86.1 Text GCN [54] 86.3 MLP over BERT [33] 85.5 LSTM over BERT [33] 84.7 CogLTX (Glove init) 87.0 only long texts 87.4 −intervention (Glove init) 84.8 Bm25 init 86.1Results. Table 3 demonstrates that CogLTX, whose relevance labels are initialized by Glove [ 34], outper- forms the other baselines, including previous attempts to aggregate the [CLS] pooling results from the slid- ing window [ 33]. Moreover, MLP or LSTM based aggregation cannot be trained end-to-end either on long texts. Ablation studies. (1) Since the text lengths in 20 NewsGroups vary greatly (see Figure 5), we further test the performance only on the texts longer than 512 tokens (15%), which is even above the global result. (2)The initialization based on Glove provides good relevance labels, but the lack of adjustments by inter- ventions still leads to 2.2% decrease in accuracy. (3) The Bm25 initialization is based on common words, which only initializes 14.2%training samples due to the short label names, e.g., . The relevant sentences are inferred by interventions and the gradually trained reasoner , achieving an accuracy of 86.1%. 4.4 Multi-label classiﬁcation Dataset and settings. In many practical problems, each text can belong to multiple classes at the same time. The multi-label classiﬁcation is usually transformed into binary classiﬁcation by training an individual classiﬁer for each label. Owing to the large capacity of BERT, we share the model for all the labels by prepending the label name at the beginning of the documents as input, i.e., [[CLS] label[SEP] doc], for binary classiﬁcation. Alibaba is a dataset of 30,000 articles extracted from an industry scenario in a large e-commerce platform. Each article advertises for several items from 67 categories. The detection of mentioned categories are perfectly modeled as multi-label classiﬁcation. To accelerate the experiment, we respectively sampled 80,000 and 20,000 label-article pairs for training and testing. For this task, we ﬁnetune RoBERTa for 10 epochs in CogLTX. Table 4: Alibaba result (%). Model Accuracy Micro- F1Macro-F1 BoW+SVM 89.9 85.8 55.3 Bi-LSTM 70.7 62.1 48.2 TextCNN 95.3 94.1 91.3 sliding window 94.5 92.7 89.9 CogLTX(tiny) 95.5 94.4 92.4 CogLTX(large) 98.2 97.8 97.2Results. Table 4 shows that CogLTX outper- forms common strong baselines. The word em- beddings used by TextCNN [ 17] and Bi-LSTM are from RoBERTa for fair comparison. Even CogLTX-tiny (7.5M parameters) outperforms TextCNN. However, the max-pooling results of RoBERTa-large sliding window are worse than CogLTX (7.3% Macro- F1). We hypothesize this is due to the tendency to assign higher probabili- ties to very long texts in max-pooling, highlighting the efﬁcacy of CogLTX. 4.5 Memory and time consumption Memory. The memory consumption of CogLTX is constant during training, superior to the O(L2) complexity of vanilla BERT. We also compare longformer [ 4], whose space complexity is roughly O(L)if the number of global attention tokens is small relative to Land independent of L. The detailed comparison is summarized in Figure 6 (Left). 8 Time. To accelerate the training of reasoner , we can cache the scores of blocks during training judge and then each epoch only needs 2×time of single-BERT training. As the numbers of epochs until convergence are similar for CogLTX and sliding window in training, the main concern of CogLTX is the speed of inference. Figure 6 (Right) shows the time to process a 100,000-sample synthetic dataset with different text lengths. CogLTX, with O(n)time complexity, is faster than vanilla BERT after L> 2,048 and approaches the speed of sliding window as the text length Lgrows. Figure 6: Memory and Time consumption with varying text length. The data about memory are measured with batch size = 1on a Tesla V100. The batch size is 8 in the measurement of inference time consumption, and CogLTX does 1-step reasoning. 5 Conclusion and discussion We present CogLTX, a cognition inspired framework to apply BERT to long texts. CogLTX only needs ﬁxed memory during training and enables attentions between faraway sentences. Similar ideas were investigated on document-level in DrQA [ 6] and ORQA [ 23], and there are also previous works to extract important sentences in unsupervised ways, e.g. based on the metadata about structure [ 24]. Experiments on 4 different large datasets show its competitive performance. CogLTX is expected to become a general and strong baseline in many complex NLP tasks. CogLTX deﬁnes a pipeline for long text understanding under the “key sentences” assumption. Ex- tremely hard sequence-level tasks might violate it, thus efﬁcient variational bayes methods (estimating the distribution of z) with affordable computation still worth investigations. Besides, CogLTX has a drawback to miss antecedents right before the blocks, which is alleviated by prepending the entity name to each sentence in our HotpotQA experiments, and could be solved by position-aware retrieval competition or coreference resolution in the future. The work is supported by NSFC for Distinguished Young Scholar (61825602), NSFC (61836013), and a research fund supported by Alibaba. The authors would like to thank Danqi Chen and Zhilin Yang for their insightful discussion, and responsible reviewers of NeurIPS 2020 for their valuable suggestions. 9 Broader Impact Positive impact. The proposed method for understanding longer texts is inspired by the theory of working memory in the human brain. After the success of pretraining language models that learn from extremely large corpus, it still remains mysterious how human being can memorize, understand, and conduct efﬁcient yet effective reasoning process within a small memory budget, given a very few examples. Exploring such methods in fact may help design more elegant mechanism, or architecture that connects sub-models to solve complex tasks that require rich context and information. From a societal perspective, the proposed method can be also applied to many applications, e.g., legal document analysis, public opinion monitoring and searching. Negative impact. With the help of such methods, social platforms may get better understanding about their users by analysing their daily posts. Longer texts understanding speciﬁcally provide more accurate and coherent interpretation of who they are, which is a privacy threat.  
dynabert dynamic bert with adaptive width and depth 	DynaBERT: Dynamic BERT with Adaptive Width and Depth Lu Hou1, Zhiqi Huang2, Lifeng Shang1, Xin Jiang1, Xiao Chen1, Qun Liu1 1Huawei Noah’s Ark Lab { 2Peking University, China Abstract The pre-trained language models like BERT, though powerful in many natural language processing tasks, are both computation and memory expensive. To alleviate this problem, one approach is to compress them for speciﬁc tasks be- fore deployment. However, recent works on BERT compression usually com- press the large BERT model to a ﬁxed smaller size. They can not fully satisfy the requirements of different edge devices with various hardware performances. In this paper, we propose a novel dynamic BERT model (abbreviated as Dyn- aBERT), which can ﬂexibly adjust the size and latency by selecting adaptive width and depth. The training process of DynaBERT includes ﬁrst training a width-adaptive BERT and then allowing both adaptive width and depth, by dis- tilling knowledge from the full-sized model to small sub-networks. Network rewiring is also used to keep the more important attention heads and neurons shared by more sub-networks. Comprehensive experiments under various efﬁciency constraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its largest size has comparable performance as BERT BASE (orRoBERTa BASE), while at smaller widths and depths consistently outperforms existing BERT compres- sion methods. Code is available at / . 1 Introduction Recently, pre-trained language models based on the Transformer [ 24] structure like BERT [ 5] and RoBERTa [ 14] have achieved remarkable results on natural language processing tasks. However, these models have many parameters, hindering their deployment on edge devices with limited storage, computation, and energy consumption. The difﬁculty of deploying BERT to these devices lies in two aspects. Firstly, the hardware performances of various devices vary a lot, and it is infeasible to deploy one single BERT model to all kinds of edge devices. Thus different architectural conﬁgurations of the BERT model are desired. Secondly, the resource condition of one device under different circumstances can be quite different. For instance, on a mobile phone, when a large number of or programs are running, the resources that can be allocated to the current BERT model will be fewer. Thus once the BERT model is deployed, dynamically selecting a part of the model (also referred to as sub-networks ) for inference based on the device’s current resource condition is also desirable. Note that unless otherwise speciﬁed, the BERT model mentioned in this paper refers to a task-speciﬁc BERT rather than the pre-trained model. There have been some attempts to compress and accelerate inference of the models using low-rank approximation [ 15,12], weight-sharing [ 4,12], knowledge distillation [ 21,23,10,28], quantization [ 1,33,22,8] and pruning [ 16,17,3,25]. However, these methods usually compress the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. model to a ﬁxed size and can not meet the requirements above. In [ 4,7,6,13,29,34], Transformer- based models with adaptive depth are proposed to dynamically select some of the Transformer layers during inference. However, these models only consider compression in the depth direction and generate a limited number of architectural conﬁgurations, which can be restrictive for various deployment requirements. Some studies now show that the width direction also has high redundancy. For example, in [ 25,17], it is shown that only a small number of attention heads are required to keep comparable accuracy. There have been some works that train convolutional neural networks (CNNs) with adaptive width [ 32,31,30], and even both adaptive width and depth [ 2]. However, since each Transformer layer in the BERT model includes both a Multi-Head Attention (MHA) module and a position-wise Feed-forward Network (FFN) that perform in two different dimensions (i.e., the sequence and the feature dimensions), the width of the BERT model can not be simply deﬁned as the number of kernels as in CNNs. Moreover, successive training of ﬁrst along depth and then width as in [ 2] can be sub-optimal since these two directions are hard to disentangle. This may also cause the knowledge learned in the depth direction to be forgotten after the width is trained to be adaptive. In this work, we propose a novel DynaBERT model that offers ﬂexibility in both width and depth directions of the BERT model. Compared to [ 4,7,6,13] where only depth is adaptive, DynaBERT enables a signiﬁcantly richer number of architectural conﬁgurations and better exploration of the balance between model accuracy and size. Concurrently to our work, ﬂexibility in both directions is also proposed in [ 27], but on the Transformer structure [ 24] and on machine translation task. Besides the difference in the model and task, our proposed DynaBERT also advances in the following aspects: (1) We distill knowledge from the full-sized teacher model to smaller student sub-networks to reduce the accuracy drop caused by the lower capacity of smaller size. (2) Before allowing both adaptive width and depth, we train an only width-adaptive BERT (abbreviated as DynaBERTW) to act as a teacher assistant to bridge the large gap of model size between the student and teacher. (3) For DynaBERTW, we rewire the connections in each Transformer layer to ensure that the more important heads and neurons are utilized by more sub-networks. (4) Once DynaBERT is trained, no further ﬁne-tuning is required for each sub-network. Extensive experiments on the GLUE benchmark and SQuAD under various efﬁciency constraints show that, our proposed dynamic BERT (or RoBERTa) at its largest size performs comparably as BERT BASE (orRoBERTa BASE), while at smaller sizes outperforms other BERT compression methods. 2 Method In this section, we elaborate on the training method of our DynaBERT model. The training process (Figure 1) includes two stages. We ﬁrst train a width-adaptive DynaBERTWin Section 2.1 and then train the both width- and depth-adaptive DynaBERT in Section 2.2. Directly using the knowledge distillation to train DynaBERT without DynaBERTW, or ﬁrst train a depth-adaptive BERT and then distill knowledge from it to DynaBERT leads to inferior performance (Details are in Section 3.3). DynaBERT 𝐿𝐿 2× W Transformer layer3𝐿𝐿 4×𝐿𝐿× 𝑊𝑊×𝐿𝐿× 𝐿𝐿× 𝑊𝑊× 𝑊𝑊 2×𝑊𝑊 4×3𝑊𝑊 4×adaptive widthadaptive depth Figure 1: A two-stage procedure to train DynaBERT . First, using knowledge distillation (dashed lines) to transfer the knowledge from a ﬁxed teacher model to student sub-networks with adaptive width in DynaBERTW. Then, using knowledge distillation (dashed lines) to transfer the knowledge from a trained DynaBERTWto student sub-networks with adaptive width and depth in DynaBERT. 2 compute compute rankrankrewire rewire 𝑁ℎheads in MHA1 2𝑁ℎ… 𝑁ℎheads in MHA1 2𝑁ℎ…Add & Norm Feed Forward Network (FFN) Add & Norm Multi -head Attention (MHA) 𝐿×1 2 3𝑑𝑓𝑓… 1 2 3…𝑑𝑓𝑓|𝜕𝐿 𝒘𝒘| |𝜕𝐿 𝒉𝒉|Figure 2: Rewire connections in BERT based on the importance of attention heads in MHA and neurons in the intermediate layer of FFN.Algorithm 1 Train DynaBERTWor DynaBERT. 1:iftraining DynaBERTWthen net, depthList =[1]. 3:else: . 5:initialize a ﬁxed teacher model and a trainable stu- dent model with InitM . 6:foriter= 1,···, Ttrain do 7: Get next mini-batch of training data. 8: Clear gradients in the student model. 9: do 10: do 11: Compute loss L. 12: Accumulate gradient L.backward (). 13: end for 14: end for 15: Update with the accumulated gradients. 16:end for 2.1 Training DynaBERTWwith Adaptive Width Compared to CNNs stacked with regular convolutional layers, the BERT model is built with Trans- former layers, and the width of it can not be trivially determined due to the more complicated computation involved. Speciﬁcally, a standard Transformer layer contains a Multi-Head Attention (MHA) layer and a Feed-Forward Network (FFN). In the following, we rewrite the original formula- tion of MHA and FFN in [ 24] in a different way to show that, the computation of the attention heads of MHA and the neurons in the intermediate layers of FFN can be performed in parallel. Thus we can adjust the width of a Transformer layer by varying the number of attention heads and neurons in the intermediate layer of FFN. For thet-th Transformer layer, suppose the input to it is the sequence length and hidden state size, respectively. Following [ 17], we divide the computation of the MHA into the computations for each attention head. suppose there are NHattention heads in each layer, with by WQ h,WK h,WV h,WO . The output of the headhis computed as Attnh WQ h,WK h,WV h,WO h(X)=Softmax (1√ dXWQ hWK⊤ hX⊤)XWV hWO⊤ h.In multi-head attention, NHheads are computed in parallel to get the ﬁnal output [25]: MHAttn WQ,WK,WV,WO(X) =∑NH h=1Attnh WQ h,WK h,WV h,WO h(X). (1) Suppose the two linear layers in FFN are parameterized by ∈ Rdff×d,b2∈Rd, wheredffis the number of neurons in the intermediate layer of FFN. Denote the input of FFN is A∈Rn×d, the output of FFN can be divided into computations of dffneurons: ) =∑dff i=1GeLU (AW1 :,i+b1 i)W2 i,:+b2. (2) Based on (1) and (2), the width of a Transformer layer can be adapted by varying the number of attention heads in MHA and neurons in the intermediate layer of FFN (Figure 2). For width multipliermw, we retain the leftmost ⌊mwNH⌋attention heads in MHA and ⌊mwdff⌋neurons in the intermediate layer of FFN. In this case, each Transformer layer is roughly compressed by a ratio ofmw. This is not strictly equal as layer normalization and bias in linear layers also have very few parameters. The number of neurons in the embedding dimension is not adapted because these neurons are connected through skip connections across all Transformer layers and cannot be ﬂexibly scaled. 2.1.1 Network Rewiring To fully utilize the network’s capacity, the more important heads or neurons should be shared across more sub-networks. Thus before training the width-adaptive network, we rank the attention heads and neurons according to their importance in the ﬁne-tuned BERT model, and then arrange them with descending importance in the width direction (Figure 2). 3 Following [ 18,25], we compute the importance score of a head or neuron based on the variation in the training lossLif we remove it. Speciﬁcally, for one head with output h, its importance Ihcan be estimated using the ﬁrst-order Taylor expansion as ∂h(h−0) + ∂hh⏐⏐⏐⏐ if we ignore the remainder Rh=0. Similarly, for a neuron in the intermediate layer of FFN, denote the set of weights in to it as }, its importance is estimated by⏐⏐⏐⏐∂L ∂ww⏐⏐⏐⏐=⏐⏐⏐⏐∑ 2d i=1∂L ∂wiwi⏐⏐⏐⏐. Empirically, we use the development set to calculate the importance of attention heads and neurons. 2.1.2 Training with Adaptive Width After the connections of the BERT model are rewired according to Section 2.1.1, we use knowledge distillation to train DynaBERTW. Speciﬁcally, we use the rewired BERT model as the ﬁxed teacher network, and to initialize DynaBERTW. Then we distill the knowledge from the ﬁxed teacher model to student sub-networks at different widths in stage in Figure 1). Following [ 10], we transfer the knowledge in logits y, embedding (i.e., the output of the embedding layer) E, and hidden states (i.e. the output of each Transformer layer) Hl(l= 1,2,···,L) from the teacher model to ) l of the student sub-network with width multiplier mw. Here ) l∈Rn×d. Denote SCE as the soft cross-entropy loss and MSE as the mean squared error. The three distillation loss terms are ℓpred(y(mw),y) =SCE(y(mw),y), ℓ emb(E(mw),E) =MSE(E(mw),E), ℓhidn(H(mw),H) =∑L l=1MSE(H(mw) l,Hl). Thus the training objective is ) +) +ℓhidn(H(mw),H)), (3) the scaling parameters that control the weights of different loss terms. Note that we use the same scaling parameter for the distillation loss of the embedding and hidden states, because the two have the same dimension and similar scale. Empirically, we choose (λ1,λ2) = (1,0.1) is about one magnitude larger than ℓpred. The detailed training process of DynaBERTWis shown in Algorithm 1, where we restrict the depth multiplier mdto1(i.e., the largest depth) as DynaBERTWis only adaptive in width. To provide more task-speciﬁc data for distillation learning, we use the data augmentation method in TinyBERT [ 10], which uses a pre-trained BERT [ 5] trained from the masked language modeling task to generate task-speciﬁc augmented samples. 2.2 Training DynaBERT with Adaptive Width and Depth After DynaBERTWis trained, we use it as the ﬁxed teacher model, and to initialize the DynaBERT model. Then we distill the knowledge from the ﬁxed teacher model at the maximum depth to student sub-networks at equal or lower depths (Second stage in Figure 1). To avoid catastrophic forgetting of learned elasticity in the width direction, we still train over different widths in each iteration. For width multipliermw, the objective of the student sub-network with depth multiplier mdstill contains three termsℓ′ pred,ℓ′ embandℓ′ hidn as in (3). It makes the logits y(mw,md), embedding E(mw,md)and hidden states H(mw,md)mimic the teacher model with the maximum depth. When the depth multiplier md<1, the student has fewer layers than the teacher. In this case, we use the “Every Other” strategy in [ 7] and drop layers evenly to get a balanced network. Then we match the hidden states of the remaining layers LSin the student sub-network with those at depth d∈LT which satisﬁes mod (d+ 1,1 1−md)̸= 0from the teacher model as ℓ′ )) =∑ ) l,H(mw) l′). 4 We used+ 1here because we want to keep the knowledge in the last layer of the teacher model which is shown to be important in [ 28]. A detailed example can be found at Appendix A. Thus the distillation objective can still be written as L=λ1ℓ′ )) +λ2(ℓ′ )) +ℓ′ ) For simplicity, we do not tune λ1,λ2and choose (λ1,λ2) = (1,1)in our experiments. The training procedure can be found in Algorithm 1. After training with the augmented data and the distillation objective above (Step 1), one can further ﬁne-tune the network using the original data and the cross- entropy loss between the predicted labels and ground-truth labels (Step 2). Step 2 further improves the performance on some data sets empirically (Details are in Section 3.3). In this work, we report results of the model with higher average validation accuracy of all sub-networks, between before (Step 1) and after ﬁne-tuning (Step 2) with the original data. 3 Experiment In this section, we evaluate the efﬁcacy of the proposed DynaBERT on the General Language Un- derstanding Evaluation (GLUE) tasks [ 26] and the machine reading comprehension task SQuAD v1.1 [ 19], using both BERT BASE [5] and RoBERTa BASE [14] as the backbone models. The corre- sponding both width- and depth-adaptive BERT and RoBERTa models are named as DynaBERT and DynaRoBERTa, respectively. For BERT BASE andRoBERTa BASE, the number of Transformer layers isL= 12 , the hidden state size is d= 768 . In each Transformer layer, the number of heads in MHA isNH= 12 , and the number of neurons in the intermediate layer in FFN is dff= 3072 . The list of width multipliers is [], and the list of depth multipliers is [1.0,0.75,0.5]. There are a total of 4×3 = 12 different conﬁgurations of sub-networks. We use Nvidia V100 GPU for training. Detailed for the experiments are in Appendix B.2. We compare the proposed DynaBERT and DynaRoBERTa with (i) the base models BERT BASE [5] andRoBERTa BASE [14]; and (ii) popular BERT compression methods, including distillation methods DistilBERT [ 21], TinyBERT [ 10], and adaptive-depth method LayerDrop [ 7]. The results of the compared methods are taken from their original paper or ofﬁcial code repository. We evaluate the efﬁcacy of our proposed DynaBERT and DynaRoBERTa under different efﬁciency constraints, including #parameters, FLOPs, the latency on Nvidia K40 GPU and Kirin 810 A76 ARM CPU (Details can be found in Appendix B.3). 3.1 Results on the GLUE benchmark Data. The GLUE benchmark [ 26] is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE data sets are included in Appendix B.1. Following [ 5], for the development set, we report Spearman correlation for STS-B , Matthews correlation for CoLA and accuracy for the other tasks. For the test set of QQP andMRPC , we report “F1”. Main Results. Table 1 shows the results of sub-networks derived from the proposed DynaBERT and DynaRoBERTa. The proposed DynaBERT (or DynaRoBERTa) achieves comparable performances asBERT BASE (orRoBERTa BASE) with the same or smaller size. For most tasks, the sub-network of DynaBERT or DynaRoBERTa with the maximum size does not necessarily have the best performance, indicating that redundancy exists in the original BERT or RoBERTa model. Indeed, with the proposed method, the model’s width and depth for most tasks can be reduced without performance drop. Another observation is that using one speciﬁc width multiplier usually has higher accuracy than using the same depth multiplier. This indicates that compared to the depth direction, the width direction is more robust to compression. Sub-networks from DynaRoBERTa, most of the time, perform signiﬁcantly better than those from DynaBERT under the same depth and width. Test set results in Appendix C.1 also show that DynaBERT (resp. DynaRoBERTa) at its largest size has comparable or better accuracy as BERT BASE (resp. RoBERTa BASE). Comparison with Other Methods. Figure 3 compares DynaBERT and DynaRoBERTa on SST-2 andMNLI with other methods under different efﬁciency constraints, i.e., #parameters, FLOPs, latency on Nvidia K40 GPU and Kirin 810 ARM CPU. Results of the other data sets are in Appendix C.1. Note that each number of TinyBERT and DistilBERT uses a different model, while different numbers of our proposed DynaBERT and DynaRoBERTa use different sub-networks within one model. 5 Table 1: Development set results of the GLUE benchmark using DynaBERT and DynaRoBERTa with different width and depth multipliers (mw,md). Method CoLA STS-B MRPC RTE BERT BASE 58.1 89.8 87.7 71.1 DynaBERTmwmd 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 59.7 59.1 54.6 90.1 89.5 88.6 86.3 85.8 85.0 72.2 71.8 66.1 0.75x 60.8 59.6 53.2 90.0 89.4 88.5 86.5 85.5 84.1 71.8 73.3 65.7 0.5x 58.4 56.8 48.5 89.8 89.2 88.2 84.8 84.1 83.1 72.2 72.2 67.9 0.25x 50.9 51.6 43.7 89.2 88.3 87.0 83.8 83.8 81.4 68.6 68.6 63.2 MNLI-(m/mm) QQP QNLI SST-2 BERT BASE 84.8/84.9 90.9 92.0 92.9 DynaBERTmwmd 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 84.9/85.5 84.4/85.1 83.7/84.6 91.4 91.4 91.1 92.1 91.7 90.6 93.2 93.3 92.7 0.75x 84.7/85.5 84.3/85.2 83.6/84.4 91.4 91.3 91.2 92.2 91.8 90.7 93.0 93.1 92.8 0.5x 84.7/85.2 84.2/84.7 83.0/83.6 91.3 91.2 91.0 92.2 91.5 90.0 93.3 92.7 91.6 0.25x 83.9/84.2 83.4/83.7 82.0/82.3 90.7 91.1 90.4 91.5 90.8 88.5 92.8 92.0 92.0 CoLA STS-B MRPC RTE RoBERTa BASE 65.1 91.2 90.7 81.2 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 63.6 61.0.7 59.5 91.3 91.0 90.0 88.7 89.7 88.5 82.3 78.7 72.9 0.75x 63.7 61.4 54.9 91.0 90.7 89.7 90.0 89.2 88.2 79.4 77.3 70.8 0.5x 61.3 58.1 52.9 90.3 90.1 88.9 90.4 90.0 86.5 75.1 73.6 71.5 0.25x 54.2 46.7 39.8 89.6 89.2 87.5 88.2 88.0 84.3 70.0 70.0 66.8 MNLI-(m/mm) QQP QNLI SST-2 RoBERTa BASE 87.5/87.5 91.8 93.1 95.2 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 88.3/87.6 87.7/87.2 86.2/85.8 92.0 92.0 91.7 92.9 92.5 91.4 95.1 94.3 93.3 0.75x 88.0/87.3 87.5/86.7 85.8/85.4 91.9 91.8 91.6 92.8 92.4 91.3 94.6 94.3 93.3 0.5x 87.1/86.4 86.8/85.9 84.8/84.2 91.7 91.5 91.2 92.3 91.9 90.8 93.6 94.2 92.9 0.25x 84.6/84.7 84.0/83.7 82.1/82.0 91.2 91.0 90.5 90.9 90.9 89.3 93.9 93.2 91.6 From Figure 3, the proposed DynaBERT and DynaRoBERTa achieve comparable accuracy as BERT BASE andRoBERTa BASE, but often require fewer parameters, FLOPs or lower latency. Under the same efﬁciency constraint, sub-networks extracted from DynaBERT outperform DistilBERT and TinyBERT. Sub-networks extracted from DynaRoBERTa outperform LayerDrop by a large margin. They even consistently outperform LayerDrop trained with much more data. We speculate that it is because LayerDrop only allows ﬂexibility in the depth direction. On the other hand, ours enables ﬂexibility in both width and depth directions, which generates a signiﬁcantly larger number of architectural conﬁgurations and better explores the balance between model accuracy and size. BERT RoBERTa DistilBERT TinyBERT LayerDrop LayerDrop+more data DynaBERT DynaRoBERTa 40 60 80 100 120 # Accuracy (%) SST-2 40 60 80 100 120 # Accuracy (%) MNLI (a) #parameters(G). 0 5 10 15 20 25 Accuracy (%) SST-2 0 5 10 15 20 25 Accuracy (%) MNLI (b) FLOPs(G). 0 50 100 150 200 Nvidia K40 latency (s)909192939495 Accuracy (%) SST-2 0 50 100 150 200 Nvidia K40 latency (s)788082848688 Accuracy (%) MNLI (c) Nvidia K40 GPU latency(s). 0 100 200 300 400 500 Kirin 810 latency ( Accuracy (%) SST-2 0 100 200 300 400 500 Kirin 810 latency ( Accuracy (%) MNLI (d) Kirin 810 ARM CPU latency(ms). Figure 3: Comparison of #parameters, FLOPs, latency on GPU and CPU between our proposed DynaBERT and DynaRoBERTa and other methods. The GPU latency is the running time of 100 batches with batch size 128 and sequence length 128. The CPU latency is tested with batch size 1 and sequence length 128. Average accuracy of MNLI-m andMNLI-mm is plotted. 6 3.2 Results on SQuAD SQuAD v1.1 (Stanford Question Answering Dataset) [ 19] contains 100k crowd-sourced ques- tion/answer pairs. Given a question and a passage, the task is to extract the start and end of the answer span from the passage. The performance metric used is EM (exact match) and F1. Table 2 shows the results of sub-networks extracted from DynaBERT. Sub-network with only 1/2 width or depth of BERT BASE already achieves comparable or even better performance than it. Figure 4 shows the comparison of sub-networks of DynaBERT and other methods. We do not compare with LayerDrop [ 7] because SQuAD results are not reported in their paper. From Figure 4, with the same number of parameters, FLOPs, sub-networks extracted from DynaBERT outperform TinyBERT and DistilEBRT by a large margin. Table 2: Development set results on SQuAD v1.1. Method SQuAD v1.1 BERT BASE 81.5/88.7 DynaBERTmwmd 1.0x 0.75x 0.5x 1.0x 82.6/89.7 82.1/89.3 81.5/88.8 0.75x 82.3/89.5 82.1/89.3 80.9/88.5 0.5x 81.9/89.2 81.7/89.0 80.0/87.8 0.25x 80.7/88.1 79.9/87.5 76.6/85.0 40 60 80 100 120 # EM SQuAD v1.1 BERT BERT DistilBERT TinyBERT DynaBERT 0 20 40 60 80 EM SQuAD v1.1 BERT BERT DistilBERT TinyBERT DynaBERTFigure 4: Comparison of #parameters and FLOPs between DynaBERT and other methods. 3.3 Ablation Study Training DynaBERTWwith Adaptive Width. In Table 3, we evaluate the importance of network rewiring, knowledge distillation and data augmentation (DA) in the training of DynaBERTWusing the method in Section 2.1 on the GLUE benchmark. Due to space limit, only average accuracy of 4 width multipliers are shown in Table 3. Detailed accuracy for each width multiplier can be found in in Appendix C.2. without network rewiring, knowledge distillation and data augmentation is called “vanilla DynaBERTW”. We also compare against the baseline of using separate networks, each of which is initialized from the BERT BASE with a certain width multiplier ], and then ﬁne-tuned on the downstream task. From Table 3, vanilla the separate network baseline. Interestingly, the performance gain is more obvious for smaller data sets CoLA ,STS-B ,MRPC andRTE. After network rewiring, the average accuracy increases by over 2 points. The average accuracy further increases by 1.5 points with knowledge distillation and data augmentation. Table 3: Ablation study in training DynaBERTW. Average accuracy of 4 width multipliers is reported. MNLI-m MNLI-mm QQP QNLI SST-2 CoLA STS-B MRPC RTE avg. Separate network 82.2 82.2 90.3 87.8 91.0 39.9 84.6 78.8 61.6 77.6 Vanilla DynaBERTW 82.2 82.5 90.6 89.1 91.2 44.0 87.4 80.5 64.2 79.0 + Network rewiring 83.1 83.0 90.9 90.4 91.7 51.4 89.1 83.8 69.7 81.4 + Distillation and DA 84.5 84.9 91.0 92.1 92.7 55.9 89.7 86.1 69.5 82.9 Training DynaBERT with Adaptive Width and Depth. In Table 4, we evaluate the effect of knowledge distillation, data augmentation and ﬁnal ﬁne-tuning in the training of DynaBERT described in Section 2.2. Detailed accuracy for each width and depth multiplier is in Appendix C.2. The DynaBERT trained without knowledge distillation, data augmentation and ﬁnal ﬁne-tuning is called “vanilla DynaBERT”. From Table 4, with knowledge distillation and data augmentation, the average accuracy of each task is signiﬁcantly improved compared to the vanilla counterpart on all three data sets. Additional ﬁne-tuning further improves the performance on SST-2 andCoLA , but not MRPC . Empirically, we choose the model with higher average accuracy between before and after ﬁne-tuning with the original data using the method described in Section 2.2. DynaBERTWas a “teacher assistant”. In Table 5, we also compare with directly distilling the knowledge from the rewired BERT to DynaBERT without DynaBERTW. The average accuracy of 12 conﬁgurations of DynaBERT using DynaBERTWor not, are reported. As can be seen, using a width-adaptive DynaBERTWas a “teacher assistant” can efﬁciently bridge the large gap of size between the student and teacher, and has better performance on all three data sets investigated. 7 Table 4: Ablation study in training DynaBERT . Average accuracy of 12 conﬁgurations is reported. SST-2 CoLA MRPC Vanilla DynaBERT 91.3 46.0 82.1 + Distillation and DA 92.5 52.8 84.5 + Fine-tuning 92.7 54.8 83.2Table 5: Whether using DynaBERTWas a “teacher assistant”. Average accuracy of 12 conﬁgurations is reported. SST-2 CoLA MRPC DynaBERT 92.7 54.8 84.5 - DynaBERTW 92.3 54.1 84.4 Adaptive Depth First or Adaptive Width First? To ﬁnally obtain both width- and depth-adaptive DynaBERT, one can also train a only depth-adaptive model DynaBERTDﬁrst as the “teacher assis- tant”, and then distill knowledge from it to DynaBERT. Table 6 shows the accuracy of DynaBERTW different compression rates of the Transformer layers. As can be seen, signiﬁcantly better than DynaBERTDfor smaller width/depth multiplier 0.5. This may because unlike the width direction where the computation of attention heads and neurons are in parallel (Equations (1) and (2) in Section 2.1), the depth direction computes layer by layer con- secutively. Thus we can not rewire the connections based on the importance of layers in DynaBERTD, leading to severe accuracy drop of sub-networks with smaller depth in DynaBERTD. Table 6: Comparison of DynaBERTWand DynaBERTD. QNLI SST-2 CoLA STS-B MRPC mwormd 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x DynaBERTW 92.5 92.4 92.3 92.9 93.1 93.0 59.0 57.9 56.7 90.0 90.0 89.9 86.0 87.0 87.3 DynaBERTD 92.4 91.9 90.6 92.9 92.8 92.1 58.3 58.3 52.2 89.9 89.0 88.3 87.3 85.8 84.6 3.4 Looking into DynaBERT We conduct a case study on the DynaBERT trained on CoLA by visualizing the attention distributions in Figure 5. The sentence used is “the cat sat on the mat.” In [ 25,11,20], the attention heads for task are found to mainly play “positional”, “” functions. The positional head points to itself, adjacent tokens, [CLS], or [SEP] tokens, forming vertical or diagonal lines in the attention maps. The head points to tokens in a speciﬁc syntactic relation, and the attention maps do not have speciﬁc patterns. Figure 5: Attention maps of sub-networks with different widths and depths in DynaBERT trained on CoLA . The sentence used is “the cat sat on the mat.” 8 From Figure 5, the attention patterns in the ﬁrst three layers in the sub-network with (mw,md)= (0.25,1.0)are quite similar to those in the full-sized model, while those at intermediate layers show a clear function fusion. For instance, H1 in L5, H1-3 in L8, H3 in L9 (marked with red squares) in the sub-network with ( to exhibit more syntactic or semantic patterns than their positional counterparts in the full-sized model. This observation is consistent with the ﬁnding in [9] that linguistic information is encoded in BERT’s intermediate layers. Similarly, by comparing the attention maps in sub-networks with (), functions (marked with green squares) also start to fuse when the depth is compressed. Interestingly, we also ﬁnd that DynaBERT improves the ability of distinguishing linguistic accept- able and non-acceptable sentences for CoLA . This is consistent with the superior performance of DynaBERT than BERT BASE in Table 1. The attention patterns of SST-2 also explain why it can be compressed by a large rate without severe accuracy drop. Details can be found in Appendix C.4. 3.5 Discussion Comparison of Conventional and Inplace Distillation. To train width-adaptive CNNs, in [ 31], inplace distillation is used to boost the performance. Inplace distillation uses sub-network with the maximum width as the teacher while sub-networks with smaller widths in the same model as students. Training loss includes the loss from both the teacher network and the student network. Here we also adapt inplace distillation to train DynaBERTWin Table 7, and compare it with the conventional distillation used in Section 2.1. For inplace distillation, the student mimics the teacher and the teacher mimics a ﬁxed ﬁne-tuned task-speciﬁc BERT, via distillation loss over logits, embedding, and hidden states. From Table 7, inplace distillation has higher average accuracy on MRPC andRTE in training DynaBERTW, but performs worse on three data sets after training DynaBERT. Table 7: Comparison of conventional distillation and inplace distillation. Average accuracy of 4 width multipliers (DynaBERTW) or 12 conﬁgurations (DynaBERT) is reported. Distillation type SST-2 CoLA MRPC RTE SST-2 CoLA MRPC RTE 55.9 86.1 54.8 84.5 69.5 Inplace [31] 92.6 55.9 87.0 70.0 92.5 54.5 84.8 69.0 Different Methods to Train Section 2.1, we rewire the net- work only once, and train by alternating over four different width multipliers. In Table 8, we also adapt the following two methods in training width-adaptive CNNs to BERT: (1) using progressive rewiring (PR) as in [ 2] which progressively rewires the network as more width multipliers are supported; and (2) universally slimmable training (US) [ 31], which randomly samples some width multipliers in each iteration. The detailed setting of these two methods is in Appendix C.3. By comparing with Table 3, using PR or US has no signiﬁcant difference from using the method in Section 2.1. Table 8: Training DynaBERTWwith PR and US. Average accuracy of 4 width multipliers is reported. MNLI-m MNLI-mm QQP QNLI SST-2 CoLA STS-B MRPC RTE avg. PR [2] 82.3 82.8 90.9 90.4 91.6 52.8 89.1 84.5 70.3 81.6 US [31] 82.6 82.9 90.6 90.3 91.5 51.2 89.1 83.8 69.6 81.3 4 Conclusion In this paper, we propose DynaBERT which can ﬂexibly adjust its size and latency by selecting sub-networks with different widths and depths. DynaBERT is trained by knowledge distillation. We adapt the width of the BERT model by varying the number of attention heads in MHA and neurons in the intermediate layer in FFN, and adapt the depth by varying the number of Transformer layers. Network rewiring is also used to make the more important attention heads and neurons shared by more sub-networks. Experiments on various tasks show that under the same efﬁciency constraint, sub-networks extracted from the proposed DynaBERT consistently achieve better performance than the other BERT compression methods. 9 Broader Impact Traditional machine learning computing relies on mobile perception and cloud computing. However, considering the speed, reliability, and cost of the data transmission process, cloud-based machine learning may cause delays in inference, user privacy leakage, and high data transmission costs. In such cases, in addition to end-cloud collaborative computing, it becomes increasingly important to run deep neural network models directly on edge. Recently, pre-trained language models like BERT have achieved impressive results in various natural language processing tasks. However, the BERT model contains tons of parameters, hindering its deployment to devices with limited resources. The difﬁculty of deploying BERT to these devices lies in two aspects. Firstly, the performances of various devices are different, and it is unclear how to deploy a BERT model suitable for each edge device based on its resource constraint. Secondly, the resource condition of the same device under different circumstances can be quite different. Once the BERT model is deployed to a speciﬁc device, dynamically selecting a part of the model for inference based on the device’s current resource condition is also desirable. Motivated by this, we propose DynaBERT. Instead of compressing the BERT model to a ﬁxed size like existing BERT compression methods, the proposed DynaBERT can adjust its size and latency by selecting a sub-network with adaptive width and depth. By allowing both adaptive width and depth, the proposed DynaBERT also enables a large number of architectural conﬁgurations of the BERT model. Moreover, once the DynaBERT is trained, no further ﬁne-tuning is required for each sub-network, and the beneﬁts are threefold. Firstly, we only need to train one DynaBERT model, but can deploy different sub-networks to different hardware platforms based on their performances. Secondly, once one sub-network is deployed to a speciﬁc device, this device can select the same or smaller sub-networks for inference based on its dynamic efﬁciency constraints. Thirdly, different sub-networks sharing weights in one single model dramatically reduces the training and inference cost, compared to using models separately for different hardware platforms. This can reduce carbon emissions, and is thus more friendly. Though not originally developed for compression, sub-networks of the proposed DynaBERT out- perform other BERT compression methods under the same efﬁciency constraints like #parameters, FLOPs, GPU and CPU latency. Besides, the proposed DynaBERT at its largest size often achieves better performances as BERT BASE with the same size. A possible reason is that allowing adaptive width and depth increases the training difﬁculty and acts as regularization, and so contributes posi- tively to the performance. In this way, the proposed training method of DynaBERT also acts as a regularization method that can boost the generalization performance. Meanwhile, we also ﬁnd that the compressed sub-networks of the learned DynaBERT have good . In order to maintain the representation power, the attention patterns of sub-networks with smaller width or depth of the trained DynaBERT exhibit function fusion, compared to the full-sized model. Interestingly, these attention patterns even explain the enhanced performance of DynaBERT on some tasks, e.g., enhanced ability of distinguishing linguistic acceptable and non-acceptable sentences for CoLA . Besides the positive broader impacts above, since DynaBERT enables easier deployment of BERT, it also makes the negative impacts of BERT more severe. For instance, application in dialogue systems replaces help-desks and can cause job loss. Extending our method to generative models like GPT also faces the risk of generating offensive, biased or unethical outputs.  
incorporating bert into parallel sequence decoding with adapters 	Incorporating BERT into Parallel Sequence Decoding with Adapters Junliang Guo1, Zhirui Zhang2, Linli Xu1,3∗, Hao-Ran Wei2, Boxing Chen2, Enhong Chen1 1Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China 2Alibaba DAMO Academy 3IFLYTEK Co., Ltd. ,{linlixu, 2{zhirui.zzr, funan.whr, Abstract While large scale pre-trained language models such as BERT [ 5] have achieved great success on various natural language understanding tasks, how to efﬁciently and effectively incorporate them into models and the cor- responding text generation tasks remains a non-trivial problem. In this paper, we propose to address this problem by taking two different BERT models as the encoder and decoder respectively, and ﬁne-tuning them by introducing simple and lightweight adapter modules, which are inserted between BERT layers and tuned on the task-speciﬁc dataset. In this way, we obtain a ﬂexible and efﬁcient model which is able to jointly leverage the information contained in the source-side and target- side BERT models, while bypassing the catastrophic forgetting problem. Each com- ponent in the framework can be considered as a plug-in unit, making the framework ﬂexible and task agnostic. Our framework is based on a parallel sequence decoding algorithm named Mask-Predict [ 8] considering the bi-directional and conditional independent nature of BERT, and can be adapted to traditional autoregressive decoding easily. We conduct extensive experiments on neural machine translation tasks where the proposed method consistently outperforms autoregressive baselines while reducing the inference latency by half, and achieves scores on IWSLT14 German-English translation. When adapted to autoregressive decoding, the proposed method achieves 30.60/43.56 BLEU scores on WMT14 translation, on par with the baseline models. 1 Introduction Pre-trained language models [ 26,27,5,39] have received extensive attention in natural language processing communities in recent years. Generally, training of these models consists of two stages. Firstly, the model is trained on a large scale monolingual corpus in a manner, and then ﬁne-tuned end-to-end on downstream tasks with task-speciﬁc loss functions and datasets. In this way, pre-trained language models have achieved great success on various natural language understanding tasks such as reading comprehension and text classiﬁcation, and BERT [ 5] is one of the most successful models among them. While ﬁne-tuning BERT for common language understanding tasks is , for natural language generation which is one of the core problems in NLP [ 2,34,24], how to incorporate BERT ∗Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. remains substantially challenging. We conclude the main challenges as three-fold considering that the framework [ 33] is the backbone model of generation tasks. On the encoder side, as studied in [ 41], simply initializing the encoder with a pre-trained BERT will actually hurt the performance. One possible explanation could be that training on a complex task with rich resources (e.g., machine translation) leads to the catastrophic forgetting problem [ 23] of the pre-trained model. On the decoder side, which can be treated as a conditional language model, it is naturally non-trivial to marry unconditional pre-training with conditional ﬁne-tuning. And the bidirectional nature of BERT also prevents it from being directly applied to common autoregressive text generation. In addition, ﬁne-tuning the full model is parameter inefﬁcient considering the enormous scale of recent pre-trained language models [28] while being unstable and fragile on small datasets [20]. To tackle these challenges, in this paper, we propose a new paradigm of incorporating BERT into text generation tasks under the framework. Speciﬁcally, we construct our framework based on the following steps. We ﬁrst choose two pre-trained BERT models from the source/target side respectively, and consider them as the . For example, on the WMT14 English-German machine translation task, we take as the encoder as the decoder. Then, we introduce lightweight neural network components named adapter layers and insert them into each BERT layer to achieve the adaptation to new tasks. While ﬁne-tuning on task speciﬁc datasets, we freeze the parameters of BERT layers and only tune the adapter layers. We design different architectures for adapters. Speciﬁcally, we stack two feed-forward networks as the encoder adapter, mainly inspired from [ 3]; and an attention module is considered as the decoder adapter. Considering that BERT utilizes bi-directional context information and ignores conditional dependency between tokens, we build our framework on a parallel sequence decoding algorithm named Mask-Predict [ 8] to make the most of BERT and keep the consistency between training and inference. In this way, the proposed framework achieves the following beneﬁts. 1) By introducing the adapter modules, we decouple the parameters of the pre-trained language model and task-speciﬁc adapters, therefore bypassing the catastrophic forgetting problem. And the conditional information can be learned through the based adapter on the decoder side; 2) Our model is parameter efﬁcient and robust while tuning as a beneﬁt from the lightweight nature of adapter modules. In addition, thanks to parallel decoding, the proposed framework achieves better performance than autoregressive baselines while doubling the decoding speed; 3) Each component in the framework can be considered as a plug-in unit, making the framework very ﬂexible and task agnostic. For example, our framework can be adapted to autoregressive decoding by only incorporating the source-side BERT encoder and adapters while keeping the original Transformer decoder. We evaluate our framework on various neural machine translation tasks, and the proposed framework achieves scores on the IWSLT14 German-English translation tasks, achieving over traditional autoregressive baselines with half of the inference latency. When adapting to autoregressive decoding, we achieve 30.60/43.56 BLEU scores on the WMT14 translation tasks, on par with the baseline models. 2 Background Pre-Trained Language Models Pre-trained language models aim at learning powerful and con- textual language from a large text corpus by learning [ 26,5,27,28, 39,6,22], and they have remarkably boosted the performance of standard natural language under- standing tasks such as the GLUE benchmark [ 35]. BERT [ 5] is one of the most popular pre-training approaches, whose pre-training objective consists of masked language modeling (MLM) and next sentence prediction. The idea of MLM has been applied widely to other tasks such as neural machine translation [ 8]. Given an input sentence x= (x1,x2,...,x n), MLM ﬁrst randomly chooses a fraction (usually 15%) of tokens in xand substitutes them by a special symbol [MASK] , then predicts the masked tokens by the residual ones. Denote xmas the masked tokens and xras the residual tokens, the objective function of MLM can be written as: ) =−|xm|∑ t=1logP(xm t|xr;θenc), (1) the number of masked tokens. 2 Among the alternative pre-training methods, UniLM [ 6] extends BERT with unidirectional and predicting objectives, making it possible to ﬁne-tune the pre-trained model for text generation tasks. XLM [ 19] achieves cross-lingual pre-training on supervised parallel datasets with a similar objective function as MLM. MASS [ 30] proposes a monolingual pre-training framework where the encoder takes the residual tokens xras input and the decoder predicts the masked tokens . BART [ 21] adopts a similar framework and trains the model as a denoising autoencoder. Although achieving impressive results on various text generation tasks, these models are equipped with large-scale training corpora, therefore are time and resource consuming to train from scratch. In this paper, we focus on leveraging public pre-trained BERT models to deal with text generation tasks. Incorporating Pre-Trained Models There exist several recent works trying to incorporate BERT into text generation, which are mainly focused on leveraging the feature representation of BERT. Knowledge distillation [ 15,18] is applied in [ 37,38,4] to transfer the knowledge from BERT to either the encoder [ 38] or decoder side [ 37,4]. Zhu et al. [41] introduces extra attention based modules to fuse the BERT representation with the encoder representation. Most of these methods only incorporate BERT on either the source side or the target side. Our framework, on the other hand, is able to utilize the information of BERT from both sides. Fine-Tuning with Adapters Adapters are usually light-weight neural networks added into internal layers of pre-trained models to achieve the adaptation to downstream tasks, and have been successfully applied to ﬁne-tune vision models [ 29], language models [ 16,36] and multilingual machine translation models [ 3,17]. Different from these works, we explore combining two pre-trained models from different domains into a framework with the help of adapters. Parallel Decoding Parallel sequence decoding hugely reduces the inference latency by neglecting the conditional dependency between output tokens, based on novel decoding algorithms including non- autoregressive decoding [ 9,11,32,12], decoding [ 31,10] and Mask-Predict [ 8,13]. In Mask-Predict, the framework is trained as a conditional masked language model as: LCMLM () =−|ym|∑ t=1logP(ym ), (2) where (x,y)is a sample of parallel training pairs from the dataset, ymandyrare the target tokens, θencandθdecare the parameters of the encoder and decoder respectively. During inference, the model iteratively generates the target sequence in a manner, which ﬁts well with the bi-directional and conditional independent nature of BERT. Inspired by that, we conduct training and inference of our model in a similar way, which is introduced in Section 3.3. 3 Framework In this section we introduce the proposed framework of ﬁne-tuning BERT with adapters, termed as Adapter-Bert Networks (AB-Net) and illustrated in Figure 1. We start with the problem deﬁnition. Problem Deﬁnition Given two pre-trained BERT models XBERT andYBERT on the source side and the target side respectively, we aim at ﬁne-tuning them in a framework by introducing adapter modules, on a parallel training dataset (X,Y)which consists of pairs of source and target sequences (x,y)∈(X,Y). The loss function of our framework is deﬁned in a similar way as the conditional MLM loss introduced in Equation (2): ) =−|ym|∑ t=1logP(ym ), (3) the parameters of encoder adapters and decoder adapters respectively. 3.1 Adapter-Bert Networks The architecture of BERT [ 5] is akin to a Transformer encoder [ 34], which is constructed by self- attention, feed-forward layers, layer normalization [ 1] and residual connections [ 14]. We denote a 3  Forward Layer 𝑥Feed Forward Layer ×2BERT Layer×𝑀Encoder Adapter Forward Layer AttentionFeed Forward LayerDecoder AdapterBERT LayerAdapter ModulesBERT ModulesDecoder OutputFigure 1: An illustration of the proposed framework. Blue blocks constitute the pre-trained BERT models which are frozen during ﬁne-tuning, and orange blocks represent the adapter components which are inserted into each BERT layer and trained during ﬁne-tuning. the source sequence and the residual target sequence in Equation (3) respectively. MandNindicate the number of layers of the encoder and decoder. For simplicity, we omit some architecture details such as layer normalization and residual connections. BERT layer block as XBERT (·)orYBERT (·). Please refer to Appendix A for more details about the model architectures. To ﬁne-tune BERT for natural language generation, we introduce adapter layers and insert them into each BERT layer. On the encoder side, we follow [ 3] and simply construct the adapter layer with layer normalization as well as two feed-forward networks with non-linearity between them: Z=W1·(LN(H)), H (Z)), (4) the input and output hidden states of the adapter respectively, LNindicates layer normalization, W1andW2are the parameters of the feed-forward networks. The only hyper- parameter brought by this module is the dimension dAencof the internal hidden state Z, through which we can ﬂexibly control the capacity and efﬁciency of adapter layers. Denoting the encoder adapter layer as AENC(·), for each encoder layer in the framework, the hidden state is computed as: HE l+1=AENC(XBERT (HE l)), (5) whereHE lis the output hidden state of the l-th encoder layer. And we take the hidden state HEof the last encoder layer as the representation of the source sequence. As for the decoder, the introduced adapter modules should be able to model the conditional in- formation from the source side. Therefore, we adopt the multi-head computed over the encoder output and decoder input as the adapter. Denote the attention based adapter as ADEC(Q,K,V ), then given the hidden output HD lof thel-th decoder layer, the hidden state of the (l+ 1) -th layer is calculated as: HD l+1=ADEC(YBERT (HD l),HE,HE). (6) By introducing and carefully designing the adapter modules on the encoder and decoder, our frame- work is able to utilize the pre-trained information from both sides as well as build the conditional dependency, making it possible to apply the model on conditional text generation tasks. 3.2 Discussion To the best of our knowledge, our framework is the ﬁrst work that is able to jointly integrate pre- trained models from both the encoder and decoder sides. Different from most previous works that plainly utilize BERT as a feature extractor [ 41,38,37], we directly exploit BERT as the encoder and decoder to make the most of pre-trained models. Comparing with the related works that also utilize adapter modules while ﬁne-tuning [ 16,36,3], we do not constrain the architectures of adapters to be ﬁxed, but adopt different architectures on the encoder and decoder sides. In addition, we can easily 4 extend the architectures of adapters to adjust to different downstream tasks. For example, while our framework is designed for parallel decoding, it is to transform it to traditional autoregressive decoding by extending the based adapter to a traditional Transformer decoder. We show in Table 3a that our autoregressive variant is able to achieve strong performance. Meanwhile, by integrating two large scale pre-trained models into a framework, we have illustrated their beneﬁts as well as drawbacks. The main limitation is the extra computation cost brought by the enormous pre-trained parameters. Fortunately, thanks to the lightweight and ﬂexible adapter modules, the scale of parameters that require training in our framework is less than that of an autoregressive Transformer model. Besides, we have multiple ways to adjust the scale of adapters. For example, while training, instead of inserting adapter layers to all BERT layers, we can only insert them into the top layers to speed up training. We can also reduce the hidden dimensions of adapters to control the parameter scale with negligible degradation of performance. A thorough study is conducted regarding the ﬂexibility of adapters in Section 4.4. It can be shown that, at the inference stage, even with two large pre-trained models introduced, our framework based on parallel decoding can still achieve faster decoding speed than traditional autoregressive baselines. 3.3 Training and Inference We mainly follow the training and inference paradigm used in [ 8]. To decode the target sequence in parallel, the model needs to predict the target length conditioned on the source sequence, i.e., ). We add a special [LENGTH] token to the encoder input, and take its encoder output as the representation, based on which the target length is predicted. The length prediction loss is added to the word prediction loss in Equation (3) as the ﬁnal loss of our framework. In Equation (3), given a training pair (x,y), we randomly mask a set of tokens in ywith [MASK] , and the number of the masked tokens|ym|is uniformly sampled from 1to|y|instead of being computed by a ﬁxed ratio as BERT [5]. The masked tokens are denoted as ymwhile the residual tokens are denoted as yr, While inference, the target sequence is generated iteratively in a manner. Speciﬁ- cally, after the length of the target sequence is predicted by the encoder, the decoder input is initialized with the [MASK] symbol at all positions. After the prediction process of the decoder, a number of tokens with the lowest probabilities in the decoder output are replaced by [MASK] . The obtained sequence is taken as the decoder input of the next iteration until the stop condition is hit. The number of masked tokens at each iteration follows a linear decay function utilized in [ 8]. As for the stop condition, the ﬁnal result is obtained either when the upper bound of iterations is reached, or the obtained target sequence do not change between two consecutive iterations. Details of the decoding algorithm are provided in Appendix B. 4 Experiments We mainly conduct experiments on neural machine translation to evaluate our framework. We also explore its autoregressive variant in Section 4.3, followed with ablation studies in Section 4.4. 4.1 Experimental Setup Datasets We evaluate our framework on benchmark datasets including IWSLT14 German→English (IWSLT14 De-En)2, WMT14 English↔German translation (WMT14 En-De/De- En)3, and WMT16 Romanian →English (WMT16 Ro-En)4. We show the generality of our method on several low-resource datasets including IWSLT14 English ↔ (IWSLT14 En↔It/Es/Nl). We additionally consider WMT14 English →French translation (WMT14 En-Fr) for autoregressive decoding. We follow the dataset conﬁgurations of previous works strictly. For IWSLT14 tasks, we adopt the ofﬁcial split of sets. For WMT14 tasks, we utilize newstest2013 as the validation and test set respectively. For WMT16 tasks, we use newsdev2016 as the validation and test set. For autoregressive decoding, we consider WMT16 Ro-En augmented with back translation data5to keep consistency with base- / 5 Table 1: The BLEU scores of the proposed AB-Net and the baseline methods on the IWSLT14 De-En, WMT16 Ro-En and WMT14 En-De/De-En tasks. The per-sentence decoding latency and the number of trained parameters on the WMT14 En-De task are also reported. “ ∗” indicates the results obtained by our implementation, “/” indicates the corresponding result is not provided. IWSLT14 WMT16 WMT14 #Trained Models De−En Ro−En En−De De−En Latency Parameters [34] 74M Mask-Predict [8] 31.71∗33.31 27.03 30.53 161∗ms 75M BERT-Fused NAT [41] 90M AB-Net 36.49 35.63 28.69 33.57 327ms 67M AB-Net-Enc 34.45 / 28.08 / 165ms 78M lines [ 41]. We tokenize and segment each word into wordpiece tokens with the internal preprocessing code in BERT6using the same vocabulary as pre-trained BERT models, resulting in vocabularies with30k tokens for each language. More details of the datasets are described in Appendix C. Model Conﬁgurations We mainly build our framework on bert-base models (nlayers = 12 , nheads= 12 ,dhidden = 768 ,dFFN= 3072 ). Speciﬁcally, for English we use on IWSLT14 and on WMT tasks. We use for German for all other languages. When extending to autoregressive decoding, we utilize (nlayers = 24 ,nheads = 16 ,dhidden = 1024 ,dFFN= 4096 ) for English to keep consistency with [ 41]. For adapters, on the encoder side, we set the hidden dimension between two FFN layers as dAenc= 2048 for WMT tasks and 512for IWSLT14 tasks. On the decoder side, the hidden dimension of the module is set equal to the hidden dimension of BERT models, i.e., dAdec= 768 forbert-base models and dAdec= 1024 for bert-large models. We train our framework on 1/8Nvidia 1080Ti GPUs for IWSLT14/WMT tasks, and it takes 1/7days to ﬁnish training. Our implementation is based on fairseq and is available at . Baselines We denote the proposed framework as AB-Net, and to make a fair comparison with baseline models, we also consider a variant of our model that only incorporates BERT on the source-side with encoder adapter layers and denote it as AB-Net-Enc. With parallel decoding, we consider Mask-Predict [ 8] as the backbone training and inference algorithm, based on which we re-implement BERT-Fused [ 41] and take it as the main baseline, denoted as BERT-Fused NAT. With autoregressive decoding where BERT is utilized only on the source-side, we compare our framework with BERT-Fused [41], BERT-Distilled [4] and CT-NMT [38] with their reported scores. Inference and Evaluation For parallel decoding, we utilize sequence-level knowledge distilla- tion [ 18] on the training set of WMT14 En-De/De-En tasks, to keep consistency with [ 8]. This technique has been proved by previous models that it can produce less noisy and more deterministic training data [ 9]. We use the raw training data for all other tasks. While inference, we generate multiple translation candidates by taking the top Blength predictions into consideration, and select the translation with the highest probability as the ﬁnal result. We set B= 4for all tasks. And the upper bound of iterative decoding is set to 10. For autoregressive decoding, we use beam search with width 5for all tasks. We utilize BLEU scores [ 25] as the evaluation metric. Speciﬁcally, we use and report the tokenized scores for IWSLT14 tasks and tokenized case-sensitive scores for WMT tasks. 4.2 Results The results of our framework with parallel decoding are listed in Table 1. The autoregressive Transformer model with base conﬁguration [ 34] is also compared as a baseline. In addition to BLEU scores, we also report the per-sentence decoding latency on the newstest2014 test set as well as the number of trained parameters on the WMT14 En-De task. As can be observed from Table 1, / 6 Table 2: The performance of the proposed AB-Net on IWSLT14 low-resource language pairs. Mask-Predict as well as the autoregressive model are considered as baselines. Models En-It It-En En-Es Es-En En-Nl Nl-En [34] 29.26 33.57 36.04 39.31 31.30 36.19 Mask-Predict [8] 26.05 29.50 32.15 35.37 25.78 32.91 AB-Net 31.81 34.20 37.45 42.66 32.52 38.94 Table 3: (a) The results of machine translation with autoregressive decoding of our framework and baseline methods. We directly copy the best results reported in their papers. indicates Transformer with conﬁguration. (b) The ablation study on different components of the proposed model conducted on the test set of IWSLT14 De-En. “Decoder” indicates a traditional Transformer decoder. ×indicates the setting with no convergence reached during training. (a) Results on autoregressive decoding. WMT14 WMT16 Models En−De En−Fr Ro−En [34] ∗ BERT-Distilled [4] 27.53 / / CT-NMT [38] 30.10 42 .30 / BERT-Fused [41] 30.75 43 .78 39 .10 AB-Net-Enc 30.60 43 .56 39 .21(b) Ablation study. Model Variants BLEU 33.59 (1): X BERT + Decoder × (2): (1) + A ENC 34.45 (3): (2) + Y BERT × (4): (3) + A DEC 36.49 (5): (4) + A DEC on top 6layers 34.60 (6): (5) + A ENC on top 6layers 33.78 with parallel decoding, Mask-Predict achieves considerable inference speedup but also suffers from performance degradation at the same time. Equipped with pre-trained BERT models from both sides, our framework obtains a huge performance promotion compared with Mask-Predict. In addition, we also outperforms the autoregressive baseline by a ﬁrm margin with similar scales of trainable parameters, while achieving 2.38times speedup regarding the inference speed. Compared with BERT-Fused NAT [ 41] which utilizes BERT only on the encoder side, our framework as well as the variant AB-Net-Enc both achieve better performance with less parameters to train, illustrating that the introduced adapter modules are able to leverage more information in a more efﬁcient way. Regarding the scale of trained parameters, we can notice that AB-Net-Enc actually introduces more parameters than AB-Net which utilizes BERT from both sides. The reason lies in the embedding layer. By incorporating BERT through adapters, the proposed framework gets rid of training the giant embedding layer which usually introduces ~ 15M parameters to train on each side if embeddings are not shared. Comparing with BERT-Fused NAT [ 41], our framework is able to save ~ 26% parameters while incorporating information from both sides, providing a more cost-effective solution for leveraging pre-trained models based on adapters. Results on Low-Resource Language Pairs We also study the performance of our framework on three low-resource language pairs in the IWSLT14 dataset. Results on both directions are shown in Table 2. The proposed AB-Net consistently outperforms the compared baselines among various language pairs, demonstrating the generality of our method. 4.3 Exploration on Autoregressive Decoding Here we explore the application of our framework on autoregressive decoding. As the bidirectional and conditional independent nature of BERT prevents it from being applied to autoregressive decoding, to show the ﬂexibility of the proposed framework, we directly use AB-Net-Enc as the autoregressive variant, whose encoder is initialized with the source-side BERT model and equipped with encoder adapter layers, while the decoder is an autoregressive Transformer Decoder. We compare our model with three ﬁne-tuning baselines including BERT-Fused [ 41], BERT-Distilled [ 4] and CT-NMT [ 38]. Results are shown in Table 3a. Our framework outperforms the baseline over all three translation tasks, with improvements from 1.33to2.75BLEU scores. BERT-Fused [ 41] 7 also achieves considerable performance. Nevertheless it is worth noting that BERT-Fused requires pre-training a standard Transformer model at ﬁrst (without which there will be a drop of 2.5BLEU score as reported), which is time consuming. While we simply train our Transformer decoder from scratch, we expect additional performance gains if similar tricks are applied. We have also explored other alternatives. In practice, the bi-directional property is achieved by setting the attention mask as a matrix with all 1s to enable each token to see all other tokens. Therefore, in our framework, we try to ﬁne-tune BERT on autoregressive decoding by setting the attention matrix of the decoder as an upper triangle matrix to prevent the model from attending to the future words. However, in this way, we can only achieve sub-optimal results compared with the BERT encoder variant AB-Net-Enc ( 26.40vs29.96on the test set of IWSLT14 En-De). One possible reason is that the introduced adapter parameters are not powerful enough to change the bidirectional nature of BERT without tuning it. Another solution is to mingle autoregressive pre-trained models with BERT to construct a hybrid framework, e.g., a BERT encoder and a GPT decoder, which ﬁts well in nature with an autoregressive model. We leave that for future work. 4.4 Ablation Study In this subsection, we further conduct ablation studies regarding the scale of adapters, the proposed dif- ferent components, different ﬁne-tuning strategies and baselines with . Experiments are conducted on the IWSLT14 De-En dataset with parallel decoding. 26272829210 Hidden 1M5M10M15M20M #Para Our BLEU Our #Para Trm BLEU Trm #Para Figure 2: The study on the scale of encoder adapters. Blue lines with the left y-axis indicate BLEU scores while red lines with the right y-axis indicate the number of parameters to train on the encoder side. Trm indicates the model. Best view in on the Scale of Adapters We in- vestigate the inﬂuence of the scale of adapters in Figure 2. Speciﬁcally, we ﬁx the scale of the de- coder adapter and tune the hidden dimension of the encoder adapter dAencin a wide range ( 26to 210). We also plot the number of trained parame- ters on the encoder side in our framework and in the model to make a compari- son. From Figure 2, we ﬁnd that our framework is robust to the scale of adapters, e.g., halving the dimension from 29to28only results in a 0.4 drop of the BLEU score. Compared with the autoregressive Transformer baseline, our frame- work is able to achieve better performance with to train (getting 34.81score whendAenc= 64 ), illustrating the efﬁciency of adapters. Ablations on the Different Proposed Compo- nents In Table 3b, we study the inﬂuence of different components in our framework includ- ing the pre-trained BERT models ( XBERT and YBERT ) and adapter layers ( AENC andADEC). We can ﬁnd that without utilizing adapters on either side, the model cannot converge during training, indicating the necessity of the adapter modules. While BERT models are usually very deep ( 12layers), we also explore to insert adapters into top layers only (i.e., 7~12-th layers) to reduce the scale of the introduced parameters in settings (5) and (6). As shown in Table 3b, when only introducing adapters to the top layers of both XBERT andYBERT following setting (6), our framework still outperforms the autoregressive Transformer baseline, showing that it is ﬂexible to balance the performance and scale of our framework. In addition, we can ﬁnd that the decoder adapter contributes more than the encoder adapter, illustrating the importance of modeling the conditional dependency over all scales of hidden . Comparison with Different Fine-Tuning Strategies In addition to the proposed model which freeze the BERT components and only tune the adapters while training, we also consider the variant that ﬁne-tunes the full model in AB-Net (AB-Net FB), or trains AB-Net from scratch (AB-Net SC). We train all variants for 50epochs and evaluate on the validation set of IWSLT14 De-En. Results are shown in Figure 3a, where AB-Net converges signiﬁcantly faster than AB-Net FB, and AB-Net SC does not converge. When ﬁne-tuning the full model, more GPU memory is required because 8 0 10 20 30 40 50 LossAB-Net FB, BLEU=32.03 AB-Net SC, BLEU=0.86 AB-Net, BLEU=37.14(a) Raw Data 0.5M 0.8M 1.0M 1.5M 2.0M #Back Translation BLEU 34.7134.62 BERT-Fused AB-Net (b) Figure 3: (a): Results of different ﬁne-tuning strategies. (b): Results of baselines trained with extra monolingual data via . All settings are evaluated on the validation set of the IWSLT14 De-En task. more gradient states need to be stored, therefore we have to halve the batchsize to ﬁt the model into GPUs, which slows down the training process. With the same batchsize, AB-Net saves 29% GPU memory and 26% wall-clock training time compared with AB-Net FB. Moreover, we ﬁnd that directly ﬁne-tuning BERT is very unstable and sensitive to the learning rate, while only tuning the adapters alleviates this problem and is relatively more robust. Comparison with is a simple yet effective data augmentation method in NMT [ 7,40]. While we leverage BERT models which are pre-trained with extra mono- lingual data, we also consider baselines trained with extra monolingual data via to construct fair comparisons. Speciﬁcally, we ﬁrst train an AT model on the IWSLT14 En-De task (with a28.96BLEU score), and then use it to generate additional training pairs on the English Wikipedia data, which is a subset of the training corpus of BERT. Results are shown in Figure 3b. We can ﬁnd that the gains brought by are limited, and adding over 1M monolingual data actually brings a performance drop. In addition, comparing with our method, requires to train another model and decode a large amount of monolingual data, which is time consuming. 5 Conclusion In this paper, we propose a new paradigm of incorporating BERT into text generation tasks with a framework. We initialize the with a pre-trained BERT model on the source/target side, and insert adapter layers into each BERT layer. While ﬁne-tuning on downstream tasks, we freeze the BERT models and only train the adapters, achieving a ﬂexible and efﬁcient framework. We build our framework on a parallel decoding method named Mask-Predict to match the bidirectional and conditional independent nature of BERT, and extend it to traditional autoregressive decoding in a way. Our framework avoids the catastrophic forgetting problem and is robust when ﬁne-tuning pre-trained language models. The framework achieves strong performance on neural machine translation while doubling the decoding speed of the Transformer baseline. In the future, we will try to combine two different pre-trained models together in our framework, such as a BERT encoder and a GPT/XLNet decoder, to explore more possibilities on autoregressive decoding. This research was supported by the National Natural Science Foundation of China (61673364, U1605251), Anhui Provincial Natural Science Foundation (2008085J31) and the National Key R&D Program of China (2018YFB1403202). We would like to thank the Information Science Laboratory Center of USTC for the hardware and software services. We thank the anonymous reviewers for helpful feedback on early versions of this work. The work was done when the ﬁrst author was an intern at Alibaba. 9 Broader Impact The proposed framework can be seen as a new and general paradigm of designing sequence-to- sequence models by leveraging pre-trained models, which has a wide range of applications not limited to the text generation tasks discussed in the paper, e.g., end-to-end speech/image translation. If proper pre-trained models for the speciﬁc task are provided (such as BERT for text or ResNet for images), our framework can then provide a cost-effective solution to leverage them without tuning their massive parameters or re-training them from scratch, which will save lots of resources for researchers who are individual or afﬁliated with academic institutions. In addition, different from most pre-training approaches which particularly focus on English, our framework works well when dealing with various low-resource languages as shown in the paper, therefore we may help improve the performance of existing low-resource machine translation systems. On the other hand, although we only need to tune the adapters while training, we have to load the whole framework into GPUs, which limits the choices of because large-scale BERT models will occupy more memory than traditional Transformer models. As a consequence, our framework may not perform as its best on GPUs with limited memory. From a broader perspective, the proposed framework is not free from the risks of automation methods. For example, the model may inherit the biases contained in data. And in our framework, both pre-training and ﬁne-tuning datasets may have biases. Therefore we encourage future works to study how to detect and mitigate similar risks that may arise in our framework.  
probabilistic linear solvers for machine learning 	Probabilistic Linear Solvers for Machine Learning Jonathan Wenger Philipp Hennig University of Tübingen Max Planck Institute for Intelligent Systems Tübingen, Germany {, Abstract Linear systems are the bedrock of virtually all numerical computation. Machine learning poses speciﬁc challenges for the solution of such systems due to their scale, characteristic structure, stochasticity and the central role of uncertainty in the ﬁeld. Unifying earlier work we propose a class of probabilistic linear solvers which jointly infer the matrix, its inverse and the solution from matrix-vector product observations. This class emerges from a fundamental set of desiderata which constrains the space of possible algorithms and recovers the method of conjugate gradients under certain conditions. We demonstrate how to incorporate prior spectral information in order to calibrate uncertainty and experimentally showcase the potential of such solvers for machine learning. 1 Introduction Arguably one of the most fundamental problems in machine learning, statistics and scientiﬁc com- putation at large is the solution of linear systems of the form Ax∗=b, where A∈Rn×n sym is a symmetric positive deﬁnite matrix [ 1–3]. Such matrices usually arise in the context of second-order or quadratic optimization problems and as Gram matrices. Some of the numerous application areas in machine learning and related ﬁelds are least-squares regression [ 4], kernel methods [ 5], Kalman ﬁltering [ 6], Gaussian (process) inference [ 7], spectral graph theory [ 8], (linear) differential equations [9] and (stochastic) second-order methods [10]. Linear systems in machine learning are typically large-scale, have characteristic structure arising from generative processes, and are subject to noise. These distinctive features call for linear solvers that can explicitly make use of such structural information. While classic solvers are highly optimized for general problems, they lack key functionality for machine learning. In particular, they do not consider generative prior information about the matrix. An important example are kernel Gram matrices, which exhibit speciﬁc sparsity structure and spectral properties, depending on the kernel choice and the generative process of the data. Exploiting such prior information is a prime application for probabilistic linear solvers, which aim to quantify numerical uncertainty arising from limited computational resources. Another key challenge, which we will not yet address here, are noisy matrix evaluations arising from data subsampling. Ultimately, linear algebra for machine learning should integrate all sources of uncertainty in a computational pipeline – aleatoric, epistemic and numerical – into one coherent probabilistic framework. Contribution This paper sets forth desiderata for probabilistic linear solvers which establish ﬁrst principles for such methods. From these, we derive an algorithm incorporating prior information on the matrix Aor its inverse A−1, which jointly estimates both via repeated application of A. This results in posterior beliefs over the two operators and the solution which quantify numerical uncertainty. Our approach uniﬁes and extends earlier formulations and constitutes a new way of 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. Linear System A x∗= b HPrior E[A] x0= b i= 1 :k A s1... sk= y1... ykPosterior E[A] xk= b E[H]Samples A0 A1 A2 A3 H0 H1 H2 H3Figure 1: Illustration of a probabilistic linear solver. Given a prior for AorHmodelling the linear operator Aand its inverse A−1, posterior beliefs are inferred via observations yi=Asi. This induces a distribution on the solution x∗, quantifying numerical uncertainty arising from ﬁnite computation. The plot shows k= 3iterations of Algorithm 1 on a toy problem of dimension n= 5. interpreting linear solvers. Further, we propose a prior covariance class which recovers the method of conjugate gradients as its posterior mean and uses prior spectral information for uncertainty calibration, one of the primary shortcomings of probabilistic linear solvers. We conclude by presenting simpliﬁed examples of promising applications of such solvers within machine learning. 2 Probabilistic Linear Solvers LetAx∗=bbe a linear system with A∈Rn×n sym positive deﬁnite and linear solvers (PLS) [ 11–13] iteratively build a model for the linear operator A, its inverse H=A−1or the solution x∗, represented by random variables A,Horx. In the framework of probabilistic numerics [14,15] such solvers can be seen as Bayesian agents performing inference via linear observations Y= [ from actions S= [ by an internal ). For a matrix-variate prior prior (generative) information, our solver computes posterior beliefs over the matrix, its inverse and the solution of the linear system. An illustration of a probabilistic linear solver is given in Figure 1. Desiderata We begin by stipulating a fundamental set of desiderata for probabilistic linear solvers. To our knowledge such a list has not been collated before. Connecting previously disjoint threads, the following presents a roadmap for the development of these methods. Probabilistic linear solvers modelling AandA−1must assume matrix-variate distributions which are expressive enough to capture structure and generative prior information either for Aor its inverse. The distribution choice must also allow efﬁcient sampling and density evaluation. It should encode symmetry and positive deﬁniteness and must be closed under positive linear combinations. Further, the two models for the system matrix or its inverse should be translatable into and consistent with each other. Actions siof a PLS should be model-based and induce a tractable distribution on linear observations yi=Asi. Since probabilistic linear solvers are low-level procedures, their inference procedure must be lightweight. Given () observations this requires tractable posteriors over A,Handx, which are calibrated in the sense that at convergence the true solution x∗ represents a draw from the posterior p(x|Y,S). Finally, such solvers need to allow of the problem and ideally should return beliefs over non-linear properties of the system matrix extending the functionality of classic methods. These desiderata are summarized concisely in Table 1. 2.1 Bayesian Inference Framework Guided by these desiderata, we will now outline the inference framework for A,Handxforming the base of the algorithm. The choice of a matrix-variate prior distribution is severely limited by the desideratum that conditioning on linear observations yi=Asimust be tractable. This reduces the choice to stable distributions [ 16] and thus excludes candidates such as the Wishart, which has measure zero outside the cone of symmetric positive semi-deﬁnite matrices. For symmetric matrices, this essentially forces use of the symmetric matrix-variate normal distribution, introduced in this context by Hennig [11]. Given A0,WA 0∈Rn×n sym, assume a prior distribution p(A) =N(A;A0,WA 0 WA 0), 2 Table 1: Desired properties of probabilistic linear solvers. Symbols ( ,∼, ) indicate which properties are encoded in our proposed solver (see Algorithm 1) and to what degree. No. Property Formulation (1) distribution over matrices A∼D, pD(A) (2) symmetry A=A⊺a.s. (3) positive deﬁniteness ∀v̸= 0 :v⊺Av>0a.s. ∼ (4) positive linear combination in same distribution family ∀αj>0 :∑ jαjAj∼D (5) corresponding priors on the matrix and its inverse p(A)←→p(H) (6) model-based policy ) (7) matrix-vector product in tractable distribution family As∼D′ (8) noisy observations p(Y|A,S) =N(Y;AS,Λ) (9) tractable posterior ) (10) calibrated uncertainty ]) ∼ (11) ( (12) distributions over non-linear derived quantities of A det(A), σ(A),A=L⊺L,... where denotes the symmetric Kronecker product [ 17].1The symmetric matrix-variate Gaussian induces a Gaussian distribution on linear observations. While it has non-zero measure only for symmetric matrices, its support is not the positive deﬁnite cone. However, positive deﬁniteness can still be enforced post-hoc (see Proposition 1). We assume noise-free linear observations of the form yi=Asi, leading to a Dirac likelihood p(Y|A,S) = lim ) =δ(Y−AS). The posterior distribution follows from the properties of Gaussians [ 4] and has been investigated in detail in previous work [18, 11, 13]. It is given by p(A|S,Y) =N(A;Ak,Σk)with Ak=A0+∆A 0U⊺+U(∆A 0)⊺−US⊺∆A 0U⊺ Σk=WA 0(In−SU⊺) WA 0(In−SU⊺) where ∆A 0=Y−A0SandU=WA 0S(S⊺WA 0S)−1. We aim to construct a probabilistic model Hfor the inverse with the model Aas well. However, not even in the scalar case does the inverse of a Gaussian have ﬁnite mean. We ask instead what Gaussian model forHis as consistent as possible with our observational model for A. For a prior of the form p(H) =N(H;H0,WH 0 WH 0)and likelihood p(S|H,Y) =δ(S−HY), we analogously to the A-model obtain a posterior distribution p(H|S,Y) =N(H;Hk,ΣH k)with Hk=H0+∆H 0(UH)⊺+UH(∆H 0)⊺−UHY⊺∆H 0(UH)⊺ ΣH k=WH 0(In−Y(UH)⊺) WH 0(In−Y(UH)⊺) where ∆H 0Y(Y⊺WH 0Y)−1. In Section 3 we will derive a covariance class, which establishes correspondence between the two Gaussian viewpoints for the linear operator and its inverse and is consistent with our desiderata. 2.2 Algorithm The above inference procedure leads to Algorithm 1. The degree to which the desiderata are encoded in our formulation of a PLS can be found in Table 1. We will now go into more detail about the policy, the choice of step size, stopping criteria and the implementation. Policy and Step Size In each iteration our solver collects information about the linear operator A via actions sidetermined by the policy π(s|A,H,x,A,b). The next action si=−E[H]ri−1is 1See Sections S2 and S3 of the supplementary material for more detail on Kronecker-type products and matrix-variate normal distributions. 3 Algorithm 1: Probabilistic Linear Solver with Uncertainty Calibration 1procedure PROBLINSOLVE (A(·),b,A,H) # prior for AorH 2 x0←E[H]b # initial guess 3 r0←Ax0−b 4 while min(√ tr(Cov[ # stopping criteria 5 si←−E[H]ri−1 # compute action via policy 6 yi←Asi # make observation 7 αi←−s⊺ iri−1(s⊺ iyi)−1# optimal step size 8 xi←xi−1+αisi # update solution estimate 9 ri←ri−1+αiyi # update residual 10 A←INFER (A,si,yi) # infer posterior distributions 11 H←INFER (H,si,yi) # (see Section 2.1) 12 Φ,Ψ←CALIBRATE (S,Y) # calibrate uncertainty 13 x←N (xk,Cov[Hb]) # belief over solution 14 return (x,A,H) chosen based on the current belief about the inverse. If E[H] =A−1, i.e. if the solver’s estimate for the inverse equals the true inverse, then Algorithm 1 converges in a single step since ) =A−1b=x∗. The step size minimizing the quadratic q(xi+αsi) =1 ) along the action siis given byαi= arg minαq(xi+αsi) =s⊺ i(b−Axi)(s⊺ iAsi)−1. Stopping Criteria Classic linear solvers typically use stopping criteria based on the current residual of the relative and absolute tolerances δrtolandδatol. However, this residual may oscillate or even increase in all but the last step even if the error ∥x∗−xi∥2is monotonically decreasing [ 19,20]. From a probabilistic point of view, we should stop if our posterior uncertainty is sufﬁciently small. Assuming the posterior covariance is calibrated, it holds that ( 2] = tr(Cov[ x]). Hence given calibration, we can bound the expected (relative) error between our estimate and the true solution by terminating when√ tr(Cov[ ). A probabilistic criterion is also necessary for an extension to the noisy setting, where classic convergence criteria become stochastic. However, probabilistic linear solvers typically suffer from miscalibration [21], an issue we will address in Section 3. Implementation We provide an open-source implementation of Algorithm 1 as part of PROBNUM, a Python package implementing probabilistic numerical methods, in an online code repository: The mean and covariance up- and downdates in Section 2.1 when performed iteratively are of low rank. In order to maintain numerical stability these updates can instead be performed for their respective Cholesky factors [ 22]. This also enables efﬁcient sampling or evaluation of probability density functions downstream. 2.3 Theoretical Properties This section details some theoretical properties of our method such as its convergence behavior and computational complexity. In particular we demonstrate that for a speciﬁc prior choice Algorithm 1 recovers the method of conjugate gradients as its solution estimate. All proofs of results in this section and the next can be found in the supplementary material. We begin by establishing that our solver is a conjugate directions method and therefore converges in at most nsteps in exact arithmetic. Theorem 1 (Conjugate Directions Method) Given a prior p(H) =N(H;H0,WH 0 WH 0)such that H0,WH 0∈Rn×n sym positive deﬁnite, then actions siof Algorithm 1 are A-conjugate, i.e. for holds that s⊺ iAsj= 0. 4 We can obtain a better convergence rate by placing stronger conditions on the prior covariance class as outlined in Section 3. Given these assumptions, Algorithm 1 recovers the iterates of (preconditioned) CG and thus inherits its favorable convergence behavior (overviews in [23, 10]). Theorem 2 (Connection to the Conjugate Gradient Method) Given a scalar prior mean A0=H−1 0=αIwithα>0, assume (1)and(2)hold, then the iterates xiof Algorithm 1 are identical to the ones produced by the conjugate gradient method. A common phenomenon observed when implementing conjugate gradient methods is that due to cancellation in the computation of the residuals, the search directions [ 24,25,3]. In fact, they can become independent up to working precision for ilarge enough [ 25]. One way to combat this is to perform complete of the search directions in each iteration as originally suggested by Lanczos [26]. Algorithm 1 does this implicitly via its choice of policy which depends on all previous search directions as opposed to just si−1for (naive) CG. Computational Complexity The solver has time complexity without uncertainty calibration. Compared to CG, inferring the posteriors in Section 2.1 adds an overhead of four outer products and four matrix-vector products per iteration, given (1)and(2). Uncertainty calibration outlined in Section 3 adds between iteration depending on the sophistication of the scheme. Already for moderate nthis is dominated by the iteration cost. In practice, means and covariances do not need to be formed in memory. Instead they can be evaluated lazily as linear operators v↦→Lv, ifSandYare stored. This results in space complexity O(kn). 2.4 Related Work Numerical methods for the solution of linear systems have been studied in great detail since the last century. Standard texts [ 1,2,10,3] give an in-depth overview. The conjugate gradient method recovered by our algorithm for a speciﬁc choice of prior was introduced by Hestenes and Stiefel [19]. Recently, randomization has been exploited to develop improved algorithms for large-scale problems arising from machine learning [ 27,28]. The key difference to our approach is that we do not rely on sampling to approximate large-scale matrices, but instead perform probabilistic inference. Our approach is based on the framework of probabilistic numerics [ 14,15] and is a natural continuation of previous work on probabilistic linear solvers. In historical order, Hennig and Kiefel [18] provided a probabilistic interpretation of Quasi-Newton methods, which was expanded upon in [ 11]. This work also relied on the symmetric matrix-variate Gaussian as used in our paper. Bartels and Hennig [29] estimate numerical error in approximate least-squares solutions by using a probabilistic model. More recently, Cockayne et al. [21] proposed a Bayesian conjugate gradient method performing inference on the solution of the system. This was connected to the matrix-based view by Bartels et al. [13]. 3 Prior Covariance Class Having outlined the proposed algorithm, this section derives a prior covariance class which satisﬁes nearly all desiderata, connects the two modes of prior information and allows for calibration of uncertainty by appropriately choosing remaining degrees of freedom in the covariance. The third desideratum posited that AandHshould be almost surely positive deﬁnite. This evidently does not hold for the matrix-variate Gaussian. However, we can restrict the choice of admissable WA 0to act likeAonspan(S). This in turn induces a positive deﬁnite posterior mean. Proposition 1 (Hereditary Positive Deﬁniteness [30, 18]) LetA0∈Rn×n sym be positive deﬁnite. Assume the actions and WA 0S=Y, then holds that Ai+1is symmetric positive deﬁnite. Prior information about the linear system usually concerns the matrix Aitself and not its inverse, but the inverse is needed to infer the solution x∗of the linear problem. So a way to translate between a Gaussian distribution on AandHis crucial. Previous works generally committed to either one view or the other, potentially discarding available information. Below, we show that the two correspond, if we allow ourselves to constrain the space of possible models. We impose the following condition. Deﬁnition 1 LetAiandHibe the means of AandHat stepi. We say a prior induces posterior correspondence ifA−1 i=Hifor all 0≤i≤k. If only A−1 iY=HiY,weak posterior correspondence holds. 5 The following theorem establishes a sufﬁcient condition for weak posterior correspondence. For an asymmetric prior model one can establish the stronger notion of posterior correspondence. A proof is included in the supplements. Theorem 3 (Weak Posterior Correspondence) LetWH 0∈Rn×n sym be positive deﬁnite. Assume H0=A−1 0, and that WA 0,A0,WH 0satisfy WA 0S=Y, (1) S⊺(WA 0A−1 0−AWH 0) =0, (2) then weak posterior correspondence holds for the symmetric Kronecker covariance. Given the above, let A0be a symmetric positive deﬁnite prior mean and H0=A−1 0. Deﬁne the orthogonal projections PA Y=A−1 0Y(Y⊺A−1 0Y)−1Y⊺A−1 0 with respect to the inner products induced by AandA−1 0, as well as to the spaces )⊥. We propose the following prior covariance class given by the prior covariance factors WA 0=PA S+PS⊥ΦPS⊥andWH 0=PH0 Y+PY⊥ΨPY⊥, (3) where degrees of freedom. This choice of covariance class satisﬁes Theorem 1, Proposition 1, Theorem 3 and for a scalar mean also Theorem 2. Therefore, it produces symmetric realizations, has symmetric positive semi-deﬁnite means, it links the matrix and the inverse view and at any given time only needs access to . It is also compatible with a preconditioner by simply transforming the given linear problem. This class can be interpreted as follows. The derived covariance factor WA 0acts like Aon the space by the algorithm. On the remaining space its uncertainty is additionally determined by the degrees of freedom in Φ. Likewise, our best guess for A−1isA−1 0on the space spanned by Y. On the orthogonal space span(Y)⊥the uncertainty is also inﬂuenced by Ψ. Note that the prior depends on actions and observations collected during a run of Algorithm 1, hence one might call this an empirical Bayesian approach. This begs the question how the algorithm is realizable for the proposed prior (3)given its dependence on future data. Notice that the posterior mean in Section 2.1 only depends on WA 0S=YnotonWA 0alone. Using eq. (3), at iteration iwe have WA 0S1:i=Y1:i, i.e. the observations made up to this point. Similar reasoning applies for the inverse. Now, the posterior covariances do depend on WA 0, respectively WH 0alone, but prior to convergence we only require tr(Cov[ x])for the stopping criterion. We show in Section S4.3 under the assumptions of Theorem 2 how to compute this at any iteration iindependent of future actions and observations. Therefore prior to convergence of Algorithm 1 the covariance factors are never explicitly formed . Uncertainty Calibration Generally the actions of Algorithm 1 identify eigenpairs (λi,vi)in descending order of λiv⊺ ir0which is a well-known behavior of CG (see eqn. 5.29 in [ 10]). In part, since this dynamic of the underlying Krylov subspace method is not encoded in the prior, the solver in its current form is typically miscalibrated (see also [ 21]). While this non-linear information is challenging to include in the Gaussian framework, we can choose ΦandΨin(3)to empirically calibrate uncertainty. This can be interpreted as a form of hyperparameter optimization similar to optimization of kernel parameters in GP regression. We would like to encode prior knowledge about the way AandHact in the respective orthogonal spaces )⊥. For the Rayleigh quotient R(A,v) = (v⊺Av)(v⊺v)−1it holds ). Hence for vectors vlying in the respective null spaces of S andYour uncertainty should be determined by the not yet explored eigenvalues λk+1,...,λnof AandH. Without prior information about the eigenspaces, we choose Φ=φIandΨ=ψI. If a priori we know the respective spectra, a choice is φ=ψ−1=1 n−kn∑ i=k+1λi(A). In the absence of prior spectral information we can make use of already collected quantities during a run of Algorithm 1. We build a regression model p(lnRi|Y,S)for the ln- Rayleigh quotient lnR(A,si)given actions si. Such a model can then encode the well studied 6 0 20 40 60 80 100 120 [λmin(A),λmax(A)] GP posterior p(lnRi|Y,S) Rayleigh quotient R(A,si) Uncertainty scale φ=ψ−1Figure 2: Rayleigh regression. Uncer- tainty calibration via GP regression on {lnR(A,si)}k i=1afterk= 91 iterations of Algorithm 1 on an n= 1000 dimen- sional Mátern32 kernel matrix inversion problem. The degrees of freedom φ= ψ−1>0are set based on the average pre- dicted Rayleigh quotient for the remaining n−k= 909 2: Uncertainty calibration for kernel matrices. Monte Carlo estimate ¯ cal- ibration given 105/nsampled linear problems of the form (K+ε2I)x∗=bfor each kernel and calibration method. For ¯w≈0the solver is well calibrated, for ¯ and for ¯. Kernel n none Rayleigh ε2λk+1:n Matérn32 102−5.99−0.24 0.32 0.09 Matérn32 103−1.93 7.53 4.26 4.19 Matérn32 1043.87 17.16 8.48 8.47 Matérn52 Matérn52 103−4.63 1.43−0.80−0.81 Matérn52 104−4.34 10.81 0.80 0.80 RBF RBF 103−4.94 6.60 0.77 0.77 RBF 1040.14 21.32 2.92 2.92 behaviour of CG, whose Rayleigh coefﬁcients rapidly decay at ﬁrst, followed by a slower continuous decay [ 10]. Figure 2 illustrates this approach using a GP regression model. At convergence, we use the prediction of the Rayleigh quotient for the remaining n−kdimensions by choosing φ=ψ−1= exp( 1 n−kn∑ ]) , i.e. uncertainty about actions in span(S)⊥is calibrated to be the average Rayleigh quotient as an approximation to the spectrum. Depending on the application a simple or more complex model may be useful. For large problems, where generally k≪n, more sophisticated schemes become feasible. However, these do not necessarily need to be demanding due to the simple nature of this regression problem with few data. For example, approximate [31] or even exact GP regression [32] is possible in O(k)using a Kalman ﬁlter. 4 Experiments This section demonstrates the functionality of Algorithm 1. We choose some – deliberately simple – example problems from machine learning and scientiﬁc computation, where the solver can be used to quantify uncertainty induced by ﬁnite computation, solve multiple consecutive linear systems, and propagate information between problems. Gaussian Process Regression GP regression [ 7] infers a latent function f:RN→Rfrom data D= (X,y), where X∈Rn×Nandy∈Rn. Given a prior p(f) =GP(f; 0,k)with kernelkfor the unknown function f, the posterior mean and marginal variance at mnew inputs ˜x∈RN×mare E[˜f] =˜] = is the Gram matrix of the kernel and ˜k=k(X,˜x)∈Rn×m. The bulk of computation during prediction arises from solving the linear system (K+ε2I)z=bfor some right-hand side b∈Rn repeatedly. When using a probabilistic linear solver for this task, we can quantify the uncertainty arising from ﬁnite computation as well as the belief of the solver about the shape of the GP at a set of not yet computed inputs. Figure 3 illustrates this. In fact, we can estimate the marginal variance of the GP without solving the linear system again by multiplying ˜kwith the estimated inverse of K+ε2I. In large-scale applications, we can trade off computational expense for increased uncertainty arising from the numerical approximation and quantiﬁed by the probabilistic linear solver. By assessing the numerical uncertainty arising from not exploring the full space, we can judge the quality of the estimated GP mean and marginal variance. Kernel Gram Matrix Inversion Consider a linear problem Kx∗=b, where Kis generated by a Mercer kernel. For a ν-times continuously differentiable kernel the eigenvalues λn(K)decay 7 −505yk= 2 k= 6 k= 10 data E[f] EH[E[˜f]] 2 sd H[E[˜f]] −4−2 0 2 4 x024y−E[f] −4−2 0 2 4 x−4−2 0 2 4 xV[f] EH[V[˜f]] 2 sd H[V[˜f]]Figure 3: Numerical uncertainty in GP inference. Computing posterior mean and covariance of a GP regression using a PLS. Top: GP mean for a toy data set ( n= 16 ) computed with increasing number of iterations kof Algorithm 1. The numerical estimate of the GP mean approaches the true mean. Note that the numerical variance is different from the marginal variance of the GP. Bottom: GP variance and estimate of GP variance with numerical uncertainty. The GP variance estimate is computed using the estimated inverse from computing E[˜f]without any additional solver iterations . approximately as|λn|∈O (n−ν−1 2)[33]. We can make use of this generative prior information by specifying a parametrized prior mean µ(n) = ln( θ′ 0n−θ1) =θ0−θ1ln(n)for the ln-Rayleigh quotient model. Typically, such Gram matrices are and therefore K′=K+ε2Iis used instead, implying λ(K′)i≥ε2. In order to assess calibration we apply various differentiable kernels to the airline delay dataset from January 2020 [ 34]. We compute the ln-ratio statistic w(x∗) =1 2ln(tr(Cov[ no calibration, calibration via Rayleigh quotient GP regression using µ(n)as a prior mean, calibration by setting φ=ε2and calibration using the average spectrum φ=λk+1:n. The average ¯ sampled test problems is shown in Table 2.2Without any calibration the solver is generally overconﬁdent. All tested calibration procedures reverse this, resulting in more cautious uncertainty estimates. We observe that Rayleigh quotient regression overcorrects for larger problems. This is due to the fact that its model correctly predicts Kto be numerically singular from the dominant Rayleigh quotients, however it misses the information that the spectrum of K′is bounded from below by ε2. If we know the (average) of the remaining spectrum, signiﬁcantly better calibration can be achieved, but often this information is not available. Nonetheless, since in this setting the majority of eigenvalues satisfy λ(K′)i≈ε2by , we can get to the same degree of calibration. Therefore, we can improve the solver’s uncertainty calibration at constant cost O(1)per iteration. For more general problems involving Gram matrices without damping we may want to rely on Rayleigh regression instead. Galerkin’s Method for PDEs In the spirit of applying machine learning approaches to problems in the physical sciences and vice versa [ 35], we use Algorithm 1 for the approximate solution of a PDE via Galerkin’s method [9]. Consider the Dirichlet problem for the Poisson equation given by {−∆u(x,y) =f(x,y) (x,y)∈int Ω u(x,y) =u∂Ω(x,y) (x,y)∈∂Ω where Ωis a connected open region with sufﬁciently regular boundary and u∂Ω:∂Ω→Rdeﬁnes the boundary conditions. One obtains an approximate solution by projecting the weak formulation of the PDE to a ﬁnite dimensional subspace. This results in the Galerkin equation Au=f, i.e. a linear system where Ais the Gram matrix of the associated bilinear form. Figure 4 shows the induced uncertainty on the solution of the Dirichlet problem for f(x,y) = 15 andu∂Ω(x,y) = (x2−2y)2(1 + sin(2πx)). The mesh and corresponding Gram matrix were computed using FENICS [36]. We can exploit two properties of Algorithm 1 in this setting. First, if we need to solve multiple related problems (Aj,fj)j, by solving a single problem we obtain an estimate of the solution to all other problems. We can successively use the posterior over the inverse as a prior for the next problem. This approach is closely related to subspace recycling in numerical linear algebra [ 37,38]. 2We decrease the number of samples with the dimension because forming dense kernel matrices in memory and computing their eigenvalues becomes prohibitive – notbecause of the cost of our solver. 8 u(a) Ground truth E[u] u1 u2 (b) Solution mean & samples u−E[u] - (c) Signed error Cov[u]−1 2(u−E[u]) - (d) Uncertainty calibration Figure 4: Solving the Dirichlet problem with a probabilistic linear solver. Figures 4a and 4b show the ground truth and mean of the solution computed with Algorithm 1 after k= 23 iterations along with samples from the posterior. The posterior on the coarse mesh can be used to assess uncertainty about the solution on a ﬁner mesh. The signed error computed on the coarse mesh in Figure 4c shows that the approximation is better near the top boundary of Ω. Given perfect uncertainty calibration, Figure 4d represents a sample from N(0,I). The apparent structure in the plot and smaller than expected deviations in the upper part of Ωindicate the conservative conﬁdence estimate of the solver. Second, suppose we ﬁrst compute a solution in a subspace corresponding to a coarse discretization for computational efﬁciency. We can then leverage the estimated solution to extrapolate to an (adaptively) reﬁned discretization based on the posterior uncertainty. In machine learning lingo these two approaches can be viewed as forms of transfer learning . 5 Conclusion In this work, we condensed a line of previous research on probabilistic linear algebra into agit st self-contained algorithm for the solution of linear problems in machine learning. We proposed ﬁrst principles to constrain the space of possible generative models and derived a suitable covariance class. In particular, our proposed framework incorporates prior knowledge on the system matrix or its inverse and performs inference for both in a consistent fashion. Within our framework we identiﬁed parameter choices that recover the iterates of conjugate gradients in the mean, but add calibrated uncertainty around them in a lightweight manner. To our knowledge our solver, available as part of the PROBNUMpackage, is the ﬁrst practical implementation of this kind. In the ﬁnal parts of this paper we showcased applications like kernel matrix inversion, where prior spectral information can be used for uncertainty calibration and outlined example use-cases for propagation of numerical uncertainty through computations. Naturally, there are also limitations remaining. While our theoretical framework can incorporate noisy matrix-vector product evaluations into its inference procedure via a Gaussian likelihood, practically tractable inference in the inverse model is more challenging. Our solver also opens up new research directions. In particular, our outlined regression model on the Rayleigh quotient may lead to a probabilistic model of the eigenspectrum. Finally, the matrix-based view of probabilistic linear solvers could inform probabilistic approaches to matrix decompositions, analogous to the way Lanczos methods are used in the classical setting. Broader Impact Our research on probabilistic linear solvers is primarily aimed at members of the machine learning ﬁeld working on uncertainty estimation which use linear solvers as part of their toolkit. We are convinced that numerical uncertainty induced by ﬁnite computational resources is a key missing component to be quantiﬁed in machine learning settings. By making numerical uncertainty explicit like our solver does, holistic probabilistic models incorporating all sources of uncertainty become possible. In fact, we hope that this line of work stimulates further research into numerical linear algebra for machine learning, a topic that has been largely considered solved by the community. This is ﬁrst and foremost a methods paper aiming to improve the quantiﬁcation of numerical uncer- tainty in linear problems. While methodological papers may seem far removed from application and questions of ethical and societal impact, this is not the case. Precisely due to the general nature of the problem setting, the linear solver presented in this work is applicable to a broad range of applications, 9 from regression on ﬂight data, to optimization in robotics, to the solution of PDEs in meteorology. The ﬂip-side of this potential impact is that arguably, down the line, methodological research suffers from dual use more than any specialized ﬁeld. While we cannot control the use of a probabilistic linear solver due to its general applicability, we have tried, to the best of our ability, to ensure it performs as intended. We are hopeful that no speciﬁc population group is put at a disadvantage through this research. We are providing an open-source implementation of our method and of all experiments contained in this work. Therefore anybody with access to the internet is able to retrieve and reproduce our ﬁndings. In this manner we hope to adress the important issues of accessibility and . and Disclosure of Funding The authors gratefully acknowledge ﬁnancial support by the European Research Council through ERC StG Action 757275 / PANAMA; the DFG Cluster of Excellence “Machine Learning - New Perspectives for Science”, EXC 2064/1, project number 390727645; the German Federal Ministry of Education and Research (BMBF) through the Tübingen AI Center (FKZ: 01IS18039A); and funds from the Ministry of Science, Research and Arts of the State of . JW is grateful to the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for support. We thank the reviewers for helpful comments and suggestions. JW would also like to thank Alexandra Gessner and Felix Dangel for a careful reading of an earlier version of this manuscript.  
retrieval augmented generation for knowledge intensive nlp tasks 	 Generation for NLP Tasks Patrick Lewis†‡, Ethan Perez?, Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†, Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela† †Facebook AI College London;?New York University; plewis@fb.com Abstract Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on tasks, their perfor- mance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research prob- lems. Pre-trained models with a differentiable access mechanism to explicit non- parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a ﬁne-tuning recipe for generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a seq2seq baseline. 1 Introduction Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl- edge from data [ 47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [ 51,52]. While this development is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t provide insight into their predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric memory with non-parametric (i.e., ) memories [ 20,26,48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that combine masked language models [ 8] with a differentiable retriever, have shown promising results, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.  Backprop through T VeUiÀcaWiRQ: FacW ) QXeVWiRQ GeQeUaWiRQFacW \ QXeU\Figure 1: Overview of our approach. We combine a pre-trained retriever ( Query Encoder +Document Index ) with a pre-trained seq2seq model ( Generator ) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treat zas a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the “workhorse of NLP,” i.e. (seq2seq) models. We endow pre-trained, generation models with a non-parametric memory through a ﬁne-tuning approach which we refer to as generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [ 32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [ 64,55], stack- augmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training. Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera- tion for tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve results on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [ 24]. Despite these being extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches. For generation, we experiment with MS-MARCO [ 1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FEVER [ 56] fact veriﬁcation, we achieve results within 4.3% of pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1 2 Methods We explore RAG models, which use the input sequence xto retrieve text documents zand use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever p⌘(z|x)with parameters ⌘that returns (top-K truncated) distributions over text passages given a query xand (ii) a generator p✓(yi|x, z, y 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at / examples/rag/ . An interactive demo of RAG models can be found at / 2 by✓that generates a current token based on a context of the previous i 1tokens y1:i 1, the original input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence , the model uses the same document to predict each target token. The second approach, RAG-Token , can predict each target token based on a different document. In the following, we formally introduce both models and then describe the , as well as the training and decoding procedure. 2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence . Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x)via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence (y|x)⇡X , z)=X ip✓(yi|x, z, y 1:i 1) RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deﬁne: pRAG-Token (y|x)⇡NY iX , z i,y1:i 1) Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component p⌘(z|x)is based on DPR [26]. DPR follows a bi-encoder architecture: p⌘(z|x)/exp  d(z)>q(x)  d(z)=BERT d(z),q(x)=BERT q(x) where d(z)is a dense representation of a document produced by a BERT BASE document encoder [8], andq(x)a query representation produced by a query encoder , also based on BERT BASE. Calculating top-k (p⌘(·|x)), the list of kdocuments zwith highest prior probability p⌘(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [ 24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory . 2.3 Generator: BART The generator component p✓(yi|x, z, y 1:i 1)could be modelled using any . We use BART-large [ 32], a pre-trained seq2seq transformer [ 58] with 400M parameters. To combine the input xwith the retrieved content zwhen generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained results on a diverse set of generation tasks and outperforms T5 models [32]. We refer to the BART generator parameters ✓as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj,yj), we 3 minimize the negative marginal log-likelihood of each target,P stochastic gradient descent with Adam [ 28]. Updating the document encoder BERT dduring training is costly as it requires the document index to be periodically updated as REALM does during pre-training [ 20]. We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and index) ﬁxed, only ﬁne-tuning the query encoder BERT qand the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxyp(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p0 ✓(yi|x, y 1:i 1)=P , z i,y1:i 1)To decode, we can plug p0 ✓(yi|x, y 1:i 1)into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x)does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using p✓(yi|x, z, y 1:i 1). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis ywe run an additional forward pass for each document zfor which ydoes not appear in the beam, multiply generator probability with p⌘(z|x)and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, |Y|can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that p✓(y|x, z i)⇡0where ywas not generated during beam search from x, z i. This avoids the need to run additional forward passes once the candidate set Yhas been generated. We refer to this decoding procedure as “Fast Decoding.” 3 Experiments We experiment with RAG in a wide range of tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31]and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [ 23] with a Hierarchical Navigable Small World approximation for fast retrieval [ 37]. During training, we retrieve the top kdocuments for each query. We consider k2{5,10}for training and set kfor test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for tasks [ 20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [ 5,7,31,26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA” approaches [ 52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [ 29], TriviaQA (TQA) [ 24]. WebQuestions (WQ) [ 3] and CuratedTrec (CT) [ 2]. As CT and WQ are small, we follow DPR [ 26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [ 31,26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a setting, we use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the weather in Volcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging generation task. We use the splits from SearchQA [ 10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [ 67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [ 42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external sources, and speciﬁcity as high mutual dependence between the input and output [ 33]. We follow best practice and use pairwise comparative evaluation [ 34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriﬁcation FEVER [ 56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiﬁcation task ( enough info) and the 2-way () task studied in Thorne and Vlachos [57]. In both cases we report label accuracy. 4 Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of "open-book" approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [ 20]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross- encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Model NQ TQA WQ CT Closed BookT5-11B [52] 34.5 - /50.1 37.4 - T5-11B+SSM[52] 36.6 - /60.5 44.7 - Open BookREALM [20] 40.4 - / - 40.7 46.8 DPR [26] 41.5 57.9/ - 41.1 50.6 RAG-Token 44.1 55.2/66.1 45.5 50.0 RAG-Seq. 44.5 56.8/ 68.0 45.2 52.2Table 2: Generation and classiﬁcation Test Scores. MS-MARCO SotA is [ 4], FEVER-3 is [ 68] and FEVER-2 is [ 57] *Uses gold . Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8*49.9*76.8 92.2 * BART 15.1 19.7 38.2 41.6 64.0 81.1 RAG-Tok. 17.3 22.2 40.1 41.572.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2 to more effective over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches model performance, which is impressive given that (i) those models access gold passages with speciﬁc information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. Table 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a generation model. Evaluators also ﬁnd RAG generations to be more speciﬁc by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating “Sun”, the posterior is high for document 2 which mentions “The Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens. This observation suggests that the generator can complete the titles without depending on speciﬁc documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The Sun. BART completes the generation "The SunAlso Rises" isanovel bythis author of"The Sun Also Rises" indicating the title "The Sun Also Rises" is stored in BART’s parameters. Similarly, BART will complete the partial decoding "The SunAlso Rises" isanovel bythis author of"A with "The SunAlso Rises" isanovel bythis author of"AFarewell toArms" . This example shows how parametric and non-parametric memories work together —the non-parametric component helps to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory. 4.4 Fact Veriﬁcation Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of models, which are complex pipeline systems with domain-speciﬁc architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 Document 1: his works are considered classics of ... His wartime experiences formed the basis for his novel”A Farewell to Arms”(1929) ...Document 2: ... artists of the 1920s ”Lost Generation” . His debut novel,”The Sun Also Rises”, was publishedin 1926. 1Doc 2Doc 3Doc 4Doc 5Figure 2: RAG-Token document posterior p(zi|x, y i,y i)for each generated token for input “Hem- ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating “A Farewell to Arms" and for document 2 when generating “The Sun Also Rises". Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation MS- MARCOdeﬁne middle earBART?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. what currency needed in scotlandBART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed in Scotland is the pound sterling. Jeopardy Question Gener - state has the largest number of counties in the U.S. RAG-T It’s the only U.S. state named for a U.S. president RAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park The Divine epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio RAG-T Dante’s "Inferno" is the ﬁrst part of this epic poem RAG-S This 14th century work is divided into 3 sections: "Inferno", "Purgatorio" & "Paradiso" For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top kdocuments retrieved by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases. 4.5 Additional Results Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than BART for Jeopardy question generation. Following recent work on decoding [33,59,39], we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing any decoding. Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks. We compare RAG’s dense retriever to a word overlap-based BM25 retriever [ 53]. Here, we replace RAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 show the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial. Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA [ 5] Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed between these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”) 7 Table 4: Human assessments for the Jeopardy Question Generation Task. Factuality Speciﬁcity BART better 7.1% 16.8% RAG better 42.7% 37.4% Both good 11.7% 11.8% Both poor 17.7% 6.9% No majority 20.8% 20.1%Table 5: Ratio of distinct to total tri-grams for generation tasks. MSMARCO Jeopardy QGen Gold 89.6% 90.0% BART 70.7% 32.4% RAG-Token 77.8% 46.8% RAG-Seq. 83.5% 53.8% Table 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent. Model NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2 Exact Match B-1 QB-1 R-L B-1 Label Accuracy RAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.475.1 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.472.9 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3 RAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.474.5 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5 to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory. Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe signiﬁcant differences in performance between them. We have the ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence. 1020304050KR e t r i e v e dD o c Exact e t r i e v e dD o c s4050607080NQ Answer Recall @ e t r i e v e dD o c / Rouge-L scoreRAG-Tok R-LRAG-Tok B-1RAG-Seq R-LRAG-Seq B-1Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor- mance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved. 5 Related Work Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [ 36], dialogue [ 41,65,9,13], translation [ 17], and language modeling [ 19,27]. Our work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single architecture is capable of achieving strong performance across several tasks. 8  Architectures for NLP Prior work on architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench- marks [ 60,61] after ﬁne-tuning [ 49,8]. GPT-2 [ 50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [ 32] and T5 [ 51,52] propose a single, pre-trained model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed architecture, by learning a retrieval module to augment pre-trained, generative language models. Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [ 44,26] similar to ours. Some work optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering, using search [ 46], reinforcement learning [ 6,63,62], or a latent variable approach [ 31,20] as in our work. These successes leverage different architectures and optimization techniques to achieve strong performance on a single task, while we show that a single architecture can be ﬁne-tuned for strong performance on a variety of tasks. Memory-based Architectures Our document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [ 64,55]. Concurrent work [ 14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [ 9,13] or, closer to our work, over retrieved text directly [ 15]. A key feature of our memory is that it is comprised of raw text rather distributed , which makes the memory both (i) human-readable, lending a form of to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. approaches Our method shares some similarities with style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a ﬁnal output. These approaches have proved successful in a number of domains including Machine Translation [ 18,22] and Semantic Parsing [ 21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work. 6 Discussion In this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual and speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks. 9 Broader Impact This work offers several positive societal beneﬁts over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and . RAG could be employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs. With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [ 54]. Advanced language models may also lead to the automation of various jobs in the coming decades [ 16]. In order to mitigate these risks, AI systems could be employed to ﬁght against misleading content and automated spam/phishing. The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. Funding Disclosure EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program. This work was funded by Facebook.  
sequence to multi sequence learning via conditional chain mapping for mixture signals 	Sequence to Multi-Sequence Learning via Conditional Chain Mapping for Mixture Signals Jing Shi1,2,∗, Xuankai Chang1,∗, Pengcheng Guo1,3,∗, Shinji Watanabe1†, Yusuke Fujita4, Jiaming Xu2,Bo Xu2,Lei Xie3 1Center for Language and Speech Processing, Johns Hopkins University, U.S.A 2Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, China 3ASLP@NPU, School of Computer Science, Northwestern Polytechnical University, Xi’an, China 4Hitachi, Ltd. Research & Development Group, Japan , , , B, Abstract Neural models are well established for applications which can be cast as mapping a single input sequence into a single output sequence. In this work, we focus on one-to-many sequence transduction problems, such as extracting multiple sequential sources from a mixture sequence. We extend the standard model to a conditional multi-sequence model, which explicitly models the relevance between multiple output sequences with the probabilistic chain rule. Based on this extension, our model can conditionally infer output sequences one-by-one by making use of both input and contextual output sequences. This model additionally has a simple and efﬁcient stop criterion for the end of the transduction, making it able to infer the variable number of output sequences. We take speech data as a primary test ﬁeld to evaluate our methods since the observed speech data is often composed of multiple sources due to the nature of the superposition principle of sound waves. Experiments on several different tasks including speech separation and multi-speaker speech recognition show that our conditional multi-sequence models lead to consistent improvements over the conventional models. 1 Introduction Many machine learning tasks can be formulated as a sequence transduction problem, where a system provides an output sequence given the corresponding input sequence. Examples of such tasks include machine translation, which maps text from one language to another, automatic speech recognition (ASR), which receives a speech waveform and produces a transcription, and video captioning, which generates the descriptions of given video scenes. In recent years, the development of neural sequence- to-sequence (seq2seq) models [ 9,40] with attention mechanisms has led to signiﬁcant progress in such tasks [2, 51, 44, 48, 10, 5]. In reality, the observed data contains various entangled components, making the one-to-many se- quence transduction for mixture signals a common problem in machine learning [ 37,18,32]. This problem often happens in audio and speech processing due to the sequential properties and superposi- tion principle of sound waves. For example, given the overlapped speech signal, speech separation is a problem of extracting individual speech sources, and multi-speaker speech recognition is a problem ∗equal contribution †corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. of decoding transcriptions of individual speakers. This type of problem is called the cocktail party problem [ 8,3]. The existing methods to tackle this common problem can be roughly divided into two categories according to the correlation strength of multiple output sequences: serial mapping and parallel mapping. Serial mapping aims to learn the mappings through Input SequenceOutput Sequence 1Output Sequence 2Input SequenceOutput Sequence 1 Output Sequence 2(a) Serial Mapping Input SequenceOutput Sequence 1 Output Sequence 2e.g., wav speech recognition machine translation text(b) Parallel Mapping e.g., mixture wav separated sources(c) Conditional Chain Mapping Figure 1: mapping approaches. (a) and (b) shows the serial mapping and parallel mapping used by existing methods respectively, and (c) refers to our Conditional Chain mapping strategy. a forward pipeline, as shown in Figure 1(a). With the serial form, the output sequence of the ﬁrst seq2seq model is fed into the following seq2seq model to output another sequence. This method is quite common when the logic and relationship between different output sequences are . For example, a cross-lingual speech translation system contains two components: speech recognition and machine translation. Serial mapping ﬁrst recognizes the speech into the source language text and then use another model to translate it into the target language text. However, serial mapping methods usually suffer from some drawbacks. First, many of them need to be trained separately for different components, without taking advantage of the raw input information in the latter components. And the error accumulation through the pipeline will make the system suboptimal. The other category is parallel mapping, as shown in Figure 1(b), which simultaneously outputs multiple sequences. This method is often used when the outputs are from the same domain. Speech separation and multi-speaker ASR are typical examples following this paradigm. Similar to serial mapping, parallel mapping could not effectively model the inherent relationship that exists between different outputs, and usually assumes the number of the output sequence is ﬁxed (e.g., the ﬁxed number of speakers in speech separation tasks), which limits its application scenarios. In this paper, we propose a new uniﬁed framework aiming at the (seq2Mseq) transduction task, which can address the disadvantages of both the serial mapping and parallel mapping methods. For clarity, we refer to our methods as Conditional Chain (Cond- Chain) model, combining both the serial mapping and parallel mapping with the probabilistic chain rule. Simultaneous modeling for these two methods not only makes the framework more ﬂexible but also encourages the model to automatically learn the efﬁcient relationship between multiple outputs. To instantiate the idea, as shown in Figure 1(c), we assume that the input sequence Ocan be mapped intoNdifferent sequences si,i∈{1,..,N}. We take sequence Oas the primary input for every output sequence. Meanwhile, the outputs will be generated one-by-one with the previous output sequence as a conditional input. We consider that the multiple outputs from the same input have some relevance at the information level. By combining both the serial and parallel connection, our model learns the mapping from the input to each output sequence as well as the relationship between the output sequences. In this paper, we introduce the general framework in Section 2, and present a speciﬁc implementation for the tasks of speech separation and recognition in Section 3. We discuss some related work in Section 4 and describe our experiments in Section 5, and ﬁnally conclude in Section 6. Our source code and Supplementary Material could be available on our webpage: . github.io/ . 2 General framework We assume that the input sequence O∈O with length of Tcan be mapped into Ndifferent sequences si,i∈{1,..,N}, where the output index irepresents a particular domain Di. All the output sequences form a set }}. The basic formulation of our strategy is to estimate the joint probability of multiple output sequences, i.e., p(S|O). The joint probability is factorized into the 2 product of conditional probabilities by using the probabilistic chain rule with/without the conditional independence assumption (denoted by ·), as follows: p(S|O) =  p(s1|O)∏N i−2,...,s 1)serial mapping∏N 1) parallel mapping∏N i=1p(si|O,s i−1,...,s 1) conditional chain mapping(1) where, we also present the formulation of serial mapping and parallel mapping for comparison. As shown in Eq. 1, the serial mapping methods adopt a ﬁxed order and the conditional distributions are only constructed with sequence from the previous step, i.e., p(S|O) =p(s1|O)∏N i=2p(si|si−1). As a contrast, parallel mapping simpliﬁes the joint distribution by the conditional independence assumption, which means all the output sequences are only conditioned on the raw input, i.e., p(S|O) =∏p(si|O). For our conditional chain mapping, we manage to explicitly model the inherent relevance from the data, even if it seems very independent intuitively. To achieve this, we depart from the conditional independence assumption in parallel mapping or the Markov assumption in serial mapping. Instead, with the probabilistic chain rule, our method models the joint distribution of output sequences over an input sequence Oas a product of conditional distributions. We can also apply the same methodology to the regression output (e.g., speech separation). In our model, each distribution 1)in Eq. 1 is represented with a conditional encoder- decoder structure. Different from the conventional one-to-one sequence transduction for learning the mappingO↦→D i, additional module in our model preserves the information from previous target sequences and takes it as a condition for the following targets. This process is formulated as follows: Ei= Encoder i(O)∈RDE i×TE i, (2) Hi= CondChain( Ei,ˆ si−1)∈RDH i×TH, (3) ˆ si= Decoder i(Hi)∈DTi i, (4) where, all the Disymbols are the number of dimensions for the features, and TE i,TH i,Tirepresent the size of temporal dimension. In the above equations, Encoder iandDecoder irefer to the speciﬁc networks designed for learning the mapping for the reference sequence si. Note that the Encoder i andDecoder ihere may also consist of linear layers, attention mechanism or other neural networks besides the standard RNN layer, so the lengths of the hidden embeddings Eiand the estimation sequence ˆ simay vary from the input, i.e., TE i,TH i,Timay not equal the T. For thei-th output, theEiin Encoder gets a length of TE iwhile the ˆ sishould get the same length with the reference si∈DTi i, whereTiis the length of the sequence sifrom domainDi. Different from the conventional seq2seq model, we utilize a conditional chain ( CondChain in Eq. 3) to store the information from the previous sequences and regard them as conditions. This conditional chain is analogous to the design of memory cell in the LSTM model and the key component to realize Figure 1(c). Similarly, the conditional chain in Eq. 3 does not serve a speciﬁc target domain alone, it models some uniﬁed information for multi-sequence outputs. In other words, the is specialized for each target sequence, but the conditional chain is shared by all the transduction steps i. For most situations, when the logic and relationship between different output sequences is straightfor- ward, we could set a ﬁxed ordering of the outputted sequence, like the cross-lingual speech translation showed in Figure 1(a). Differently, for the outputs from the same domain, i.e., Di=Dj,i̸=j, theEncoder andDecoder for each step could be shared with the same architecture and parameters, which yields less model parameters and better efﬁciency for training. 3 Implementation for speech processing This section describes our implementation of the proposed conditional chain model by using speciﬁc multi-speaker speech separation / recognition tasks as examples. Both of them are typical examples of seq2Mseq tasks with input from mixture signals. 3.1 Basic model Multi-speaker speech separation / recognition aims at isolating individual speaker’s voices from a recording with overlapped speech. Figure 2 shows the network structure under our conditional chain 3 Silent Wav Enc Enc Enc LSTM LSTM LSTM Mixture WavDec Dec Dec STOPPrediction 1 Prediction 2 Prediction 3 Enc Connection (a) Conditional Chain Model for speech 𝐇𝐇𝟐𝟐 𝐄𝐄Embed Embed Embed LSTM LSTM LSTM Mixture WavDec Dec 1 Prediction 2 Enc 𝑂𝑂𝐇𝐇𝟏𝟏 𝐇𝐇𝟐𝟐 𝐄𝐄 (b) Conditional Chain Model for multi-speaker 3 𝒔𝒔0 Fusion Fusion Fusion Fusion Fusion Fusion CondChain𝒔𝒔0 Token Seq 1 Token Seq 2 <blank> Seq <blank> SeqFigure 2: mapping with conditional model for multi speaker speech separation or recognition. In each sub-ﬁgure, the block with same name are all shared. mapping for these two tasks. For both of them, the input sequence is from the speech domain. Let us ﬁrst assume this to be an audio waveform O∈RT. Another common feature for these two tasks lies in that the output sequences are from the same domain ( Di=Dj,i̸=j), which means we could use a shared model at each step, i.e., Encoder iin Eq. 2 and Decoder iin Eq. 4 are respectively the same networks with different i. For speech separation, the target sequences si∈RTare all from the same domain as the input, i.e.,Di=O, and with the same length of the input mixture. Thus, we could use an identical basic Encoder (Enc in Figure 2(a)) to extract acoustic features from the input waveform Oand predicted waveform ˆsi. As a contrast, multi-speaker speech recognition outputs a predicted token sequence si∈VTwith token vocabulary V. We introduce an additional embedding layer, Embed , to map these predicted tokens as continuous , which is used for conditional representation. In speech separation, as illustrated in Figure 2(a), both the mixed audio and the predicted source will go through an Encoder (Enc) to extract some basic auditory features. For the mixture waveform O, another separator ( Separator ) will also be used as the function to learn some hidden representation which is suitable for separation. And both the Enc and the Separator form the process in Eq. 2. For theFusion block, due to the same lengths from input and output, a simple concatenation operation is used to stack the feature dimension for each frame. For the CondChain in Eq. 3, we use a unidirectional LSTM. At each source step i, the Decoder ( Dec) is used to map the hidden state Hi into the ﬁnal separated speech source. Multi-speaker ASR is also performed in a similar form, as illustrated in Figure 2(b). Note that we use connectionist temporal classiﬁcation (CTC) [ 14] as a multi-speaker ASR network, since CTC is simple but yet powerful end-to-end ASR, and also the CTC output tokens without removing blank and repetition symbols can have the same length with the auditory feature sequence. Thus, we can realize the Fusion processing with a simple concatenation operation, similarly to speech separation. 3.2 Stop criterion One beneﬁt of the conventional seq2seq model is the ability to output a sequence by predicting the end of the sequence ( ⟨EOS⟩symbol) as a stop criterion. This advantage is inherited in our model to tackle the variable numbers of multiple sequences. For example, current speech separation or recognition models are heavily depending on a ﬁxed number of speakers [ 21] or require extra clustering steps [ 15]. Thanks to the introduction of the above stop criterion, we can utilize the mixture data with various numbers of speakers during training, and can be applied to the case of unknown numbers of speakers during inference. In our implementation, when we have the total number of output sequences as N, we attach an extra sequence to reach the stop condition during training. The target of this last sequence prediction for both speech separation and recognition tasks must be the silence, and we use the silent waveform and silent symbol (an entire ⟨blank⟩label sequence in CTC), respectively. 4 For different tasks, the stop criterion should correspond to the form of target sequences. In speech separation task, we set the stop criterion as the prediction of silent waveform, implying that there is no more speech left. Similarly, in multi-speaker recognition, we encourage the last predicted utterance as , which is a higher dimensional ⟨EOS⟩used by seq2seq model. More explicitly, we use the average energy to determine pure silence in separation task and average posterior of <blank> label in ASR task to determine the end of prediction. 3.3 Training strategy with and ordering Like the conventional seq2seq approach [ 2], we use a popular [ 47] technique by exploiting the ground-truth reference as a conditional source si−1. provides proper guidance and makes training more efﬁcient, especially at the beginning of the training, when the model is not good enough to produce reasonable estimation. Considering the unordered nature of multiple sources in multi-speaker speech separation or recognition, we adopt a greedy search method to choose the appropriate permutation of the reference sequences. This method achieves good performance in practice while maintaining high efﬁciency. More details about and reference permutation search could be found in Section B in the Supplementary Material. 4 Related Work Speech Separation As the core part of the cocktail party problem [ 8], speech separation draws much attention recently [ ]. The common design of this task is to disentangle overlapped speech signals from a mixture speech with a ﬁxed number of speakers, which is a typical example of the problem. Most existing approaches in this area follow the paradigm mentioned in Section 1, trained with permutation invariant training (PIT) technique [ 53]. This design should know the number of speakers in advance and could only tackle the data with the same number of speakers [ 36]. These constraints limit their application to real scenes, while our proposed structure can provide a solution to the variable and unknown speaker number issues. This study is inspired by recurrent selective attention networks (RSAN) [ 20], which has been proposed to tackle the above variable number of speakers in speech separation by iteratively subtracting a source spectrogram from a residual spectrogram. Similar ideas have also been proposed in time-domain speech separation [41] and speaker diarization [12]. However, the RSAN is based on the strong assumption of acoustic spectral subtraction in the time-frequency domain, and its application is quite limited. On the other hand, our conditional chain model reformulates it as a general sequence to multi-sequence transduction problem based on the probabilistic chain rule, which is applicable to the other problems including multi-speaker ASR than time-frequency domain speech separation. In addition, the relevance between current estimation and the former is learned by a conditional chain network in Eq. 3, which is more ﬂexible and even applied to time-domain speech separation, making it totally end-to-end transduction from waveform to waveforms. Multi-speaker speech recognition Multi-speaker speech recognition [ 46,52,33,35], which aims to directly recognize the texts of each individual speaker from the mixture speech, has recently become a hot topic. Similar to the speech separation task, most of the previous methods follow the parallel mapping paradigm mentioned in Section 1. These methods could only tackle the data with the ﬁxed number of speakers and require external speaker counting modules (e.g., speaker diarization [42, 1, 34]), which lose an end-to-end transduction function, unlike our proposed method. 5 Experiments We tested the effectiveness of our framework with speech data as our primary testing ground, where the sequence mapping problem is quite common and important. To be speciﬁc, the following sections describe the performance of our conditional chain model towards multi-speaker speech separation and speech recognition tasks, compared to other baselines. Furthermore, we also evaluated a joint model of speech separation and recognition, using multiple conditions from both waveform and text domains. In the Section A of Supplementary Material, we provide the implementation details about all our experiments, and we also extend our model to one iterative speech denoising task in Section D. 5 (c) Mixture Sample 2 of Four Speakers (d) Spectrogram of Mixture Sample 2Spk1 Spk2 Spk3 Spk4(a) Mixture Sample 1 of Four Speakers (b) Spectrogram of Mixture Sample 1 (mix2-1) Estimation of Spk 1 from Mixture Sample 2 (mix2-3) Estimation of Spk 3 from Mixture Sample 2(mix1-1) Estimation of Spk 1 from Mixture Sample 1 (mix1-3) Estimation of Spk 3 from Mixture Sample 1Spk1 Spk2 Spk3 Spk4Amplitude Frequency Amplitude Frequency Frequency Frequency Frequency ) Estimation of Spk 2 from Mixture Sample 2 (mix2-4) Estimation of Spk 4 from Mixture Sample 2(mix1-2) Estimation of Spk 2 from Mixture Sample 1 (mix1-4) Estimation of Spk 4 from Mixture Sample 1Frequency Frequency Frequency Frequency Time Time Time Figure 3: Visualization of two examples (mix1&mix2) with 4-speaker mixture from our WSJ0-4mix testset. For the mixture, both the waveform and spectrogram are showed. More examples and audios are available on our webpage: / Table 1: Performance for speech separation on WSJ0-mix dataset, compared with the same base model and SOTA methods. The∗means the same base model with identical settings and hyper- parameters. The Nin WSJ0-Nmix means the dataset with ﬁxed Nspeakers. All the "multiple architectures" based methods are trained speciﬁcally towards each speciﬁc N. Eval (OC) SI-SNRi in WSJ0-Nmix Methods Training DataTraining with variable number of speakersSingle architecture or multiple architectures 2mix 3mix 4mix 5mix RSAN [20] WSJ0-2mix × single 8.8 - - - OR-PIT [41] WSJ0-2&3mix ✓ single 14.8 12.6 10.2 - TasNet [23] WSJ0-Nmix× multiple 14.6 11.6 - - Our implemented TasNet∗[50] WSJ0-Nmix× multiple 15.4 12.8 - - ConvTasNet [29] WSJ0-Nmix× multiple 15.3 12.7 8.5 6.8 WSJ0-2mix × single 15.6 - - - WSJ0-3mix × single 12.7 13.3 - - WSJ0-2&3mix ✓ single 16.3 13.4 - -Conditional TasNet∗ WSJ0 2-5 mix ✓ single 16.7 14.2 12.5 11.7 DPRNN [29] WSJ0-Nmix× multiple 18.8 14.7 10.4 8.4 V oice Separation [29] WSJ0-Nmix× multiple 20.1 16.9 12.9 10.6 5.1 Datasets For the speech mixtures, i.e., the input Ofor our tasks, with different numbers of speakers, data from the Wall Street Journal (WSJ) corpus is used. In the two-speaker scenario, we use the common benchmark called WSJ0-2mix dataset introduced in [ 15]. The 30 h training set and the 10 h validation set contains two-speaker mixtures generated by randomly selecting speakers and utterances from the WSJ0 training set si_tr_s , and mixing them at various ratios (SNRs) uniformly chosen between 0 dB and 10 dB. The 5 h test set was similarly generated using utterances from 18 speakers from the WSJ0 validation set si_dt_05 and evaluation set si_et_05 . For three-speaker experiments, similar methods are adopted except the number of speakers is three. The WSJ0-2mix and WSJ0-3mix datasets have become the de-facto benchmarks for multi-speaker source separation, and we compare our results to alternative methods. Besides the separation task, we also instantiate our conditional chain model on multi-speaker speech recognition with the same WSJ0-2mix dataset. Although we just use the monaural speech in this work, it should be possible to apply our conditional chain model into the spatialized multi-channel speech, like the spatialized wsj0-mix, but we defer this to future work. 5.2 Multi-speaker speech separation First, we investigate the application of our conditional chain model to speech separation benchmarks. We take TasNet as the main base model in Figure 2(a), which is a simple but powerful de-facto- standard method in speech separation. Table 1 reports the results with different training settings (the number of speakers), compared with the same base model. By following the convention of this benchmark, we use the downsampled 8 kHz WSJ0-2mix set to reduce the memory consumption 6 Table 2: The estimation counting results on WSJ0-mix test set with variable number of speakers (2 to 5 in our experiments). The overall accuracy of the counting is 94.8%. Here we set the threshold of the energy value per frame in the estimated speech as 3×10−4to judge whether to stop the iteration. RefEst2 3 4 5 6Sum Acc (%) 2 2961 39 0 0 03000 98.7 3 12 2884 104 0 03000 96.1 4 0 42 2658 300 03000 88.6 5 0 0 125 2875 03000 95.2 of separation. We retrained the TasNet with the same settings and from an open source implementation [ 50]. It should be noticed that the TasNet and most speech separation methods could only be trained and used in the same number of speakers, while our conditional chain model removes this limitation. In terms of the architecture of the model, we only added a single layer of LSTM compared with the base model, resulting in a negligible increase in the number of parameters. The ratio improvement (SI-SNRi) results from Table 1 show that our conditional strategy achieves better performance than the base model (TasNet) under the same conﬁguration. For the ﬁxed number of speakers (2 or 3), our model improves on the original results, especially when there are more speakers (0.2 dB gains in WSJ0-2mix while 0.5 dB in WSJ0-3mix). Moreover, thanks to the chain strategy in our model, the WSJ0-2&3 mix datasets could be concurrently taken to train the model, and the performance is better than the training with each dataset. Compared with the RSAN [ 20] and OR-PIT [ 41] as mentioned in Section 4, which also could handle variable number of sources, our model achieves signiﬁcant performance improvements as a result of our end-to-end speech transduction in time domain. To further verify the upper limit that our model can reach with more speakers in speech separation task, we remixed two datasets, which we refer to as WSJ0-4mix and WSJ0-5mix, by simply concatenating the given mixture list from the standard WSJ0-2&3mix. As we expect, without adding any additional speech sources, the performance trained with WSJ0-2to5mix gains further improvement in WSJ0-2&3 mix and get reasonable SI-SNRi results in even 4 and 5 speaker mixtures. Besides the baseline models related to our methods, we also report the results from two strong works, i.e., DPRNN[ 26] and V oice Separation [ 29], which two upgrade the model architecture from the TasNet. Especially, V oice Separation [ 29] achieves the SOTA results in speech separation. However, their methods require multiple models (or multiple submodels in [ 29]) for each number of speakers in advance, which import additional model complexities or training procedures, and also cannot be applied to more speakers than the training setup. Even then, with the suboptimal base model (TasNet), our conditional chain model gets better results in 5 speaker mixtures. And we could clearly observe that as the number of speakers increases, our model achieves less performance degradation. We expect to realize further gains with our conditional chain model by improving the base models to DPRNN or V ocice Separation, which will be a part of our future work. Furthermore, Table 2 reports the estimation counting accuracy with the trained WSJ0 2-5mix datasets. Here, we set the threshold of the energy value per frame in the estimated speech as 3×10−4to judge whether to stop the iteration, resulting in the overall accuracy of 94.8%, which is signiﬁcantly better than the V oice Separation [ 29] (≤62%). It is worth mentioning that the upper bound of speaker number was set as 5 in our trained model, so there is a strong tendency to predict silence at the 6-th step (similar with the observation from RSAN [ 20]), leading to higher accuracy for 5-speaker mixtures than the 4-speaker ones. We also visualize two examples from the WSJ0-4mix test set in Figure 3, where we observe clear spectrograms with the estimated sources from pretty tangled mixtures. 5.3 Multi-speaker speech recognition Second, we evaluate our proposed method on the multi-speaker speech recognition task with the same WSJ0-mix corpus. We use the Transformer [ 43] as the basic Enc architecture block in Figure 2(b) to build speech recognition models, optimized with the CTC criterion [ 14]. Unlike the separation experiment in Section 5.2, which used 8 kHz, this section used the sampling rate as 16 kHz, which is the default setup for ASR to achieve better performance like most other previous works. An LSTM- 7 Table 3: WER (%) for multi-speaker speech recognition on WSJ0- Nmix- 16 kHz dataset. Methods System Training DataWER WSJ0-2mix WSJ0-3mix (1) DPCL + GMM-HMM [17] HMM WSJ0-2mix 30.8% - (2) PIT-DNN-HMM [30] HMM WSJ0-2mix 28.2% - (3) DPCL + DNN-HMM [28] HMM WSJ0-2mix 16.5% - (4) PIT-RNN [6] WSJ0-2mix 25.4% - (5) WSJ0-2mix 17.7% - (6) CTC WSJ0-2mix 31.2% - (7) CTC WSJ0-2mix 24.7% - (8) CTC WSJ0-1&2&3mix 14.8% 35.7% Table 4: SI-SNRi (dB) and WER (%) for multi-speaker joint training on WSJ0-mix- 8 kHz dataset. Methods Finetune Part Condition Training DataWSJ0-2mix SI-SNRi WER Conditional TasNet + Pre-trained ASR - Wave WSJ0-2mix 15.2 dB 25.1% + With Multiple Condition Separation Wave + CTC WSJ0-2mix 15.5 dB 17.2% + With Joint Training Separation, ASR Wave WSJ0-2mix 12.0 dB 15.3% + With Multiple Condition Separation, ASR Wave + CTC WSJ0-2mix 10.3 dB 14.4% based word-level language model with shallow fusion is used during decoding [ 16]. We compare our conditional chain CTC with the other systems including the HMM systems (1): deep clustering (DPCL)+GMM-HMM [ 17], (2): PIT-DNN-HMM [ 30] , and (3): DPCL+DNN-HMM [28], and the systems (4): PIT-RNN [ 6] and (5): . Note that all the PIT based methods correspond to the parallel mapping method in Figure 1(b), and they cannot deal with variable numbers of speakers. We compare the effectiveness of the proposed conditional chain models with the same CTC ar- chitecture based on PIT in Table 3. Our Transformer (7) is signiﬁcantly better than the corresponding PIT based system (6). Furthermore, the proposed conditional chain model can utilize the mixture speech data that has a variable number of speakers. To show this beneﬁt, we train our model using the combination of the single and multi-speaker mixture WSJ0 training data. It can be seen that the conditional chain model trained with the combination of speech (8) achieves the best WERs, 14.9%, on the 2-speaker mixture evaluation set among all the other systems including (1) – (7). Also, the proposed method can be applied to the 3-speaker mixture evaluation set and achieves reasonable performance ( 37.9%) in this very challeng- ing condition. This result is consistent with the observation in the speech separation experiment in Section 5.2. Interestingly, by analyzing the hypothesis generation process, we found that the speaker with the longest text is predicted ﬁrst generally, described in Section C in the Supplementary Material. Our conditional models are trained in a greedy fashion. We experimentally prove that it is important in our method. One common way for the output order of the multi-output network is to use the pre- determined order of targets, which was also used in multi-speaker speech separation and recognition several years ago [ 11]. We compare the greedy method with pre-determined order in the speech recognition task using the (7)th experiment settings.. The greedy method and pre-determined order method achieve the WERs of 24.7% and 28.4%, respectively. The pre-determined order method is worse than the proposed greedy method. 5.4 Cross-domain condition in joint separation and recognition In previous experiments of separation and multi-speaker speech recognition tasks, we explore the effectiveness of our conditional chain model with output sequences from the same domain, as depicted in Figure 2. In contrast, in this subsection, we evaluate a combination of cross-domain conditions by using the ASR output to guide separation learning. This direction is motivated by the so-called informational masking effect in the cocktail party problem [ 4,19], where the linguistic clue may exert a strong inﬂuence on the outcome of perception. 3The model was proposed in [7] and was open source in ESPnet. We reproduced their results ﬁrst and used the code to get the results on WSJ0-2mix data. 8 Prediction Wav 1 Prediction Wav 2 Prediction Wav 3 Enc EncSilent Wav LSTM LSTM Mixture WavDec Dec Enc Separator 𝑂𝑂𝐇𝐇𝟏𝟏 𝐇𝐇𝟐𝟐�𝑺𝑺1 𝐄𝐄𝒔𝒔0 Embed EmbedASR Enc LSTMDec 𝐇𝐇𝟑𝟑�𝑺𝑺2 EmbedASR STOP�𝑺𝑺3 Fusion Fusion Fusion Conditional Connection > Seq Token Seq 1 Token Seq 2Figure 4: Conditional Chain Model for multi-speaker joint speech separation and recognition. Speciﬁcally, as illustrated in Figure 4, in each step of our conditional chain model, both the separated waveform and the CTC alignment from the ASR model are utilized as the conditions for the next step, encouraging CondChain in Eq. 3 to jointly capture the information spanning multiple sources. The baseline conditional TasNet is trained using waveform instead of chunk-level as in Section 5.2. In addition, unlike Section 5.3, we use a pre-trained ASR based on a single speaker CTC ASR system trained on the WSJ training set SI284 without overlap with WSJ0-2mix test set to stabilize our joint model training. We also use the downsampled 8 kHz data for the reduction of the memory consumption in separation and non-truncation of the speech mixture for appropriate ASR evaluation. Details of framework are explained in the Appendix A.3. The results are shown in Table 4. Note that the numbers listed in this experiment cannot be strictly compared with those in Sections 5.2 and 5.3 due to the above different conﬁguration requirements between separation and recognition. When directly feeding the separated waveform from the baseline conditional TasNet to the pre-trained ASR, we get 25.1% WER. Finetuning of the TasNet with both waveform and CTC alignment conditions achieved 0.3dB improvement of SI-SNRi. The improvement shows that the semantic condition, such as CTC alignment, provides a good guide to the separation learning. Finetuning of both TasNet and ASR models with both waveform and CTC alignment conditions yields the best WER of 14.4%, while waveform-only conditioning obtains the WER of 15.3%. Note that this joint training severely degraded the SI-SNRi result, but the WER result gets a signiﬁcant improvement. This intriguing phenomenon demonstrates that the separation evaluation metric (SI-SNRi) does not always give a good indication of ASR performance, as studied in [ 38,39]. 6 Conclusions In this work, we introduced conditional chain model, a uniﬁed method to tackle a one-to-many sequence transduction problem for mixture signals. With the probabilistic chain rule, the standard model is extended to output multiple sequences with explicit modeling of the relevance between multiple output sequences. Our experiments on speech separation and multi- speaker speech recognition show that our model led to consistent improvements with negligible increase in the model size compared with the conventional methods, In terms of the application scope, although we verify the proposed method with two speciﬁc tasks, speech separation and recognition, this conditional chain model can be ﬂexibly extended to other problems for mixture signals, as a general machine learning framework. Therefore, as a future work, it might be interesting to adopt this method to other sequential tasks including natural language processing and video analysis. Another exciting direction would be introducing the attention mechanism into the fusion and conditional chain part, which could ﬂexibly capture the implicit relationship between input and output from variable domains. 9  This work was supported by the Major Project for New Generation of AI (Grant No. 2018AAA0100400), and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB32070000). Broader impact Beneﬁts Our conditional chain model addresses the problems where one input sequence is mapped to multiple sequences by taking advantage of the intrinsic interaction between the output sequences. There are a variety of applications that can beneﬁt from the use of the conditional information, such as the text generation tasks. Another important application is the cocktail party problem in speech processing. With the parallel mapping models, which are the dominant method at present, the model cannot handle the variable number of speakers ﬂexibly due to the limitation of the model structure. In such models, the solution to label permutation problems is to exhaustively compute all the permutations with the computation cost of N!, which cannot be neglected when the number of speakers are more than 3. However, using the conditional model can avoid this problem. It also proves the effectiveness of our model which achieves relatively good performance in both separation and recognition tasks. We make a further step towards attacking cocktail party problem. This will improve the communication quality of human-computer interaction. And our method can also be applied in meeting transcription system to provide better performance. We would like to make our code available latter to facilitate the study applied to other tasks. Drawbacks There is no doubt that the improvement of artiﬁcial intelligence can potentially revolutionise our societies in many ways. However, it also bring some risks to human’s privacy. With the abusing use of speech separation and recognition techniques, hackers can easily monitor people’s daily life, while a strong NLP system can also be applied to Internet fraud. We think the community should not only focus the development of techniques, but also concern the privacy issue. Besides, the widely use of artiﬁcial intelligence techniques may also lead to mass-scale unemployment problems, such as call center. 10  
a faster maximum cardinality matching algorithm with applications in machine learning 	A Faster Maximum Cardinality Matching Algorithm with Applications in Machine Learning Nathaniel Lahn∗ School of Computing and Information Sciences Radford University Radford, V A 24142 Raghvendra Department of Computer Science Virginia Tech Blacksburg, V A 24061 Jiacheng Ye Department of Computer Science Virginia Tech Blacksburg, V A 24061 yjc0513@vt.edu Abstract Maximum cardinality bipartite matching is an important graph optimization prob- lem with several applications. For instance, maximum cardinality matching in a δ-disc graph can be used in the computation of the bottleneck matching as well as and the Lévy-Prokhorov distances between probability distribu- tions. For any point sets A,B⊂R2, theδ-disc graph is a bipartite graph formed by connecting every pair of points (a,b)∈A×Bby an edge if the Euclidean distance between them is at most δ. Using the classical Hopcroft-Karp algorithm, a matching on any δ-disc graph can be found in ˜O(n3/2) time.2In this paper, we present a simpliﬁcation of a recent algorithm (Lahn and Raghvendra, JoCG 2021) for the maximum cardinality matching problem and describe how a maximum cardinality matching in a δ-disc graph can be computed asymptotically faster than O(n3/2)time for any moderately dense point set. As applications, we show that if AandBare point sets drawn uniformly at random from a unit square, an exact bottleneck matching can be computed in ˜O(n4/3) time. On the other hand, experiments suggest that the Hopcroft-Karp algorithm seems to take roughly Θ(n3/2)time for this case. This translates to substantial improvements in execution time for larger inputs. 1 Introduction Computing a maximum cardinality matching is a fundamental graph optimization problem. With origins in economics and logistics, matchings have found numerous applications. Computing popular distances between distributions, such as the Wasserstein distance as well as the Lévy-Prokhorov distance, can be reduced to a bipartite matching problem. In this paper, we consider the δ-disc graph matching which is the following: Given two sets AandBofntwo dimensional points and a parameter δ>0, aδ-disc graphGδis a bipartite graph obtained by connecting any pair of vertices (a,b)∈A×Bwith an edge provided that the Euclidean distance between aandbis at mostδ, i.e.,∥a−b∥≤δ. Letmbe the number of edges ∗Authors are ordered by last name. All authors contributed equally to this work. 2We use ˜O(·)to suppress terms in the complexity. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). in the graph Gδ. AmatchingMis a set of edges in Gδ. In theδ-disc graph matching problem, we wish to compute a maximum cardinality matching inGδ, i.e., a matching that has the largest number of edges. Any algorithm for computing a δ-disc graph matching can also be used to compute a bottleneck matching as well as the Lévy-Prokhorov distance, both of which are deﬁned next. Let M⊆A×B be any perfect matching of AandB, which is a matching where every vertex of Ais matched, i.e., |M|=n. The edge of Mwith the largest Euclidean length is its bottleneck edge. The bottleneck matching is a perfect matching M∗whose bottleneck edge length is minimized. The Euclidean length of the bottleneck edge of M∗is the bottleneck distance betweenAandB. Distances between distributions: Next, consider the case where AandBdeﬁne discrete distri- let every point a∈A(resp.b∈B) carry a probability of 1/n, i.e., PA(a) = 1/n(resp.PB(b) = 1/n). distance between PAandPB, de- noted byW∞(PA,PB), is simply the bottleneck distance between the point sets AandB. The Lévy-Prokhorov distance between PAandPBis deﬁned as follows: For ε > 0and any sub- ), letXε={b∈B| ∃a∈Xsuch that∥a−b∥ ≤ε}(resp. that∥a−b∥≤ε}). Note that Xε⊆BandYε⊆A. We say that the Lévy-Prokhorov [34] equal to the smallest value of εfor which the following property is true: ∀) +) +ε; herePA(X) =∑ a∈XPA(a) =|X|/nandPB(Y) =∑ b∈YPB(b) =|Y|/n. We can, therefore, write this condition as ∀. (1) Determining if the exact bottleneck distance (equivalently W∞(PA,PB)) is≤δcan be done by simply ﬁnding the maximum cardinality matching MinGδ. It is easy to see that Mis perfect if and only if the bottleneck distance is at most δ. Similarly, by applying Hall’s theorem, one can show and only if the maximum cardinality matching MinGεhas a size of at least (1−ε)n. Thus, theδ-disc graph matching directly relates to computing ∞-Wasserstein distance as well as the Lévy-Prokhorov distance between probability distributions. Computing δ-disc graph matching: One can use any of-the-shelf matching algorithm [ 18,31, 33,39] to compute a maximum cardinality matching in a δ-disc graph. For instance, using the well-known Hopcroft-Karp algorithm will lead to an execution time of O(m√n)on anyδ-disc graph. The HK-Algorithm executes in phases. Each phase takes O(m)time, and, in the worst-case, the algorithm converges to a maximum matching in O(√n)phases. The best-known exact algorithm for computing the exact bottleneck matching combines geometric data structures with the HK-algorithm in order to reduce the execution time of each phase from O(m)to˜O(n). As a result, they obtain an exact bottleneck matching in ˜O(n3/2)time. Inspired by a series of algorithms for weighted matching in graphs with small separators [ 4,27], Lahn and Raghvendra [ 28] presented a weighted approach to the maximum cardinality matching problem. This LR algorithm identiﬁes a set of “edge separators" incident on ω“boundary vertices”. These edges are assigned a weight of 1and all other edges receive a weight of 0. A property satisﬁed by these separator vertices is that after their removal, every connected component in the graph has no more than rvertices. Then, they present an algorithm to compute a perfect matching logn)time. As an application of their result, they show how to compute the bottleneck distance of any point sets AandBwithin a multiplicative factor of (1 +ε)in ˜. The LRalgorithm assigns dual weights to vertices and is similar in style to the Kuhn-Munkres [ 24] and Gabow-Tarjan [ 15] algorithms. The dual weights on vertices play a vital role in the proofs of correctness and efﬁciency of the LR algorithm. In this paper, we make the following contributions: •We remove the need to maintain dual weights in the LRalgorithm, resulting in a signiﬁcantly simpler algorithm. 2 •Using this algorithm, we show how to ﬁnd a maximum cardinality matching in a unit-disc graphGδin time ˜ the maximum number of points of A∪Bcontained in any disc of radius δ. Note that our algorithm is asymptotically faster than the classical Hopcroft-Karp based algorithm when k=o(√n). •Using our algorithm for δ-disc graph matching, we show how to compute the exact bottleneck distance between point sets AandB. WhenPAandPBare discrete distributions with each point having probability 1/n, the bottleneck distance can be used to compute the )) andπ(PA,PB). We are not aware of any previous polynomial time algorithms to compute the Lévy-Prokhorov distance. When A,B are chosen uniformly at random from a unit square, our algorithm for the exact bottleneck distance runs in ˜O(n4/3) time. All previous algorithms take Ω(n3/2)time. •We run experiments for the case where AandBare chosen uniformly at random from a unit square. Our experiments suggest that the Hopcroft-Karp algorithm on Gδtakes Θ(n3/2) time. In contrast, our algorithm runs substantially faster and executes in ˜O(n4/3)time. Note that, for the HK-algorithm, the upper bound of O(√n)phases is only in the worst-case. As noted by Motwani [ 32], the Hopcroft-Karp algorithm converges, with high probability, to a maximum matching in O(logn)phases for expander graphs in general and Erd ˝os-Rényi random graphs in particular. Similarly, does the HK algorithm execute asymptotically fewer when used to compute bottleneck matchings? Interestingly, our experiments suggest that the answer to this question may be in the negative. Based on our experimental results, when AandBare drawn uniformly at random from a unit square and when δis set to the bottleneck distance, the number of phases seem to grow at the rate of Ω(√n). Therefore, bottleneck matching on random point sets may represent a natural hard instance for the HK-Algorithm. In this paper, we show how to overcome the Ω(n3/2)barrier for uniformly distributed point sets by developing an ˜O(n4/3)time algorithm. For anyε>0and any point set A∪B, the LRalgorithm can also be used to compute a multiplicative (1 + of the bottleneck matching in ˜. This is done by using a grid where the side-length of each cell is a function of ε. The algorithm rounds every point to the closest cell center and ﬁnds a δ-disc graph matching using the LRalgorithm. See Section 6 of [ 28] for details. One can also use a similar approach to compute a multiplicative (1 + of the Lévy-Prokhorov distance. Replacing the original LRalgorithm with our dual-free implementation leads to simpler approximation algorithms. Applications: Wasserstein distance has found numerous applications in machine learning and computer vision [ 3,5,8,10,14,36]. Due to these applications, computing approximations of Wasserstein distances has received substantial attention [ ]. However, exact algorithms (even for discrete distributions) have a relatively high execution time [ 15,24,39]. The Lévy-Prokhorov distance have been extensively studied for its theoretical properties [ 11,38]. For instance, Lévy-Prokhorov distance metrizes weak convergence on any separable metric space [ 19]. However, a brute-force algorithm based on the deﬁnition of this metric will require a search on exponentially many possible subsets causing it to seldom be used in practice [ 16]. However, we use Hall’s theorem to show that the computation of the Lévy-Prokhorov metric reduces to the δ-disc graph matching problem and so, it is only as hard as computing the ∞-Wasserstein distance, at least for discrete distributions of the type above. In this paper, we provide exact algorithms for computing the ∞-Wasserstein and the Lévy-Prokhorov distances for 2-dimensional discrete distributions. Our algorithms can be useful in several scenarios. For instance, one can estimate Lévy-Prokhorov distances between any two continu- ous distributions by simply computing the distances between samples drawn from these distributions. From the fact that the Lévy-Prokhorov distance metrizes weak convergence, for large enough samples we can get accurate distance estimates. Faster high-precision algorithms are critical in obtaining such estimates; see [ 7,6]. For high dimensional discrete distributions, Wasserstein distance is sometimes estimated by embedding them into a lower dimensional space and computing high precision solution in this space. For example, see the sliced Wasserstein distance [23]. In the emerging area of topological data analysis, high dimensional point clouds are characterized by point sets called persistence diagrams where each point represents the so-called birth and death times of a topological feature. Different point clouds can be compared by computing the bottleneck distance between the corresponding diagrams [ 17,22,1]. This has 3 led to development of practical of bottleneck matching algorithms [ 17,22]. More recently, other Wasserstein distances between persistence diagrams have also been considered. See for instance [25, 40]. The special case of computing bottleneck matching for point sets AandBthat are drawn uniformly at random from a unit square has also received considerable attention. For instance, it has been used in the context of testing pseudo-random generators, average case analysis of bin packing algorithms [ 29], and also in statistics for analyzing the convergence of empirical measures [ 37]. δ-disc graphs have other applications as well, including in the modeling of the topology of ad-hoc wireless networks [20]. 2 Matching algorithms In this section, we present and compare two algorithms for solving the maximum cardinality matching problem on an arbitrary graph: The Hopcroft-Karp (HK) algorithm [ 18], and the LRalgorithm [ 28]. In section 2.1, we introduce the basic deﬁnitions used by most combinatorial matching algorithms and give an overview of the HK algorithm. In section 2.2, we present the LRalgorithm, highlighting the differences it has from the HK algorithm. 2.1 Preliminaries Given any matching M, the vertices of that are not matched in M. We refer to these vertices as freevertices. An alternating path Pis a path that alternates between edges that are in the matching and those that are not in the matching. An augmenting path is an alternating path that starts and ends at a free vertex. We deﬁne the length ofP as the number of edges in P. We can augment a matching Malong an augmenting path Pby updating the matching to M← M⊕P; where⊕denotes the symmetric difference operator. It is easy to see that augmenting a matching along an augmenting path Pincreases the size of the matching Mby1. Furthermore, it can be shown that Ghas no augmenting paths with respect to a matching Mif and only if Mhas maximum cardinality. These observations are the basis of the following commonly-used approach for computing a matching: repeatedly compute an augmenting path Pwith respect toMand augment maximum cardinality. Since the largest possible matching has size at most n, any such algorithm will arrive at a matching after naugmentations. This is the approach used by the classical Ford-Fulkerson and HK algorithms as well as the recent LRalgorithm. However, the algorithms differ in the details of how these augmenting paths are found. Residual graph: Given a matching M, the residual graph GMis a directed graph that assists in ﬁnding augmenting paths. The graph GMcontains the same set of vertices VasG. For any edge (a,b)inG, if(a,b)∈Mthen we add an edge directed from atobtoGM. Otherwise, we add an edge directed from btoatoGM. Furthermore, we create a source vertex sand a sink vertex twith the following additional edges. For each free vertex b∈BF, we add an edge (s,b)directed from the for each free vertex a∈AFwe add an edge from ato the sinktinGM. Note that, for the residual graph, we use (u,v)to denote an edge directed from utov. On the other hand, for the undirected graph G, we may use ( to represent the same edge between vertices uandv. Consider any directed path Pfromstotin the residual graph. Note that removing sandtfromPwill result in an augmenting path. In the Ford-Fulkerson algorithm, a single augmenting path can be found in O(m)time using any common graph search algorithm such as breadth-ﬁrst search (BFS) or depth-ﬁrst search (DFS), leading to an O(mn)time algorithm. The HK and LRalgorithms both improve upon this running time by ﬁnding potentially many augmenting paths in each iteration. HK algorithm: Initially, let M=∅. The HK algorithm executes in phases . A phase is divided up into two stages . The ﬁrst stage executes a BFS starting from sand identiﬁes the length of the shortest path (path with the fewest edges) in GMfromsto every other vertex in GM. Letℓube the length of the shortest path from stouand letℓ=ℓt. The algorithm then computes an admissible graph Aconsisting of all edges (u,v)inGMsuch that (a) ℓuandℓvare at mostℓand (b)ℓv=ℓu+ 1. Note that these edges capture the set of all minimum-length augmenting paths in GM. The second 4 stage iteratively conducts multiple partial DFSs that start from sand terminate early if a path to t is found. Following the termination of a partial-DFS, all edges visited by it are removed from the residual graph. The algorithm proceeds to the next phase if a partial-DFS terminates without ﬁnding a path from stot. It can be shown that, in each phase, the HK algorithm ﬁnds a maximal set of shortest augmenting paths in GM. Each phase involves execution of a single BFS and multiple partial DFSs. Since no two executions of DFS visit the same edge, the combined execution time of the multiple partial DFSs is bounded by O(m). Hopcroft and Karp showed that the length of the shortest augmenting path increases by at least 1 after each phase. Therefore, after√nphases, the shortest augmenting path has length at least√n. Using this, they showed that there are no more than√nfree vertices remaining, all of which can be matched by augmenting along an additional paths. Thus, the total number of phases executed by the algorithm is O(√n). Since each phase takes O(m)time, the total time taken by the algorithm is O(m√n). Somewhat surprisingly, Hopcroft and Karp [ 18] also showed that the total length of all naugmenting paths, across all phases, is only O(nlogn). 2.2 A simpliﬁed implementation of the LR algorithm Recently, Lahn and Raghvendra presented an algorithm [ 28] to compute a maximum cardinality matching. Their algorithm resembles the Kuhn-Munkres algorithm for weighted matching. In this section, we present a cleaner implementation of the LRalgorithm. These simpliﬁcations result in a closer resemblance to the HK algorithm. Unlike the LRalgorithm, our algorithm does not maintain any dual weights. In the following, we describe and contrast our algorithm with the HK-algorithm. Apart from a bipartite graph G(V,E), we are also given a subset ES⊆Eof “separator edges” as input. For any separator edge (u,v)∈ES, we denote the vertices vertices . Let Bbe the set of all boundary vertices. The analysis of the algorithm depends on ω=|B|and another parameterrthat is deﬁned next. Consider the graph G′(V,E\ES). the set of connected components of G′and letViandEibe the set of vertices and edges of Pifor all 1≤i≤t. We refer to eachPiinG′as apiece of the original graph G. Letr= maxPi∈P|Vi|, i.e., the size of the piece of Gwith the largest number of vertices. Setting weights on the edges of the graph Gand its residual graph GM:For any edge (u,v)∈E, we assign it a weightw(u,v). For any separator edge (u,v)∈ES, we setw(u,v)to1. For any other edge (u′,v′)∈E\ES, we setw(u′,v′)to0. Every edge (u,v)in the residual graph inherits the weight of the corresponding edge (u,v)inG(V,E). All edges incident on the source sand the sink t inGMreceive a weight of 0. For any path P, its weight is simply the sum of the weights of its edges. Preprocessing: In the preprocessing step, our algorithm ﬁnds a maximum cardinality matching for each piece by applying the HK-Algorithm. Let Mbe the union of these matchings computed across all pieces. At the end of this step, the difference |M∗|−|M|isO(ω), whereM∗is the maximum cardinality matching in G. So, our algorithm has to ﬁnd an additional O(ω)augmenting paths in order to compute a maximum cardinality matching. The remaining O(ω)unmatched vertices are subsequently matched in phases . Like the HK algorithm, each phase of our algorithm consists of two stages . These stages somewhat resemble the stages of the HK algorithm. We highlight the differences in the description below. Stage 1: In the ﬁrst stage, our algorithm ﬁnds, for any vertex v∈V, the minimum weight path from stovin the residual graph GMusing the weights w(·,·). Note that, since every edge weight is either 0or1, a standard BFS implementation can be modiﬁed to support such a minimum-weight search algorithm in O(m)time by simply prioritizing edges of weight 0over edges of weight 1. We call this modiﬁed version of BFS, 0/1 BFS . For any vertex v∈V, letℓvbe the weight from stovinGM as computed by the 0/1BFS and let ℓ=ℓt. Any edge ( ifℓu,ℓv≤ℓand ℓv=ℓu+w(u,v). The admissible graphAis identical to GM, except it contains only admissible edges. Similar to the HK algorithm, it can be shown that the admissible graph Acaptures every minimum-weight augmenting path. Stage 2: The second stage of our algorithm ﬁnds a set of shortest augmenting paths (by weight). It does so by iteratively conducting partial-DFSs from suntil no augmenting path is found. Each partial-DFS immediately terminates if an augmenting path Pis found. LetKbe the set of affected pieces , which are pieces that contain at least one edge of P. Unlike in the HK-Algorithm, the 5 matchingMis immediately augmented along Pand every edge visited by this partial-DFS that does not belong to an affected piece is deleted. Note that any edge from an affected piece that was visited by this partial-DFS does not get deleted and could be revisited by a later partial-DFS. In other words, edges in an affected piece can be visited multiple times within the same phase. Differences with HK algorithm: The main differences between our algorithm and the HK algorithm are: (1)Our algorithm assigns weights of 0 and 1 to the edges. No weights are assigned in the HK algorithm. (2)Our algorithm has a preprocessing step that computes a maximum cardinality matching within each piece. (3)In Stage 1, our algorithm executes a 0/1-BFS instead of the BFS executed by HK algorithm. (4)In Stage 2 of our algorithm, the partial-DFS reuses edges from affected pieces. As a result, in each phase, our algorithm may ﬁnd augmenting paths that are not . Differences (1) – (3) between HK algorithm and our algorithm do not impact the execution time of the algorithm by any more than a small constant factor. See Section F of the supplement for a discussion on this. The critical difference between the two algorithms is (4). Unlike the HK algorithm, Stage 2 of our algorithm reuses edges from affected pieces and computes a set of augmenting paths that are not necessarily . This allows for computing many more augmenting paths within each phase. As we show later, the total number of phases executed as well as the augmenting paths computed by our algorithm are identical to those computed by the LRalgorithm. Therefore, the analysis of Lahn and Raghvendra can be directly applied to our algorithm. They show that after each phase, the weight of the shortest augmenting path increases by at least one. After√ωphases, they show that there areO(√ω)free vertices which can be matched using an additional O(√ω)phases. Thus the total number of phases can be bounded by O(√ω). Edge revisits cause the execution time of a phase to increase. Note that an edge can be revisited only if it was inside an affected piece when it was most recently visited. Lahn and Raghvendra show that the total number of affected pieces is O(ωlogω)(a piece that is affected ktimes is counted ktimes in this sum). For graphs that admit recursive separators (such as planar and graphs with excluded minors), they show that the number of edges for any piece can be bounded by O(mr/n )leading to anO(mrω nlogω)bound on the total number of revisits. Theorem 1. Consider a graph Gand a set of separator edges. Suppose each piece has at most O(mr/n )edges. Our algorithm computes a maximum cardinality matching in O(m√r+m√ω+ mrω nlogn)time. For our algorithm, the assumption on an upper bound on the number of edges within each piece can be eliminated when graphs, such as those considered in this paper, support a dynamic data structure Dof the following form: Dcan store any subset A′⊆Aof vertices and, given any query vertex b∈B, it can return a vertex a∈A′that minimizes the weight of the edge (b,a). Note that the weight of(b,a)will be 1only if every edge from bto any vertex a′∈A′has a weight of 1. If no edge exists between band any vertex of A′, then the data structure returns NULL. Suppose that Dsupports arbitrary insertions and deletions from A′, as well as queries, each in Φ(n)time. Then, one can use this data structure to dynamically maintain the set of unvisited nodes of Aduring a 0/1 BFS or DFS. Consequently, one can execute 0/1 BFS and DFS in time O(nΦ(n)). As a result, the execution time of our algorithm can be improved to ) logn). In contrast, using Dto execute a Hungarian Search inside the LR algorithm seems challenging. Theorem 2. Given a graph Gthat supports a dynamic nearest neighbor data structure with query and update time of Φ(n), a maximum cardinality matching can be computed by our algorithm in )). The HK algorithm computes a maximum cardinality matching . Equivalency to original LR algorithm: The original algorithm maintains a dual weight y(v)for every vertex v∈A∪Bat any point during the algorithm. For any edge (a,b)∈(A×B)∩E, the dual weights satisfy the following: 6 ) if(a,b)̸∈M, (2) y(a)−y(b) =w(a,b) if(a,b)∈M. (3) Additionally, their algorithm maintains the invariants that all free vertices of BFhave the same dual weight of ymax= all free vertices of Ahave the same dual weight of 0. The slacks(a,b)of any edge (a,b)∈(A×B)∩Eis deﬁned as follows: if (a,b)/∈M, then s(a,b) =w(a,b)−y(b) +y(a); otherwise, (a,b)∈Mands(a,b) = 0 . In the original LR algorithm, the ﬁrst stage adjusts the dual weights so that there is at least one zero-slack augmenting path in GM. The second stage takes the subgraph consisting of zero slack edges and repeatedly executes a DFS from s. This DFS stops early if a path to t, i.e., an augmenting path, is found. After augmenting along a path, all edges visited by the DFS are deleted, unless they were in an affected piece. Note that the only fundamental difference between the original version of the LR algorithm and our simpliﬁed version is the fact that we compute minimum-weight augmenting paths while they compute zero-slack augmenting paths. The following lemma, whose proof appears in Section A of the supplement, shows that the a zero slack path computed in the LRalgorithm is also a minimum weight path. It follows that the two versions of the algorithm are equivalent. Lemma 1. During Stage 2 of the LRalgorithm, an augmenting path has zero slack if and only if it has minimum weight. 3 Applications In this section, we show how our algorithm can be applied to efﬁciently compute a maximum cardinality matching on a δ-disc graph, an optimal bottleneck matching, as well as the Lévy-Prokhorov distance between distributions. All applications considered are for point sets A,B⊂R2. For all applications, one can build a data structure Dfrom Theorem 2 with Φ(n) = logO(1)nby using a dynamic Euclidean nearest neighbor data structure; see Section H.3 of the supplement for details 3.1δ-disc graph matching LetP=A∪B. LetB(p)be a ball centered at pwith radiusδ. Considerk= maxp∈R2|B(p)∩P|, i.e.,kis the largest number of points of A∪Binside any ball of radius δ. We refer to kas the δ-density of the point set P. We show that a maximum cardinality matching in a δ-disc graph can be computed using our algorithm in ˜. Thus, when the ), our algorithm outperforms the HK algorithm. Theorem 3. For any point set P=A∪Band a parameter δ>0, a maximum cardinality matching in theδ-disc graph deﬁned on Pcan be computed in ˜, where kis theδ-density of P. This result also extends to the case where the points of AandBare independently and identically distributed random variables drawn from distributions . We say that a aδ-density ofkif, for any ball B(p)of radiusδ, the probability that a point drawn fromPlies inside the ball is at most k/n. Theorem 4. LetA,B be drawn iid from distributions . For a parameter δ>0, a maximum cardinality matching in the δ-disc graph deﬁned on A∪Bcan be computed, with high probability, in ˜, wherekis the maximum of the δ-density ofPAandPB. Proof of Theorem 3: We show how a set of separator edges can be generated so that ω= ). From Theorem 2 and since = O(n4/3k1/3), ), the execution time of the LRalgorithm can be bounded by ˜O(n4/3k1/3). Next, we describe how to generate the separator edges ES. We use a gridGto generate the separator edges. Any grid consists of a set of equispaced horizontal and vertical lines that partition R2intocells. Each cell is a square and any grid can be seen as a 7 set of these squares. Let L(G)denote the side-length of any cell C∈G. We say that a cell Cis non-empty ifC∩P̸=∅. ⌉. To generate our pieces, we choose a grid Gwhere the side-length of each cell is set to L(G) =θδ. The separator edge set ESconsists of all edges of theδ-disc graph that have their endpoints in different cells. All such edges are assigned a weight of 1. Any edges whose endpoints are contained within the same cell of Gare inE\ESand are assigned a weight of 0. Any point that has at least one separator edge incident on it becomes a boundary vertex. To generate G, we vertical and horizontal shifts and pick the one that minimizes the number of boundary vertices. We provide the details of generating Gin Section B.1 of the supplement. Our choice of Gguarantees that any point set, independent of itsδ-density. We show this in Section B.2 of the supplement. Boundingr:By this deﬁnition, the number of vertices of any piece is bounded by the maximum number of points that can lie inside any cell of G, i.e., maxC∈G|C∩P|. Note that we can cover any cell of Gwith Θ(θ2)balls of radius δeach. Due to the δ-density ofPbeingk, each of these balls can contain at most kpoints and the total number of points inside any cell can be bounded by O(θ2k) =O(n2/3/k1/3)as desired. In other words, r=O(n2/3/k1/3). Proof of Theorem 4: The construction of the grid here will be identical to the one in the proof of Theorem 3. Note also that the bound on ωprovided in that proof depends only on the construction of Gand not on the δ-density of the point set. Therefore, the same bound continues to hold here as well. In Section C of the supplement, we use the δ-density ofPAandPBalong with Chernoff’s bound to prove with high probability. 3.2 Bottleneck distance In this section, we assume that AandBare points drawn uniformly at random from a unit square. From the work of Leighton and Shor [ 29], we know that, for appropriate constants cminandcmax, the optimal bottleneck distance is at least at most with very high probability (probability exceeding 1−1/nαfor someα= Ω(√logn)). Observe that the optimal bottleneck distance will be equal to the length of some edge of A×B. As in the work of Efrat et al. [13], our algorithm will use a selection algorithm of Katz and Sharir [ 21] to ﬁnd thejth smallest edge, . Letd(j)be the length of the jth smallest edge returned by their algorithm. This allows us to execute a binary search over the edges of A×B, ordered by their length. Let gmin= 1andgmax=n2. We repeat the following process until gmax=gmin+ 1: We choose ﬁnd thejth smallest edge whose length is denoted byd(j). Ifd(j)≥δmax, we setgmax←j. Ifd(j)≤δmin, we setgmin←j. Otherwise, δmin≤d(j)≤δmax, and we ﬁnd the maximum cardinality matching in a δ-disc graph where δis set tod(j). If we obtain a perfect matching, we set gmax=j. Otherwise, the maximum matching is not perfect, and we set gmin=j. When the algorithm terminates, d(gmax)is the optimal bottleneck distance. Analysis: The algorithm makes O(logn)many guesses. These guesses are found by a selection algorithm that runs in . For each guess , we must compute a matching on a δ-disc graph, which takes using the LR algorithm. Since, P(A)andP(B)are the uniform distribution, their δ-density increases as δ increases. Therefore, the δmax-density be an upperbound on the δ-density for any execution of the LRalgorithm. The probability that any random point lies within any ball of radiusδmaxis at most (2δmax)2, which is Θ(log3/2(n)/n). Therefore, the δmax-density ofP(A) andP(B)is at )). Applying Theorem 4 gives the following: Theorem 5. LetA,B be drawn uniformly at random from a unit square. An optimal bottleneck matching can be computed between AandB, with high probability, in ˜O(n4/3)time. Practical considerations: The algorithm described uses two black-boxes that are impractical and have hidden high constants in the Big-O notation. (a) the algorithm relies on a dynamic nearest neighbor data structure, and, (b) the algorithm uses the selection algorithm of Katz and Sharir [ 21]. In Section D of the supplement, we address both (a) and (b) by presenting more practical alternatives. 8 3.3 Lévy-Prokhorov distance In this section, we show that we can use our algorithm for the δ-disc graph matching to also compute the Lévy-Prokhorov distance. We describe a simple algorithm to decide if than or at most ε. We compute a maximum cardinality matching Min anε-disc graph. Let AFandBFbe the free vertices with respect toM. Then, we say that the distance is greater than εif|AF|>εn . Otherwise, we say that the distance is at most ε. Using a binary search similar to the one described in Section 3.2, we can determine the distance in ˜. Proof via Hall’s theorem: Given any bipartite graph G(A∪B,E), for any set X⊆A, the the set of all vertices of Bthat share an edge with at least one vertex of X. Thus,Xεis the neighborhood of Xin anε-disc graph. The deﬁciency of a graph with respect to Ais µ(A) = maxX⊆A|X|−N (X). Hall’s theorem says that a bipartite graph has a perfect matching if and only if the deﬁciency of the graph with respect to Ais non-positive. Hall’s theorem can be generalized to the following. Lemma 2. For any bipartite graph G(A∪B,E), where|A|=|B|=n, and for any integer k>0, the deﬁciency with respect to AorBiskif and only if the maximum cardinality matching is of size n−k. The proof of this generalization follows in a way from the Hall’s theorem. For the sake of completion, we provide this proof in Section E of the supplement. Next, we show that the algorithm described here correctly computes the Lévy-Prokhorov distance. Recollect that, our algorithm returns the distance to be greater than εif|AF|>εn . By Lemma 2, we conclude that the deﬁciency of the graph is greater than εn, i.e., there is a set X⊆Asuch that |X|−|Xε|>εn . Thus, Equation 1 does not hold and the distance is greater than ε. Our algorithm returns a distance at most εif|AF|≤εn. In this case, from Lemma 2, the deﬁciency of the graph with respect to Ais less thanεn, i.e., for every subset . Note so an identical argument applies for Bas well. Thus, Equation 1 holds and the distance is at most ε. We conclude that the algorithm terminates with the correct ε. Theorem 6. Let the point sets two distributions PAandPBwhere each point has a probability of 1/nassociated with it. The Lévy-Prokhorov distance π(PA,PB)can be computed in ˜ wherekis theδ-density ofA∪B. 4 Experimental results In this section, we compare the performance of the HK and LR algorithms when applied to computing an exact bottleneck matching between equal-sized point sets A,B⊂R2drawn uniformly at random from a unit square, where n=|A|+|B|. Experimental setup: For each value of , }, we execute 10runs. For each run, we uniformly sample points from a unit square to obtain the point sets AandB. Next, we compute a bottleneck matching between Aand Bseparately, using both the HK algorithm and our algorithm, and record performance metrics for both algorithms. We execute our experiments on a server running CentOS Linux 7, with 12 Intel E5-2683v4 cores and 128GB of RAM.3 When guessing the bottleneck distance for each run, instead of enforcing that the number of guesses isO(logn)it is sufﬁcient in practice to continue the binary search on δuntil the relative error becomes less than a sufﬁciently small value ε(see Section D of the supplement). Both the HK-based algorithm and the LR-based algorithm use the same strategy for guessing the bottleneck distance in the experiments. Experimental results: For each ﬁgure, the data presented for each value of nis averaged over all 10 runs. Error bars represent a single standard deviation. Figure 1 presents the actual running time of both algorithms, summed over all guesses of the bottleneck distance. For datasets with more than 106points, our algorithm takes roughly half as much time as the HK algorithm and the gap seems 3Our of our algorithm and HK can be found at 9 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 # time (seconds) (all guesses) HK LR 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 # # edges explored (all guesses)1e10 HK LRFigure 1: A running time comparison between the HK algorithm and our algorithm. Left: Comparison of actual running time. Right: Comparison of total number of edge visits. 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 # # Phases (last guess) HK LR 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 # ) / # phases HK ratio Figure 2: Data for number of phases for the ﬁnal bottleneck guess. Left: A comparison of the number of phases for the HK algorithm and our algorithm. by the number of phases executed by the HK algorithm. to grow as the input size increases. However, the actual running times can be affected by several factors including the exact implementation details and execution environment. Therefore, we focus on comparing metrics that are accurate independent of the exact implementation details. Recall that both algorithms combine variants of BFS and DFS to compute augmenting paths. As a result, the total number times edges are visited during each algorithm acts as an proxy of the running time. Figure 1 shows the total number of edge visits for both algorithms. Note that this data seems to follow a similar trend to the actual running times of the algorithms. Next, we summarize our observations that help account for this difference in performance of the two algorithms. For more details, see Section G of the supplement. Recall that there are four main differences (1) – (4) between the HK algorithm and our algorithm. As discussed in Section F of the supplement, differences (1) – (3) do not have any direct signiﬁcant impact on the relative running times of the two algorithms; the most signiﬁcant difference is (4) – Stage 2 of the our algorithm reuses edges from affected pieces. This reuse of edges has two main effects on the efﬁciency of the our algorithm. First, we ﬁnd that our algorithm executes signiﬁcantly fewer phases than the HK algorithm. Speciﬁcally, as the guess of the bottleneck distance approaches the actual bottleneck distance, our results suggest that the number of phases executed by the HK algorithm seems to grow at a rate of Θ(√n)– exhibiting its worst-case analysis. In contrast, the number of phases executed by our algorithm grows at a much slower rate (see Figure 2). This explains why the our algorithm runs faster than the HK algorithm. The second impact of allowing for edge revisits is that a single edge can be revisited, perhaps many times, during a single phase. Despite this, the total number of edges visited by our algorithm is still signiﬁcantly less than the total number of edges visited by the HK algorithm (see Figure 1). 5 Conclusion We consider the maximum cardinality matching problem and present a simpliﬁcation of a recent algorithm by Lahn and Raghvendra [ 28]. In particular, we eliminate the need to maintain dual weights in their algorithm. This not only leads to a simpler algorithm but also results in new and improved exact algorithms for computing the δ-disc graph matching, bottleneck matching, as well as and the Lévy-Prokhorov distances, in low-density settings. We would like to conclude by stating the following open question: Can we design a parallel combinatorial algorithm to compute aδ-disc graph matching? 10  We would like to acknowledge, Advanced Research Computing (ARC) at Virginia Tech, which provided us with the computational resources used to run the experiments. Research presented in this paper was funded by NSF CCF-1909171. We would like to thank the anonymous reviewers for their useful feedback.  
decision transformer reinforcement learning via sequence modeling 	Decision Transformer: Reinforcement Learning via Sequence Modeling Lili Chen∗,1, Kevin Lu∗,1, Aravind Rajeswaran2, Kimin Lee1, Aditya Laskin1,Pieter Mordatch†,5 ∗equal advising 1UC AI Brain {lilichen, Abstract We introduce a framework that abstracts Reinforcement Learning (RL) as a se- quence modeling problem. This allows us to draw upon the simplicity and scalabil- ity of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Un- like prior approaches to RL that ﬁt value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can gen- erate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of model-free ofﬂine RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks. Rsaa sa a causal transformer emb. + pos. enc. linear decoder . . . 21 return stateaction . . . ^R^ Figure 1: Decision Transformer architecture1. States, actions, and returns are fed into modality- speciﬁc linear embeddings and a positional episodic timestep encoding is added. Tokens are fed into a GPT architecture which predicts actions using a causal self-attention mask. 1Our code is available at: 35th Conference on Neural Information Processing Systems (NeurIPS 2021). ...goal -2-1 -3 -4-1 -3-4 -∞-∞-∞-∞graph training dataset (random walks) goal -20 0 -∞Figure 2: Illustrative example of ﬁnding shortest path for a ﬁxed graph (left) posed as reinforcement learning. Training dataset consists of random walk trajectories and their per-node returns-to-go (middle). Conditioned on a starting state and generating largest possible return at each node, Decision Transformer sequences optimal paths. 1 Introduction Recent work has shown transformers [ 1] can model large-scale distributions of semantic concepts, including capable zero-shot generalization in language [ 2] and impressive image generation [ 3]. This stands in sharp contrast to much work in reinforcement learning (RL), which learns a single policy to model a particular narrow behavior distribution. Given the diversity of applications and impact of transformer models, we seek to examine their application to sequential decision making problems. In particular, instead of using transformers as an architectural choice for traditional RL algorithms [ 4,5], we seek to study if trajectory modeling (analogous to language modeling) can serve as a replacement for conventional RL algorithms. We consider the following shift in paradigm: instead of training a policy through conventional RL algorithms like temporal difference (TD) learning [ 6], the dominant paradigm in RL, we will train transformer models on collected experience using a sequence modeling objective. This will allow us to bypass the need for bootstrapping to propagate returns – thereby avoiding one of the “deadly triad” [ 6] known to destabilize RL. It also avoids the need for discounting future rewards, as typically done in TD-learning, which can induce undesirable short-sighted behaviors. Additionally, we can make use of existing transformer frameworks widely used in language and vision that are easy to scale, utilizing a large body of work studying stable training of transformer models; this approach removes the need for specialized RL frameworks by appealing only to commonplace supervised learning systems. Given their demonstrated ability to model long sequences and wide data distributions, transformers also have other advantages. Transformers can perform credit assignment directly via self-attention, in contrast to Bellman backups which slowly propagate rewards and are prone to “distractor” signals [ 7]. This can enable transformers to still work effectively in the presence of sparse or distracting rewards. Furthermore, a transformer modeling approach can model a wide distribution of behaviors, enabling better generalization and transfer. While “upside-down” reinforcement learning (UDRL) [8, 9, 10] also uses a supervised loss conditioned on a target return, our work is motivated by sequence modeling rather than supervised learning and seeks to beneﬁt from modeling long sequences of behaviors. See Section 6 for more discussions about related works. We explore our hypothesis by considering ofﬂine RL, where we will task agents with learning policies from suboptimal data – producing maximally effective behavior from ﬁxed, limited experience. This task is traditionally challenging due to error propagation and value overestimation [ 11]. However, it is a natural task when training with a sequence modeling objective. By training an autoregressive model on sequences of states, actions, and returns, we reduce policy sampling to autoregressive generative modeling. We can specify the expertise of the policy – which “skill” to query – by manually setting the return tokens, acting as a prompt for generation. Illustrative example. To get an intuition for our proposal, consider the task of ﬁnding a shortest path on a directed graph posed as an RL problem. The reward is 0when at the goal node and −1otherwise. We train a GPT [ 12] model to predict next token in a sequence of returns-to-go (sum of future rewards), states, and actions. Training only on random walk data – with no expert demonstrations – we can at test time generate optimal trajectories by adding a prior to generate highest possible returns (see more details and empirical results in the Appendix) and subsequently generate actions 2 conditioned on that. Thus, by combining the tools of sequence modeling with hindsight return information, we achieve policy improvement without the need for dynamic programming. Motivated by this observation, we propose Decision Transformer, where we use the GPT architecture to model trajectories (shown in Figure 1). We study whether sequence modeling can perform policy optimization by evaluating Decision Transformer on ofﬂine RL benchmarks in Atari [ 13], OpenAI Gym [ 14], and Key-to-Door [ 15] environments. We show that – without using dynamic programming – Decision Transformer performs comparably on these benchmarks to model-free ofﬂine RL algorithms [ 16,17]. Furthermore, in tasks where long-term credit assignment is required, Decision Transformer capably outperforms RL algorithms. With this work, we hope to bridge vast recent progress in transformer models with RL problems. 2 Preliminaries 2.1 Ofﬂine reinforcement learning We consider learning in a Markov decision process (MDP) described by the tuple ( S,A,P,R). The MDP tuple consists of states s∈S, actionsa∈A, transition dynamics P(s′|s,a), and a reward ). We usest,at, denote the state, action, and reward at timestept, respectively. The goal in reinforcement learning is to learn a policy which maximizes the expected return E[∑T t=1rt] in an MDP. In ofﬂine reinforcement learning, instead of obtaining data via environment interactions, we only have access to some ﬁxed limited dataset consisting of trajectories from the environment. This setting is harder as it removes the ability for agents to explore the environment and collect additional feedback. 2.2 Transformers Transformers were proposed by Vaswani et al. [1]as an architecture to efﬁciently model sequences. They consist of stacked self-attention layers with residual connections. Each self-attention layer to unique input tokens, and outputs nembeddings {zi}n i=1, preserving the input dimensions. The i-th token is mapped via linear to a keyki, queryqi, and valuevi. Thei-th output of the self-attention layer is given by weighting the valuesvjby the normalized dot product between the query qiand other keys kj: zi=n∑ j=1softmax ({⟨qi,kj′⟩}n j′=1)j·vj. (1) This allows the layer to assign “credit” by implicitly forming state-return associations via similarity of the query and key vectors (maximizing the dot product). In this work, we use the GPT architec- ture [ 12], which modiﬁes the transformer architecture with a causal self-attention mask to enable autoregressive generation, replacing the over the ntokens with only the previous tokens in the sequence ( j∈[1,i]). We defer the other architecture details to the original papers. 3 Method In this section, we present Decision Transformer, which models trajectories with minimal modiﬁcation to the transformer architecture, as summarized in Figure 1 and Algorithm 1. Trajectory representation. The key desiderata in our choice of trajectory representation are (a) it should enable transformers to learn meaningful patterns and (b) we should be able to conditionally generate actions at test time. It is nontrivial to model rewards since we would like the model to generate actions based on future desired returns, rather than past rewards. As a result, instead of modeling the rewards directly, we model the returns-to-go ˆRt=∑T t′=trt′. This leads to the following trajectory representation which is amenable to autoregressive training and generation: τ=( ) . (2) Architecture. We feed the last Ktimesteps into Decision Transformer, for a total of 3Ktokens (one for each modality: return-to-go, state, or action). To obtain token embeddings, we learn a linear 3 layer for each modality, which projects raw inputs to the embedding dimension, followed by layer normalization [ 18]. For environments with visual inputs, the state is fed into a convolutional encoder instead of a linear layer. Additionally, an embedding for each timestep is learned and added to each token – note this is different than the standard positional embedding used by transformers, as one timestep corresponds to three tokens. The tokens are then processed by a GPT [ 12] model, which predicts future action tokens via autoregressive modeling. Training. We sample minibatches of sequence length Kfrom the dataset. The prediction head corresponding to the input token stis trained to predict at– either with cross-entropy loss for discrete actions or mean-squared error for continuous actions – and the losses for each timestep are averaged. We did not ﬁnd predicting the states or returns-to-go to be necessary for good performance, although it is possible (as shown in Section 5.3) and would be an interesting study for future work. Evaluation. During evaluation rollouts, we specify a target return based on our desired performance (e.g., specify maximum possible return to generate expert behavior) as well as the environment starting state, to initialize generation. After executing the generated action, we decrement the target return by the achieved reward and obtain the next state. We repeat this process of generating actions and applying them to obtain the next return-to-go and state until episode termination. Algorithm 1 Decision Transformer Pseudocode (for continuous actions) # R, s, a, t: returns -to -go , states , actions , or timesteps # K: context length ( length of each input to ) # transformer : transformer with causal masking (GPT) # embed_s , embed_a , embed_R : linear embedding layers # embed_t : learned episode positional embedding # pred_a : linear action prediction layer # main model def (R, s, a, t): # compute embeddings for tokens pos_embedding = embed_t (t) # per - timestep ( note : not per - token ) s_embedding = embed_s (s) + pos_embedding a_embedding = embed_a (a) + pos_embedding R_embedding = embed_R (R) + pos_embedding # interleave tokens as (R_1 , s_1 , a_1 , ... , R_K , s_K ) input_embeds = stack ( R_embedding , s_embedding , a_embedding ) # use transformer to get hidden states hidden_states = transformer ( input_embeds = input_embeds ) # select hidden states for action prediction tokens a_hidden = unstack ( hidden_states ). actions # predict action return pred_a ( a_hidden ) # training loop for (R, s, a, t) in dataloader : # dims : ( batch_size , K, dim ) a_preds = (R, s, a, t) loss = mean (( a_preds - a )**2) # L2 loss for continuous actions optimizer . zero_grad (); loss . backward (); optimizer . step () # evaluation loop target_return = 1 # for instance , expert - level return R, s, a, t, done = [ target_return ], [env. reset ()] , [], [1] , False while not done : # autoregressive generation / sampling # sample next action action = (R, s, a, t)[ -1] # for cts actions new_s , r, done , _ = env. step ( action ) # append new tokens to sequence R = R + [R[ -1] - r] # decrement returns -to -go with reward s, a, t = s + [ new_s ], a + [ action ], t + [len(R)] R, s, a, t = R[-K:], ... # only keep context length of K 4 Atari OpenAI Gym Transformer (Ours) TD Learning Behavior CloningFigure 3: Results comparing Decision Transformer (ours) to TD learning (CQL) and behavior cloning across Atari, OpenAI Gym, and Minigrid. On a diverse set of tasks, Decision Transformer performs comparably or better than traditional approaches. 4 Evaluations on ofﬂine RL benchmarks In this section, we investigate if Decision Transformer can perform well compared to standard TD and imitation learning approaches for ofﬂine RL. TD learning algorithms represent the conventional state- of-the-art, while imitation learning algorithms have similar formulations to Decision Transformer. The exact algorithms depend on the environment but our motivations are as follows: •TD learning : most of these methods use an action-space constraint or value pessimism, and will be the most faithful comparison to Decision Transformer, representing standard RL methods. A model-free method is Conservative Q-Learning (CQL) [ 17] which serves as our primary comparison. In addition, we also compare against other prior model-free RL algorithms like BEAR [19] and BRAC [20]. •Imitation learning : this regime similarly uses supervised losses for training, rather than Bellman backups. We use behavior cloning here, and include a more detailed discussion in Section 5.1. We evaluate on both discrete (Atari [ 13]) and continuous (OpenAI Gym [ 14]) control tasks. The former requires long-term credit assignment, while the latter requires ﬁne-grained continuous control, representing a diverse set of tasks. Our main results are summarized in Figure 3, where we show averaged expert normalized performance for each domain. 4.1 Atari The Atari benchmark is challenging due to its visual inputs and difﬁculty of credit assignment arising from the delay between actions and resulting rewards. We evaluate our method on 1% of all samples in the DQN-replay dataset as per Agarwal et al. [16], representing 500 thousand of the 50 million transitions observed by an online DQN agent [ 21] during training; we report the mean and standard deviation of 3 seeds. We normalize scores based on a professional gamer, following the protocol of Hafner et al. [22], where 100 represents the professional gamer score and 0 represents a random policy. We compare to CQL [ 17], REM [ 16], and QR-DQN [ 23] on four Atari tasks (Breakout, Qbert, Pong, and Seaquest) that are evaluated in Agarwal et al. [16]. We use context lengths of K= 30 for Decision Transformer (except K= 50 for Pong); for results with different values of Ksee the supplementary material. We also report the performance of behavior cloning (BC), which utilizes Game DT (Ours) CQL QR-DQN REM BC Breakout 267.5±97.5 211.1 21 .1 32 .1 138 .9±61.7 Qbert 15.1±11.4104.2 1.7 1 .4 17 .3±14.7 Pong 106.1±8.1111.9 20.0 39 .1 85 .2±20.0 Seaquest 2.4±0.7 1.7 1 .4 1 .0 2 .1±0.3 Table 1: scores for the 1% DQN-replay Atari dataset. We report the mean and variance across 3 seeds. Best mean scores are highlighted in bold. Decision Transformer (DT) performs comparably to CQL on 3 out of 4 games, and outperforms other baselines in most games. 5 the same network architecture and as Decision Transformer but does not have return-to-go conditioning2. For CQL, REM, and QR-DQN baselines, we report numbers directly from the CQL paper. We show results in Table 1. Our method is competitive with CQL in 3 out of 4 games and outperforms or matches REM, QR-DQN, and BC on all 4 games. 4.2 OpenAI Gym In this section, we consider the continuous control tasks from the D4RL benchmark [ 24]. We also consider a 2D reacher environment that is not part of the benchmark, and generate the datasets using a similar methodology to the D4RL benchmark. Reacher is a task and has sparse rewards, so it represents a different setting than the standard locomotion environments (HalfCheetah, Hopper, and Walker). The different dataset settings are described below. 1.Medium: 1 million timesteps generated by a “medium” policy that achieves approximately one-third the score of an expert policy. : the replay buffer of an agent trained to the performance of a medium policy (approximately 25k-400k timesteps in our environments). : 1 million timesteps generated by the medium policy concatenated with 1 million timesteps generated by an expert policy. We compare to CQL [ 17], BEAR [ 19], BRAC [ 20], and AWR [ 25]. CQL represents the state-of- the-art in model-free ofﬂine RL, an instantiation of TD learning with value pessimism. Score are normalized so that 100 represents an expert policy, as per Fu et al. [24]. CQL numbers are reported from the original paper; BC numbers are run by us; and the other methods are reported from the D4RL paper. Our results are shown in Table 2. Decision Transformer achieves the highest scores in a majority of the tasks and is competitive with the state of the art in the remaining tasks. Dataset Environment DT (Ours) CQL BEAR BRAC-v A WR BC Medium-Expert HalfCheetah 86.8±1.3 62.4 53 .4 41 .9 52 .7 59 .9 Medium-Expert Hopper 107.6±1.8111.0 96.3 0 .8 27 .1 79 .6 Medium-Expert Walker 108.1±0.2 98.7 40 .1 81 .6 53 .8 36 .6 Medium-Expert Reacher 89.1±1.3 30.6 - - - 73.3 Medium HalfCheetah 42.6±0.1 44 .4 41 .7 46.3 37.4 43 .1 Medium Hopper 67.6±1.0 58.0 52 .1 31 .1 35 .9 63 .9 Medium Walker 74.0±1.4 79 .2 59 .1 81.1 17.4 77 .3 Medium Reacher 51.2±3.4 26.0 - - - 48.9 Medium-Replay HalfCheetah 36.6±0.8 46 .2 38 .6 47.7 40.3 4 .3 Medium-Replay Hopper 82.7±7.0 48.6 33 .7 0 .6 28 .4 27 .6 Medium-Replay Walker 66.6±3.0 26.7 19 .2 0 .9 15 .5 36 .9 Medium-Replay Reacher 18.0±2.4 19 .0 - - - 5.4 Average (Without Reacher) 74.7 63.9 48 .2 36 .9 34 .3 46 .4 Average (All Settings) 69.2 54.2 - - - 47.7 Table 2: Results for D4RL datasets4. We report the mean and variance for three seeds. Decision Transformer (DT) outperforms conventional RL algorithms on almost all tasks. 5 Discussion 5.1 Does Decision Transformer perform behavior cloning on a subset of the data? In this section, we seek to gain insight into whether Decision Transformer can be thought of as performing imitation learning on a subset of the data with a certain return. To investigate this, we propose a new method, Percentile Behavior Cloning (%BC), where we run behavior cloning on only the topX%of timesteps in the dataset, ordered by episode returns. The percentile X%interpolates between standard BC ( X= 100% ) that trains on the entire dataset and only cloning the best observed 2We also tried using an MLP with K= 1as in prior work, but found this was worse than the transformer. 4Given that CQL is generally the strongest TD learning method, for Reacher we only run the CQL baseline. 6 Dataset Environment DT (Ours) 10%BC 25%BC 40%BC 100%BC CQL Medium HalfCheetah 42.6±0.1 42 .9 43 .0 43 .1 43 .144.4 Medium Hopper 67.6±1.0 65.9 65 .2 65 .3 63 .9 58 .0 Medium Walker 74.0±1.4 78 .8 80.9 78.8 77 .3 79 .2 Medium Reacher 51.2±3.4 51 .0 48 .9 58 .2 58.4 26.0 Medium-Replay HalfCheetah 36.6±0.8 40 .8 40 .9 41 .1 4 .346.2 Medium-Replay Hopper 82.7±7.0 70.6 58 .6 31 .0 27 .6 48 .6 Medium-Replay Walker 66.6±3.0 70.4 67.8 67 .2 36 .9 26 .7 Medium-Replay Reacher 18.0±2.4 33.1 16.2 10 .7 5 .4 19 .0 Average 56.1 56.7 52.7 49 .4 39 .5 43 .5 Table 3: Comparison between Decision Transformer (DT) and Percentile Behavior Cloning (%BC). trajectory (X→0%), trading off between better generalization by training on more data with training a specialized model that focuses on a desirable subset of the data. We show full results comparing %BC to Decision Transformer and CQL in Table 3, sweeping over %] . Note that while both %BC and DT introduce , returns are human interpretable and it is relatively natural for humans to specify a desired return compared to choosing an optimal subset for cloning. When data is plentiful – as in the D4RL regime – we ﬁnd %BC can match or beat other ofﬂine RL methods. On most environments, Decision Transformer is competitive with the performance of the best %BC, indicating it can hone in on a particular subset after training on the entire dataset distribution. In contrast, when we study low data regimes – such as Atari, where we use 1% of a replay buffer as the dataset – %BC is weak (shown in Table 4). This suggests that in scenarios with relatively low amounts of data, Decision Transformer can outperform %BC by using all trajectories in the dataset to improve generalization, even if those trajectories are dissimilar from the return conditioning target. Our results indicate that Decision Transformer can be more effective than simply performing imitation learning on a subset of the dataset. On the tasks we considered, Decision Transformer either outperforms or is competitive to %BC, without the confound of having to select the optimal subset. Game DT (Ours) 10%BC 25%BC 40%BC 100%BC Breakout 267.5±97.5 28.5±8.2 73 .5±6.4 108 .2±67.5 138 .9±61.7 Qbert 15.1±11.4 6 .6±1.7 16 .0±13.8 11 .8±5.817.3±14.7 Pong 106.1±8.1 2.5±0.2 13 .3±2.7 72 .7±13.3 85 .2±20.0 Seaquest 2.4±0.7 1.1±0.2 1 .1±0.2 1 .6±0.4 2 .1±0.3 Table 4: %BC scores for Atari. We report the mean and variance across 3 seeds. Decision Transformer (DT) outperforms all versions of %BC in most games. 5.2 How well does Decision Transformer model the distribution of returns? We evaluate the ability of Decision Transformer to understand return-to-go tokens by varying the desired target return over a wide range – evaluating the multi-task distribution modeling capability of transformers. Figure 4 shows the average sampled return accumulated by the agent over the course of the evaluation episode for varying values of target return. On every task, the desired target returns and the true observed returns are highly correlated. On some tasks like Pong, HalfCheetah and Walker, Decision Transformer generates trajectories that almost perfectly match the desired returns (as indicated by the overlap with the oracle line). Furthermore, on some Atari tasks like Seaquest, we can prompt the Decision Transformer with higher returns than the maximum episode return available in the dataset, demonstrating that Decision Transformer is sometimes capable of extrapolation. 5.3 Does Decision Transformer perform effective long-term credit assignment? To evaluate long-term credit assignment capabilities of our model, we consider a variant of the Key-to-Door environment proposed in Mesnard et al. [15]. This is a grid-based environment with a sequence of three phases: (1) in the ﬁrst phase, the agent is placed in a room with a key; (2) then, the 7  0 50 100 0 1 2 30123Seaquest 01020304050 Target Return ( 0 25 50 75100 Target Return ( 0 25 50 75100 Target Return ( 0 510152025 Target Return ( 0.04 0.02 0.000.05 Decision Transformer Oracle Best Trajectory in DatasetFigure 4: Sampled (evaluation) returns accumulated by Decision Transformer when conditioned on the speciﬁed target (desired) returns. Top: Atari. Bottom: D4RL medium-replay datasets. agent is placed in an empty room; (3) and ﬁnally, the agent is placed in a room with a door. The agent receives a binary reward when reaching the door in the third phase, but only if it picked up the key in the ﬁrst phase. This problem is difﬁcult for credit assignment because credit must be propagated from the beginning to the end of the episode, skipping over actions taken in the middle. We train on datasets of trajectories generated by applying random actions and report success rates in Table 5. Furthermore, for the Key-to-Door environment we use the entire episode length as the context, rather than having a ﬁxed content window as in the other environments. Methods that use highsight return information: our Decision Transformer model and %BC (trained only on successful episodes) are able to learn effective policies – producing near-optimal paths, despite only training on random walks. TD learning (CQL) cannot effectively propagate Q-values over the long horizons involved and gets poor performance. Dataset DT (Ours) CQL BC %BC Random 1K Random Trajectories 71.8% 13 .1% 1 .4% 69 .9% 3 .1% 10K Random Trajectories 94.6% 13 .3% 1 .6% 95.1% 3 .1% Table 5: Success rate for Key-to-Door environment. Methods using hindsight (Decision Transformer, %BC) can learn successful policies, while TD learning struggles to perform credit assignment. 5.4 Can transformers be accurate critics in sparse reward settings? In previous sections, we established that decision transformer can produce effective policies (actors). We now evaluate whether transformer models can also be effective critics. We modify Decision Transformer to output return tokens in addition to action tokens on the Key-to-Door environment. We ﬁnd that the transformer continuously updates reward probability based on events during the episode, shown in Figure 5 (Left). Furthermore, we ﬁnd the transformer attends to critical events in the episode (picking up the key or reaching the door), shown in Figure 5 (Right), indicating formation of state-reward associations as discussed in Raposo et al. [26] and enabling accurate value prediction. 5.5 Does Decision Transformer perform well in sparse reward settings? A known weakness of TD learning algorithms is that they require densely populated rewards in order to perform well, which can be unrealistic and/or expensive. In contrast, Decision Transformer can improve robustness in these settings since it makes minimal assumptions on the density of the reward. To evaluate this, we consider a delayed return version of the D4RL benchmarks where the agent does not receive any rewards along the trajectory, and instead receives the cumulative reward of the trajectory in the ﬁnal timestep. Our results for delayed returns are shown in Table 6. Delayed returns minimally affect Decision Transformer; and due to the nature of the training process, while imitation learning methods are reward agnostic. While TD learning collapses, Decision Transformer and %BC still perform well, indicating that Decision Transformer can be more robust to delayed rewards. 8 key room distractor room door room Episode reward probabilitynot pick up key pick up key and reach door pick up key and not reach door pick up key reach door Episode weight Figure 5: Left: Averages of running return probabilities predicted by the transformer model for three types of episode outcomes. Right: Transformer attention weights from all timesteps superimposed for a particular successful episode. The model attends to steps near pivotal events in the episode, such as picking up the key and reaching the door. Delayed (Sparse) Agnostic Original (Dense) Dataset Environment DT (Ours) CQL BC %BC DT (Ours) CQL Medium-Expert Hopper 107.3±3.5 9.0 59.9 102 .6 107.6 111 .0 Medium Hopper 60.7±4.5 5 .2 63.965.9 67.6 58 .0 Medium-Replay Hopper 78.5±3.7 2.0 27.6 70 .6 82.7 48 .6 Table 6: Results for D4RL datasets with delayed (sparse) reward. Decision Transformer (DT) and imitation learning are minimally affected by the removal of dense rewards, while CQL fails. 5.6 Additional Discussions For more discussions see the supplementary material. 6 Related work Ofﬂine reinforcement learning. To mitigate the impact of distribution shift in ofﬂine RL, prior algorithms either (a) constrain the policy action space [ 27,28,29] or (b) incorporate value pes- simism [ 27,17], or (c) incorporate pessimism into learned dynamics models [ 30,31]. Since we do not use Decision Transformers to explicitly learn the dynamics model, we primarily compare against model-free algorithms; adding a dynamics model tends to improve the performance of model-free algorithms. Another line of work explores learning wide behavior distribution from an ofﬂine dataset by learning a task-agnostic set of skills, either with approaches [ 32,33,34,35] or by maximizing mutual information [ 36,37,38]. Our work is similar to the approaches, which do not use iterative Bellman updates – although we use a simpler sequence modeling objective instead of a variational method, and use rewards for conditional generation of behaviors. Supervised learning in reinforcement learning settings. Some prior methods for reinforcement learning bear more resemblance to static supervised learning, such as Q-learning [ 39,40], which still uses iterative backups, or methods such as behavior cloning, which do not (discussed in previous section). Recent work [ 8,9,10] studies “upside-down” reinforcement learning (UDRL), which are similar to our method in seeking to model behaviors with a supervised loss conditioned on the target return. A key difference in our work is the shift of motivation to sequence modeling rather than supervised learning: while the practical methods differ primarily in the context length and architecture, sequence modeling enables behavior modeling even without access to the reward, in a similar style to language [ 12] or images [ 41], and is known to scale well [ 2]. The method proposed by Kumar et al. [9]is most similar to our method with K= 1, which we ﬁnd sequence modeling/long contexts to outperform (see supplementary material). Ghosh et al. [42] extends prior UDRL methods to use state goal conditioning, rather than rewards, and Paster et al. [43] further use an LSTM with state goal conditioning for online RL settings. Concurrent to our work, Janner et al. [44] propose Trajectory Transformer, which is similar to Decision Transformer but additionally uses state and return prediction, as well as discretization, which incorporates model-based components. 9 We believe that their experiments, in addition to our results, highlight the potential for sequence modeling to be a generally applicable idea for reinforcement learning. Credit assignment. Many works have studied better credit assignment via , learning an architecture which decomposes the reward function such that certain “important” states comprise most of the credit [ 45,46,15]. They use the learned reward function to change the reward of an actor- critic algorithm to help propagate signal over long horizons. In particular, similar to our long-term setting, some works have speciﬁcally shown such architectures can perform better in delayed reward settings [ 47,7,48,26]. In contrast, we allow these properties to naturally emerge in a transformer architecture, without having to explicitly learn a reward function or a critic. Conditional language generation. Various works have studied guided generation for images [ 49] and language [ 50,51]. Several works [ ] have explored training or ﬁne-tuning of models for controllable text generation. language models can also be used to learn disciminators to guide generation [ 58,50,59,60]. However, these approaches mostly assume constant “classes”, while in reinforcement learning the reward signal is time-varying. Furthermore, it is more natural to prompt the model desired target return and continuously decrease it by the observed rewards over time, since the transformer model and environment jointly generate the trajectory. Attention and transformer models. Transformers [ 1] have been applied successfully to many tasks in natural language processing [ 61,12] and computer vision [ 62,63]. However, transformers are relatively unstudied in RL, mostly due to differing nature of the problem, such as higher variance in training. Zambaldi et al. [5]showed that augmenting transformers with relational reasoning improve performance in combinatorial environments and Ritter et al. [64] showed iterative self- attention allowed for RL agents to better utilize episodic memories. Parisotto et al. [4]discussed design decisions for more stable training of transformers in the high-variance RL setting. Unlike our work, these still use actor-critic algorithms for optimization, focusing on novelty in architecture. Additionally, in imitation learning, some works have studied transformers as a replacement for LSTMs: Dasari and Gupta [65] study one-shot imitation learning, and Abramson et al. [66] combine language and image modalities for behavior generation. 7 Conclusion We proposed Decision Transformer, seeking to unify ideas in language modeling and RL. On standard ofﬂine RL benchmarks, we showed DT can match or outperform strong algorithms designed explicitly for ofﬂine RL with minimal modiﬁcations from standard language modeling architectures. Societal impact. For real-world applications, it is important to understand the types of errors transformers make in MDP settings and possible negative consequences. It will also be important to consider the datasets we train on, which can potentially add destructive biases, particularly as we consider studying augmenting RL agents with more data which may come from questionable sources. Limitations. We introduced our paradigm shift and showed its potential in our experiments, but there is signiﬁcant room for more research in this direction. The current architecture requires considerations of context length and return-to-go , and we show results on standard RL benchmarks; future work could improve the architecture and demonstrate results in more complex environments and tasks. We used a simple supervised loss that was effective in our experiments, but applications to large-scale datasets could beneﬁt from pretraining tasks. In addition, one could consider more sophisticated embeddings for returns, states, and actions. While we do not directly evaluate scaling and generalization, we utilize a method known to scale generalize well in domains such as language and vision, and we are excited about larger RL systems built upon our framework. 8 This research was supported by Berkeley Deep Drive, Open Philanthropy, and the National Science Foundation under NSF:NRI #2024675. Part of this work was completed when Aravind Rajeswaran was a PhD student at the University of Washington, where he was supported by the J.P. Morgan PhD Fellowship in AI (2020-21). We also thank Luke Metz, Daniel Freeman, and anonymous reviewers for valuable feedback and discussions, as well as Justin Fu for assistance in setting up D4RL benchmarks, and Aviral Kumar for assistance with the CQL baselines and . 10  
end to end training of multi document reader and retriever for open domain question answering 	End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering Devendra Singh Sachan1,2, Siva Reddy1,2, William Hamilton1,2, Chris Dyer3, Dani Yogatama3 1Mila - Quebec AI Institute 2School of Computer Science, McGill University 3DeepMind , {siva, {cdyer, Abstract We present an end-to-end differentiable training method for open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is hard, we approximate this using an algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to ﬂow to the reader and then to the retriever better than stage-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3 absolute exact match points, achieving new state-of-the- art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions. 1 Introduction Open-domain question answering (OpenQA) is a question answering task where the goal is to train a language model to produce an answer for a given question. In contrast to many question answering tasks, an OpenQA model is only provided with the question as its input without accompanying documents that contain the answer. One of the most promising approaches to OpenQA is based on augmenting the language model with an external knowledge source such as Wikipedia (often referred to as the evidence documents). In this approach, the model consists of two core components ( Chen et al. ,2017 ): (i) an information retrieval system to identify useful pieces of text from the knowledge source (the retriever); and (ii) a system to produce the answer given the retrieved documents and the question (the reader). We can view such a model as a latent variable model, where the latent variables represent retrieved documents that are used to produce answers given questions ( Leeet al. ,2019 ). End-to-end (joint) training of this model is challenging since we need to learn both to generate an answer given retrieved documents and what to retrieve. Previous work considers two potential solutions (see Table 1for a high-level summary). First, they adopt a stage-wise training, where the retriever is trained while freezing the reader and vice versa ( Karpukhin et al. ,2020 ,Izacard and Grave ,2021b ,a). Another 35th Conference on Neural Information Processing Systems (NeurIPS 2021). Reader and Retriever Training Model Multi-Doc End-to-End Multi-Step Unsupervised Retriever REALM ( Guu et al. ,2020 ) 33 3 DPR ( Karpukhin et al. ,2020 ) 3 RAG ( Lewis et al. ,2020b ) 33 FiD ( Izacard and Grave ,2021b ) 33 FiD-KD ( Izacard and Grave ,2021a ) 33 3 EMDR2(Our Approach) 33 3 3 Table 1: Bird’s-eye view of the recent OpenQA approaches. Multi-Doc reader indicates whether the reader architecture uses multiple documents or a single document. Retriever adaptation shows whether the retriever gets feedback from the reader to update its parameters. Disjoint denotes that ﬁrst the retriever is trained and then the reader is trained. End-to-end denotes that the reader and retriever are trained jointly in one cycle. Multi-step indicates that the reader and retriever are trained iteratively in multiple cycles. Unsupervised retriever indicates whether the retriever is initialized using unsupervised approaches or using supervised data. alternative is to constraint the reader to condition on each retrieved document et al. ,2020 )—sometimes with extra supervision for the latent variables in the form of the relevant document for a question ( Lewis et al. ,2020b ). In this paper, we consider a question answering model that combines information from multiple documents when generating answers. ( Dempster et al. , 1977 ) offers a principled template for learning this class of latent variable models. We present training of Multi-Document Reader and Retriever (§ uses feedback from the model itself as “pseudo labels” of the latent variables for optimizing the retriever and reader parameters. We use two estimates of the latent variables: (i) prior scores for updating the reader parameters and (ii) approximate posterior scores given all observed variables for the retriever parameters. We evaluate our proposed method by experimenting on three commonly used OpenQA datasets: Natural Questions, TriviaQA, and WebQuestions (§ 3). E MDR2achieves new results for models of comparable size on all datasets, outperforming recent approaches by 2-3 absolute exact match points. We also show that EMDR2is robust to retriever initialization. It achieves high accuracy with unsupervised initialization, suggesting that supervised training of the retriever may not be an essential component of the training process as suggested in prior work ( Karpukhin et al. ,2020 ). In summary, our contributions are as follows: (i) we present an end-to-end training method ( EMDR2) for systems; (ii) we demonstrate that other existing approaches of comparable size without any kind of supervision on the latent variables; (iii) we provide ablation studies for a better understanding of the contributions of different components of our proposed method; and (iv) we release our code and checkpoints to facilitate future work and for EMDR2is a framework that can be used to train text generation models for any task. We believe that our estimation technique in EMDR2is also useful for learning similar latent variable models in other domains. 2 Model Our proposed model EMDR2consists of two components: (i) a neural retriever and (ii) a neural reader, which we train jointly in an end-to-end setting. Figure 1shows an illustration of our model and training procedure. We discuss each component and our training objective in detail below. 1This makes over the latent variables easier since we only need to consider one document at a time rather than multiple documents at once. 2Our code is available at: 2 2.1 Neural Retriever: Dual Encoder Let the collection of evidence documents be denoted by D={d1,..., dM}. Given a question q, the goal of the retriever module is to select a subset of documents Z⇢D to answer the question. We model the retriever as a dual-encoder network ( Bromley et al. ,1994 ), where one encoder fq encodes the question and another fdencodes the evidence document (to a vector). The retrieval score is deﬁned as the dot product between the two resulting vectors: score (q,di; )= ), (1) where  =[   q, d]denotes the retriever parameters. We select top- Kdocuments for the question q from Dbased on the retrieval scores. We denote the set of retrieved documents by Z={z1,..., zK}. We use transformer encoders ( Vaswani et al. ,2017 ) as our fqandfd. Our transformer architecture is similar to BERT with 12 layers and 768 hidden size ( Devlin et al. ,2019 ). We use the ﬁnal representation of the ﬁrst token (i.e., the standard [CLS] token from BERT’s tokenization) as our question (and similarly document) embedding. Initializing fqandfdwith BERT weights has been shown to lead to a poor retrieval accuracy ( Leeet al. ,2019 ,Sachan et al. ,2021 ). Therefore, we initialize the retriever with an unsupervised training procedure. We discuss our initialization technique in detail in § 3.2. 2.2 Neural Reader: The reader takes as input a question qand a set of retrieved documents (to be read) Zto generate an answer. Our reader is based on the (FiD; Izacard and Grave ,2021b ) model, which is built on top of T5 ( Raffel et al. ,2020 ). T5 is a pretrained transformer that consists of an encoder geand a decoder gd. In FiD, each retrieved document zkis ﬁrst appended with its title ( tzk) and the question: xk=[CLS] q[SEP] tzk[SEP] zk[SEP] , where [CLS] is used to indicate the start of a document and [SEP] is used as a separator for the different parts of the document as well as the ﬁnal token. Each xkis then independently given as an input to the T5 encoder ge. The output corresponding to all of the retrieved documents are concatenated as: , where Nis the number of tokens in each xk3andHis the hidden size of the T5 encoder ge. In this work, we use the T5- base conﬁguration with N= 512 andH= 768 . XZis then given as an input to the T5 decoder gd. When generating an answer token, the decoder attends to both previously generated tokens (i.e., causal attention) as well as the tokens encoded in XZ(i.e., cross attention). Since XZcontains information from multiple documents, the decoder has the ability to aggregate useful signals contained in multiple documents and jointly reason over them. We deﬁne the probability of the answer as: p(a|q,Z;⇥)=TY ;⇥), (2) where ⇥denotes the reader parameters (i.e., T5 encoder and decoder) and Tis the number of answer tokens. We keep generating answer tokens until the decoder outputs a special EOStoken or a pre-speciﬁed maximum answer length is reached. 2.3 End-to-End Training of Reader and Retriever In contrast to previous work on generative question answering, we train both the reader and the retriever jointly in an end-to-end differentiable fashion. Denote our latent variable which represents a set of retrieved documents by Zand let Zbe a possible value of Z. The marginal likelihood of an answer (marginalizing over all the possible values of Z) 3We truncate and pad as necessary such that every xkhas the same length N. See § 3.2for details. 3 T5 EncodeUT5 DecodeUT5 DiVWUibXWed EYidenceDoc Inde[ oYeU GPUVWop-k MIPS¬ Top-k PUodXcWReadeU TUaining TUaining SWale EYidenceDoc Inde[ RefUeVhFigure 1: An illustration of the different components of EMDR2. Colored blocks indicate components which contain trainable parameters. is:p(a|q;⇥, ) =P ; ). The goal of our training procedure is to ﬁnd and⇥that would maximize the above objective. Exactly optimizing Eq. 3is intractable as it is combinatorial in nature.4For one particular value Z, the log-likelihood is simpler to compute: ;  ) = log p(a|q,Z; ⇥) + log p(Z|q; ). (EM) algorithm ( Dempster et al. ,1977 ) offers a solution to learning this latent variable model. In classical EM, we iteratively compute the posterior of Zgiven all observed variables and use it to update ⇥and . We propose using two estimates of Z—Zreader andZretriever —for updating the two components of the model (reader parameters ⇥and retriever parameters  ): ;⇥)| {z } reader+ log p(Zretriever |q; )|{z } retriever. (3) In the ﬁrst term, we set the value of the latent variable Z=Zreader based on the prior scores. In the second term, we seek to maximize an approximate posterior of Z=Zretriever . We discuss them in more detail below. Reader parameters ⇥.For updating ⇥(the ﬁrst term of Eq. 3), we use the top- Kdocuments with the highest individual scores (as computed by Eq. 1based on the current value of  ) to construct Zreader. This is equivalent to relying on the prior p(Z|q; ) to estimate Zreader (without using information from the answer a). We choose to use the prior to train reader parameters since the prior scores are also used at evaluation time to obtain the top- Kdocuments. As a result, there is no mismatch between training and test computations when computing p(a|q,Z;⇥) (i.e., Zthat is used at test time is obtained in exactly the same way as Zreader =Ztop-K). Retriever parameters  .For updating  (the second term of Eq. 3), we propose to use the posterior estimate. In other words, we use additional information from awhen evaluating Zretriever to train  . Using the posterior allows our retriever to learn from richer training signals as opposed to relying only on the prior. We need to be able to compute p(Zretriever |q,a;⇥, )to maximize the retriever parameters. However, computing this quantity is difﬁcult since it is a probability of a set.5Consider a set of Kdocuments (e.g., Ztop-K), where zkdenotes a document in the set. We approximate the maximization of the probability of the set by assuming that its probability is maximized if the sum of the probability of 4Contrast our objective with REALM ( Guu et al. ,2020 ), where the reader only conditions on one retrieved document zkwhen generating an answer. In this case, the latent variable represents a document assignment instead of a set of retrieved documents. 5This is true whether we choose to use the posterior probability or the prior probability. 4 each document in the set is this approximation, we arrive at a simpler quantity:PK k=1p(zk|q,a;⇥, ). Note that using Bayes rule, we can rewrite:7 ; ). (4) The reader now only conditions on one document when computing the probability of an answer p(a|q,zk;⇥). This simpler reader uses the same parameters as the more sophisticated one ⇥, but it only uses one document zkinstead of a set of documents. To compute Eq. 4, we ﬁrst obtain Kdocuments with the highest scores as computed by Eq. 1based on the current value of  . We compute the probability of document zk2Z top-Kas: p(zk|q,Ztop-K; ) ⇡exp( score (q,zk)/⌧; )PK j=1exp( score (q,zj)/⌧; ), (5) where ⌧is a temperature hyperparameter and the approximation assumes that documents beyond the very small scores so we do not need to sum over all evidence documents Min the denominator (which is in the order of tens of millions in our experiments). We then compute p(a|q,zk;⇥) similarly to Eq. 2. Overall training objective of E MDR2.Combining the above derivations, our end-to-end training objective that we seek to maximize for a particular example becomes: L= log p(a|q,Ztop-K;⇥)| {z } reader+ logKX k=1SG(p(a|q,zk;⇥) ) p(zk|q,Ztop-K; ) | {z } retriever, (6) where SGis the stop-gradient operator so that the reader parameters ⇥are not updated to also perform well given a single document zk. The stop-gradient operator in the second term of EMDR2has several beneﬁts. First, the FiD reader is trained from the ﬁrst term of the EMDR2objective in which its likelihood is conditioned on all the retrieved documents, similar to how the reader is used at test time. Second, it also makes training faster since the backward pass which is more expensive than the forward pass is not needed, which in turn reduces the usage of GPU RAM as intermediate activations need not be saved. Given a training example, we update ⇥and by taking gradients of Eq. 6with respect to ⇥and in an end-to-end fashion. Intuitively, we train the reader to generate the correct answer given Khighest scoring documents Ztop-K. For the retriever, we train it to select Kdocuments which collectively has a high score of generating an answer (since the sum over Kis inside the log in the second term) while taking into account feedback from the reader. Algorithm 1summarizes our training algorithm. Algorithm 1: End-to-end training of multi-document reader and retriever. Input: Model parameters ⇥and , evidence documents D. while not converged do • Compute Ztop-Kusing the current retriever parameters  . //E-step • Compute p(a|q,zk)for each zkusing the current reader parameters ⇥.//E-step •Update model parameters ⇥and to maximize the log-likelihood in Eq. 6.//M-step end 3 Experiments 3.1 Datasets We experiment with three commonly used open-domain question answering datasets: 6The intuition is that each element of the set contributes independently, which greatly simpliﬁes the computa- tion to ﬁnd the maximum of the set. 7We choose not to normalize with p(a|q;⇥, ) since computing this quantity would require summing over all evidence documents M. While this makes the resulting objective that we optimize not correspond to a proper probability distribution anymore, we observe that our training method still behaves well in practice. 5 •Natural Questions (NQ; Kwiatkowski et al. ,2019).NQ contains questions asked by users of the Google search engine. Similar to Leeet al. (2019 ), we use the short answer subset. •TriviaQA ( Joshi et al. ,2017).TriviaQA is a collection of trivia pairs that were collected from multiple sources on the web. •WebQuestions (WebQ; Berant et al. ,2013).WebQ questions were collected using Google Suggest API and the answers were annotated using Mechanical Turk. We use the version from Chen et al. (2017 ) where Freebase IDs in the answers are replaced by entity names. Evidence documents D.We use the preprocessed English Wikipedia dump from December 2018 released by Karpukhin et al. (2020 ) as our evidence documents. Each Wikipedia article is split into 100 words long segments. Each segment corresponds to a document in our case. There are a total of 21,015,324 documents in total. We provide descriptive statistics and other preprocessing details in Appendix A. 3.2 Implementation Details Hardware and library. We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs. We use PyTorch ( Paszke et al. ,2019 ) to implement our proposed model and relevant baselines. Model conﬁgurations. For both the retriever and reader, we use the base conﬁguration that consists of 12 layers, 768 dimensional hidden size, and 12 attention heads. In all experiments, we retrieve 50 documents, unless stated otherwise. We only use the base conﬁguration in our experiments due to GPU memory constraints. However, we believe that our results would generalize to larger conﬁgurations as well. Retrieval. To support fast retrieval, we pre-compute evidence document embeddings and store them in a distributed fashion over all the GPUs. We refer to these document embeddings as the document index. For each question, we retrieve documents in an online (on-the-ﬂy) manner by performing exact maximum inner product search (MIPS), implemented using asynchronous distributed matrix multiplication over the document index. These documents are converted to subwords using BERT’s tokenization and are given as input to the T5 reader. If a tokenized document is shorter than 512 tokens, it is padded using the tokens from the neighboring documents until the maximum token limit is reached. Such padding additionally helps to provide an extended context for answer generation. Initialization and training details. We initialize the parameters of the model with unsupervised pre-training before performing supervised training using the training examples. Unsupervised pre-training is essential as it helps to warm-start the retriever so that it outputs relevant documents for a given question. We ﬁrst pre-train the retriever parameters with unsupervised Inverse Cloze Task training ( Leeet al. , 2019 ) for 100,000 steps. We then extract sentences containing named entities from the evidence documents. Next, we replace 15% of the named entity tokens with masked tokens, which are often referred to as masked salient spans (MSS; Guu et al. ,2020 ). The masked sentence can be considered as the question and its salient spans (i.e, named entities) can be considered as the answer to train the model with Eq. 6. We train the model on these (masked sentence-named entities) pairs for 82,000 steps with a batch size of 64 using Adam ( Kingma and Ba ,2015 ). We refer to this initialization method as unsupervised pre-training with masked salient spans . We provide further description in Appendix C. After MSS training, we ﬁnetune the model on the training examples with EMDR2. We perform training for 10 epochs on NQ and TriviaQA with a batch size of 64, and for 20 epochs on WebQ with a batch size of 16. During training, we save a checkpoint every 500 steps and select the best checkpoint based on its performance on the development set. During end-to-end training, since the parameters of the document encoder ( fd) are also updated at every step, the pre-computed document embeddings become stale as training progresses. We use the most recent document encoder checkpoint to compute fresh document embeddings asynchronously with which the document index is updated after every 500 training steps to prevent staleness. 6 Model top-K NQ TriviaQA WebQ # of dev test dev test dev test params Closed-Book QA Models T5-base (Roberts et al. ,2020 ) 0 - 25.7 - 24.2 - 28.2 220M T5-large (Roberts et al. ,2020 ) 0 - 27.3 - 28.5 - 29.5 770M T5-XXL (Roberts et al. ,2020 ) 0 - 32.8 - 42.9 - 35.6 11B GPT-3 ( Brown et al. ,2020 ) 0 - 29.9 - - - 41.5 175B Open-Book QA Models BM25 + BERT ( Leeet al. ,2019 ) 5 24.8 26.5 47.2 47.1 27.1 21.3 220M ORQA ( Leeet al. ,2019 ) 5 31.3 33.3 45.1 45.0 36.8 30.1 330M REALM ( Guu et al. ,2020 ) 5 38.2 40.4 - - - 40.7 330M DPR ( Karpukhin et al. ,2020 ) 25 - 41.5 - 56.8 - 34.6 330M RECONSIDER (Iyeret al. ,2021 )† 30 - 43.1 - 59.3 - 44.4 440M RAG-Sequence ( Lewis et al. ,2020b )† 50 44.0 44.5 55.8 56.8 44.9 45.2 626M Individual Top- K(Sachan et al. ,2021 ) - - 45.9 - 56.3 - - 440M Joint Top- K(Sachan et al. ,2021 ) 50 - 49.2 - 64.8 - - 440M FiD ( Izacard and Grave ,2021b ) 100 - 48.2 - 65.0 - - 440M FiD-KD ( Izacard and Grave ,2021a ) 100 48.0 49.6 68.6 68.8 - - 440M Our Implementation (Base Conﬁguration) FiD / T5- base 0 26.0 25.1 26.7 27.8 31.0 32.4 220M FiD (DPR retriever, T5 reader) 1 37.3 38.4 50.8 50.4 40.2 38.3 440M FiD (DPR retriever, T5 reader) 50 47.3 48.3 65.5 66.3 46.0 45.2 440M FiD (MSS + DPR retriever, T5 reader) 50 48.8 50.4 68.0 68.8 43.5 46.8 440M FiD (MSS retriever, MSS reader) 50 38.5 40.1 60.0 59.8 39.1 40.2 440M EMDR2(MSS retriever, MSS reader) 50 50.4 52.5 71.1 71.4 49.9 48.7 440M Table 2: Exact match scores on three evaluation datasets. Top- Kdenotes the number of retrieved documents that are used by the reader to produce an answer. To provide a fair comparison with our , we show results from other papers with the base conﬁguration, except for RAG-Sequence that uses BART- large (Lewis et al. ,2020a ).†indicates that their results on WebQ use NQ training data to pretrain the model. Inference. We use greedy decoding for answer generation at inference time. 3.3 Baselines We compare our model to other approaches for OpenQA that can be categorized under the following two classes: •Closed-book QA models. Large-scale language models capture a lot of world knowledge in their parameters derived from the corpus they have been trained on ( Petroni et al. ,2019 ). We compare with the work of Roberts et al. (2020 ) who show that larger T5 models—when ﬁnetuned with pairs—can perform remarkably well. We also compare with the few-shot results of GPT-3 ( Brown et al. ,2020 ).8 •Open-book QA models. Similar to this work, these models consist of retriever and reader components and adopt the retrieve then predict approach for answering questions given a collection of evidence documents. These models mainly differ in how the retriever is initialized (ORQA; Leeet al. ,2019 , DPR; Karpukhin et al. ,2020 ), whether the reader processes a single document (ORQA, DPR, RAG; Lewis et al. ,2020b ) or multiple documents (FiD; Izacard and Grave ,2021b ), or whether the reader and retriever are trained jointly or in a multistage process (REALM; Guu et al. ,2020 , FiD-KD; Izacard and Grave ,2021a ). 7 3.4 Results We follow standard conventions and report exact match (EM) scores using the reference answers included in each dataset. Table 2shows our main results. We divide the table into three main sections: closed-book QA models, open-book QA models, and our implementation. The ﬁrst two sections contain results from other papers, which we include for comparisons. The last section includes results from our proposed model, as well as our of relevant baselines to control for our experimental setup. Our of T5-base provides strong baselines when the number of retrieved documents is set to 0 (no retrieval) and 1. From Table 2, we see that the setting of top- 1vastly improves performance over the setting with no retrieved documents, signifying the importance of retrieval for OpenQA tasks. When further increasing the top- kdocuments to 50, the performance of the FiD models substantially improves over the top- 1retrieval, verifying the observation from ( Izacard and Grave ,2021b ) about the importance of modeling the retrieved documents as a set . Comparing EMDR2with our of FiD illustrates the beneﬁt of our end-to-end training approach. The underlying model is similar in both cases, but the training method is different. FiD adopts a two-stage approach to ﬁrst train the retriever and then the reader. We have three variants of FiD: (i) the reader and retriever are initialized with MSS training, (ii) the retriever is initialized with DPR training, which is the setting used in the original paper ( Izacard and Grave ,2021b ), and (iii) the retriever is initialized with MSS + DPR training from ( Sachan et al. ,2021 ), as it further improves DPR recall. E all the variants by large margins on all the datasets. The current best approach for training multi-document reader and retriever is FiD-KD ( Izacard and Grave ,2021a ). FiD-KD is a complex training procedure that requires multiple training stages and performs knowledge distillation with scores. We take the results from the original paper when comparing our model with FiD-KD. the reported numbers of FiD-KD by more than 2.5 points on NQ and TriviaQA to obtain new results on these benchmarks. In addition to better performance, EMDR2also has three other advantages compared to FiD-KD: (i)EMDR2is more efﬁcient since it only uses 50 evidence documents, whereas FiD-KD leverages 100 documents; (ii) FiD-KD is based on a distillation approach which requires multiple cycles of retriever and reader training, while EMDR2only requires one cycle of end-to-end training; and (iii) FiD-KD relies on supervised initialization of the retriever to achieve its best performance. EMDR2 is more robust to the retriever initialization, as demonstrated by results even with unsupervised initialization of the retriever. For the WebQ dataset, the training set size is much smaller compared to the other datasets (Table 5). Previous approaches such as RAG rely on supervised transfer (i.e., they ﬁnetune a model pre-trained on NQ) to obtain good results. In contrast, EMDR2improves over the results from this RAG model by 3.5 points without the supervised transfer step . This result demonstrates the applicability of our approach to the low-resource setting where we only have a limited number of training examples. We also perform qualitative analysis of the model outputs, which is included in Appendix E. 3.5 Ablations Number of retrieved documents. We investigate the performance of EMDR2and FiD as we vary the number of retrieved documents Kin Figure 2. We observe that when the number of retrieved documents is increased, both EMDR2and FiD improve in performance. When Kis small, the gap between EMDR2and FiD is larger. This indicates the efﬁcacy of EMDR2in a more constrained setting where we can only retrieve a small number of documents (e.g., due to memory limitations). Retriever initialization. We explore the effect of different parameter initialization strategies when training with EMDR2: (i) unsupervised MSS pre-training, (ii) supervised retriever training (DPR), and (iii) MSS pre-training followed by supervised retriever training (MSS + DPR; Sachan et al. (2021 )). Table 3shows our results. We can see that on NQ, MSS pre-training being unsupervised leads to a lower initial retriever recall than DPR. After EMDR2training, the recall improves by 20% (highlighted in yellow cells). Training with DPR initialization leads to the same ﬁnal recall as obtained by MSS 8We note that GPT-3 is not trained on the full training examples that we use, so the results are not directly comparable. 8 05102050number of top-k Match scoreNatural Questions (dev)EMDR2FiD 05102050number of top-k Match scoreTriviaQA (dev)EMDR2FiD 05102050number of top-k Match ( 2:Performance on NQ, TriviaQA, and WebQ as we vary the number of retrieved documents.NQ (dev) TriviaQA (dev) WebQ ( EM R@50 EM R@50 EMB.T. A.T. B.T. A.T. B.T. A.T.MSS pre-training MSS 50.4 74.8 86.2 71.1 59.8 88.649.9MSS pre-training T5 66.4 86.3 50.3 74.8 86.3 70.9 59.8 88.647.7DPR training T5 82.3 86.3 50.0 83.2 86.2 70.5 84.2 88.6 49.0MSS + DPR MSS pre-training 84.5 86.3 50.5 85.3 86.3 71.2 85.0 88.6 49.9Table 3:R@50 denotes the retrieval recall from the documents. B.T. and A.T.indicates R@50 score Before Training and After Training the model, that DPR initialization of the retriever may not be an essential componentto obtain good performance in OpenQA tasks.Similar trends are also observed on TriviaQA andWebQ. Similarly, MSS + DPR initialization has a better initial recall but leads to a marginal or noimprovements in answer extraction performance over MSS pre-training. Finally, we also observethat MSS pre-training also provides an improvement of 2 points in answer extraction on WebQ whencompared to the T5 reader (shown in orange cells), highlighting its importance in the tasks.3.6 Alternative End-to-End Training ObjectivesWe compare EMDR2objective (Eq.6) to two alternative formulations for end-to-end TriviaQA WebQFiD 50 47.3 65.5 71.1 49.9Lalt-150 14.1 11.9 28.0Lalt-250 49.9 69.6 28.8Table 4:EM scores on the developmentset for alternative training objectives.In the ﬁrst alternative formulation, when training the re-triever parameters , we simply arrive at the following = second term in this objective is maximised by a uni-form retrieval, in other words, byremovingany between documents in the retriever. We include itto show the impact of an adversarial objective.In the second formulation, for each retrieved document, we approximate its posterior under the as-sumption that we have a uniform prior over the set of retrieved . We use this to train reader and retriever parameters as follows:Lalt-2= ;⇥) )||p(zk|q,Z; ) ).Intuitively, we try to match the probability of retrieving a documentzkwith the “contribution” ofthat document to the generated answera, regardless of whether the retriever is relatively more or lesslikely to retreieve the documenta our results on the development set of NQ. We observe that training with the diverges, leading to poor performance, as expected. This shows that harming theretriever during training can signiﬁcantly harm performance of the QA system. In contrast, althoughit disregards the estimated prior, still improves over the FiD baseline for NQ and9 TriviaQA. However, it still lags behind EMDR2. On WebQ, the diverges and leads to a poor performance. We leave further analysis on the convergence of as a part of future work. 4 Related Work Our work is based on end-to-end training of neural readers and retrievers, which we discuss in § 1,§2, and § 3. Here we instead focus on discussing previous work related to standalone neural retrievers, neural readers, and their application in other natural language processing tasks. Neural retrievers. There are two broad classes of neural retrievers based on the number of embed- dings computed for a document: dual encoders ( Yihet al. ,2011 ,Leeet al. ,2019 ) and multivector encoders ( Khattab and Zaharia ,2020 ,Luan et al. ,2021 ). Dual encoders store one embedding for each evidence document. Multivector encoders require multiple embeddings, which can be computa- tionally expensive for large-scale retrieval. Due to the large size of the evidence document collection in OpenQA, our work uses the more efﬁcient dual-encoder. Sachan et al. (2021 ) show that the performance of supervised dual encoders in OpenQA can be improved when pre-training with the Inverse Cloze Task for the high-resource setting or masked salient spans for the low-resource setting. Neural readers. Neural readers output an answer given retrieved documents as its input. There are also two broad classes of neural readers: extractive and generative. Extractive readers ( Clark and Gardner ,2018 ,de Masson d’Autume et al. ,2019 ,Wang et al. ,2019 ,Guu et al. ,2020 ,Karpukhin et al. ,2020 ) extract a span from a retrieved document to produce an answer. Generative readers (Izacard and Grave ,2021b ), on the other hand, generates an answer conditioned on the retrieved documents. Other application areas. In addition to question answering, methods have been successfully applied to other natural language processing tasks. In left-to-right language modeling, retrieving similar words from an external memory has been shown to improve perplexity ( Khandelwal et al. ,2020 ,Yogatama et al. ,2021 ). In machine translation, retrieving domain-speciﬁc target language tokens has improved performance in domain adaptation ( Khandelwal et al. ,2021 ). Finally, in dialog modeling, retrieving text has helped improve factual correctness in the generated conversations ( Fanet al. ,2021 ). We provide a detailed comparison of EMDR2with some of the previous work in Appendix CandD. 5 Discussion Summary of contributions. We presented EMDR2, an end-to-end training method for retrieval- augmented question answering systems. We showed how to arrive at our training objective using the algorithm. We demonstrated that EMDR2achieves performance on three benchmark OpenQA datasets. Technical limitations. EMDR2shares a few limitations with other question answering models. In particular, as evidence documents are stored in an uncompressed format, maintaining them and searching for relevant documents can be expensive (both in terms of compute and memory consumption). In our experiments, we only focused on open-domain question answering. It would be interesting to see how EMDR2performs for other text generation models as well. We also note that training is relatively resource-heavy (requiring 16 GPUs), potentially having environmental concerns. Potential negative societal impacts. While EMDR2has the potential to improve language models in the low-resource setting (as demonstrated by our results on WebQ in § 3.4), it could exhibit typical biases that are associated with large language models. For example, our model does not have an explicit mechanism to generate answers that are calibrated for fairness across all spectra. As a method, it also could be more prone to generating fake answers if an attacker manages to have access and modify information in the collection of evidence documents. 10  The authors would like to thank the DeepMind Language team, Mila’s students, and anonymous reviewers for providing us valuable feedback and useful suggestions about this work that helped us improve the paper. Funding Statement DSS was supported by the Canada CIFAR AI Chair held by Prof. William Hamilton. References Berant, J., Chou, A., Frostig, R., and Liang, P. (2013). Semantic parsing on Freebase from question- answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing . Bromley, J., Guyon, I., LeCun, Y ., Säckinger, E., and Shah, R. (1994). Signature veriﬁcation using a "siamese" time delay neural network. In Advances in Neural Information Processing Systems . Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language models are few-shot learners. In Advances in Neural Information Processing Systems . Chen, D., Fisch, A., Weston, J., and Bordes, A. (2017). Reading Wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Clark, C. and Gardner, M. (2018). Simple and effective reading comprehension. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . de Masson d’Autume, C., Ruder, S., Kong, L., and Yogatama, D. (2019). Episodic memory in lifelong language learning. In Advances in Neural Information Processing Systems . Dempster, A., Laird, N., and Rubin, D. (1977). Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B ,1(39), 1–38. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirec- tional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Fan, A., Gardent, C., Braud, C., and Bordes, A. (2021). Augmenting Transformers with KNN-Based Composite Memory for Dialog. Transactions of the Association for Computational Linguistics ,9. Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. (2020). Retrieval augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning . Iyer, S., Min, S., Mehdad, Y., and Yih, W. (2021). Reconsider: Re-ranking using span-focused for open domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Izacard, G. and Grave, E. (2021a). Distilling knowledge from reader to retriever for question answering. In International Conference on Learning . Izacard, G. and Grave, E. (2021b). Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume . 11 Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. (2017). TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Karpukhin, V., O ˘guz, B., Min, S., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. (2020). Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. (2020). Generalization through memorization: Nearest neighbor language models. In International Conference on Learning . Khandelwal, U., Fan, A., Jurafsky, D., Zettlemoyer, L., and Lewis, M. (2021). Nearest neighbor machine translation. In International Conference on Learning . Khattab, O. and Zaharia, M. (2020). Colbert: Efﬁcient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In The 2015 International Conference for Learning . Kwiatkowski, T., Palomaki, J., Redﬁeld, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin, J., Lee, K., Toutanova, K. N., Jones, L., Chang, M.-W., Dai, A., Uszkoreit, J., Le, Q., and Petrov, S. (2019). Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics . Lee, K., Chang, M.-W., and Toutanova, K. (2019). Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. (2020a). BART: Denoising pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., Riedel, S., and Kiela, D. (2020b). generation for nlp tasks. In Advances in Neural Information Processing Systems . Luan, Y., Eisenstein, J., Toutanova, K., and Collins, M. (2021). Sparse, Dense, and Attentional for Text Retrieval. Transactions of the Association for Computational Linguistics , 9. Min, S., Chen, D., Hajishirzi, H., and Zettlemoyer, L. (2019). A discrete hard EM approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , Hong Kong, China. Association for Computational Linguistics. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). Pytorch: An imperative style, high- performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems . Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. (2019). Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Qi, P., Zhang, Y., Zhang, Y., Bolton, J., and Manning, C. D. (2020). Stanza: A Python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations . 12 Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research ,21(140), 1–67. Roberts, A., Raffel, C., and Shazeer, N. (2020). How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Robertson, S. and Zaragoza, H. (2009). The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval . Sachan, D. S., Patwary, M., Shoeybi, M., Kant, N., Ping, W., Hamilton, W. L., and Catanzaro, B. (2021). End-to-end training of neural retrievers for open-domain question answering. In Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP) . Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019). Megatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint . Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems . Wang, S., Yu, M., Guo, X., Wang, Z., Klinger, T., Zhang, W., Chang, S., Tesauro, G., Zhou, B., and Jiang, J. (2018). R3: Reinforced ranker-reader for open-domain question answering. In AAAI . Wang, Z., Ng, P., Ma, X., Nallapati, R., and Xiang, B. (2019). Multi-passage BERT: A globally normalized BERT model for open-domain question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Yih, W.-t., Toutanova, K., Platt, J. C., and Meek, C. (2011). Learning discriminative projections for text similarity measures. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning . Yogatama, D., de Masson d’Autume, C., and Kong, L. (2021). Adaptive Semiparametric Language Models. Transactions of the Association for Computational Linguistics ,9, 362–373. 13 Checklist 1.For all authors... (a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] Please see the model (§ 2) and result (§ 3) sections that solidify the claims made in the abstract and introduction sections. (b)Did you describe the limitations of your work? [Yes] Please see limitations in § 5. (c)Did you discuss any potential negative societal impacts of your work? [Yes] Please see negative societal impact in § 5. (d)Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2.If you are including theoretical results... (a)Did you state the full set of assumptions of all theoretical results? [N/A] (b)Did you include complete proofs of all theoretical results? [N/A] 3.If you ran experiments... (a)Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] We include the code, data, and instructions in the supplemental material and § 3.2. (b)Did you specify all the training details (e.g., data splits, , how they were chosen)? [Yes] We specify these details in the appendix included in the supplementary material. (c)Did you report error bars (e.g., with respect to the random seed after running exper- iments multiple times)? [No] Our experiments are compute expensive and it is not feasible to perform multiple runs of the same experiment with different seeds. All our training runs use the same seed value (1234). As an alternative to running multiple seeds, we perform a number of ablation studies (§ 3.5). (d)Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Please see § 3.2under hardware and library. 4.If you are using existing assets (e.g., code, data, models) or new assets... (a)If your work uses existing assets, did you cite the creators? [Yes] Please see § 3.1for the details. (b)Did you mention the license of the assets? [Yes] Our work is based on open-source data and framework. When applicable, we describe the license information in the appendix. (c)Did you include any new assets either in the supplemental material or as a URL? [Yes] We include our code in the supplementary material. (d)Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e)Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5.If you used crowdsourcing or conducted research with human subjects... (a)Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b)Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c)Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14 
end to end training of multi document reader and retriever for open domain question answering 	End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering Devendra Singh Sachan1,2, Siva Reddy1,2, William Hamilton1,2, Chris Dyer3, Dani Yogatama3 1Mila - Quebec AI Institute 2School of Computer Science, McGill University 3DeepMind , {siva, {cdyer, Abstract We present an end-to-end differentiable training method for open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is hard, we approximate this using an algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to ﬂow to the reader and then to the retriever better than stage-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3 absolute exact match points, achieving new state-of-the- art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions. 1 Introduction Open-domain question answering (OpenQA) is a question answering task where the goal is to train a language model to produce an answer for a given question. In contrast to many question answering tasks, an OpenQA model is only provided with the question as its input without accompanying documents that contain the answer. One of the most promising approaches to OpenQA is based on augmenting the language model with an external knowledge source such as Wikipedia (often referred to as the evidence documents). In this approach, the model consists of two core components ( Chen et al. ,2017 ): (i) an information retrieval system to identify useful pieces of text from the knowledge source (the retriever); and (ii) a system to produce the answer given the retrieved documents and the question (the reader). We can view such a model as a latent variable model, where the latent variables represent retrieved documents that are used to produce answers given questions ( Leeet al. ,2019 ). End-to-end (joint) training of this model is challenging since we need to learn both to generate an answer given retrieved documents and what to retrieve. Previous work considers two potential solutions (see Table 1for a high-level summary). First, they adopt a stage-wise training, where the retriever is trained while freezing the reader and vice versa ( Karpukhin et al. ,2020 ,Izacard and Grave ,2021b ,a). Another 35th Conference on Neural Information Processing Systems (NeurIPS 2021). Reader and Retriever Training Model Multi-Doc End-to-End Multi-Step Unsupervised Retriever REALM ( Guu et al. ,2020 ) 33 3 DPR ( Karpukhin et al. ,2020 ) 3 RAG ( Lewis et al. ,2020b ) 33 FiD ( Izacard and Grave ,2021b ) 33 FiD-KD ( Izacard and Grave ,2021a ) 33 3 EMDR2(Our Approach) 33 3 3 Table 1: Bird’s-eye view of the recent OpenQA approaches. Multi-Doc reader indicates whether the reader architecture uses multiple documents or a single document. Retriever adaptation shows whether the retriever gets feedback from the reader to update its parameters. Disjoint denotes that ﬁrst the retriever is trained and then the reader is trained. End-to-end denotes that the reader and retriever are trained jointly in one cycle. Multi-step indicates that the reader and retriever are trained iteratively in multiple cycles. Unsupervised retriever indicates whether the retriever is initialized using unsupervised approaches or using supervised data. alternative is to constraint the reader to condition on each retrieved document et al. ,2020 )—sometimes with extra supervision for the latent variables in the form of the relevant document for a question ( Lewis et al. ,2020b ). In this paper, we consider a question answering model that combines information from multiple documents when generating answers. ( Dempster et al. , 1977 ) offers a principled template for learning this class of latent variable models. We present training of Multi-Document Reader and Retriever (§ uses feedback from the model itself as “pseudo labels” of the latent variables for optimizing the retriever and reader parameters. We use two estimates of the latent variables: (i) prior scores for updating the reader parameters and (ii) approximate posterior scores given all observed variables for the retriever parameters. We evaluate our proposed method by experimenting on three commonly used OpenQA datasets: Natural Questions, TriviaQA, and WebQuestions (§ 3). E MDR2achieves new results for models of comparable size on all datasets, outperforming recent approaches by 2-3 absolute exact match points. We also show that EMDR2is robust to retriever initialization. It achieves high accuracy with unsupervised initialization, suggesting that supervised training of the retriever may not be an essential component of the training process as suggested in prior work ( Karpukhin et al. ,2020 ). In summary, our contributions are as follows: (i) we present an end-to-end training method ( EMDR2) for systems; (ii) we demonstrate that other existing approaches of comparable size without any kind of supervision on the latent variables; (iii) we provide ablation studies for a better understanding of the contributions of different components of our proposed method; and (iv) we release our code and checkpoints to facilitate future work and for EMDR2is a framework that can be used to train text generation models for any task. We believe that our estimation technique in EMDR2is also useful for learning similar latent variable models in other domains. 2 Model Our proposed model EMDR2consists of two components: (i) a neural retriever and (ii) a neural reader, which we train jointly in an end-to-end setting. Figure 1shows an illustration of our model and training procedure. We discuss each component and our training objective in detail below. 1This makes over the latent variables easier since we only need to consider one document at a time rather than multiple documents at once. 2Our code is available at: 2 2.1 Neural Retriever: Dual Encoder Let the collection of evidence documents be denoted by D={d1,..., dM}. Given a question q, the goal of the retriever module is to select a subset of documents Z⇢D to answer the question. We model the retriever as a dual-encoder network ( Bromley et al. ,1994 ), where one encoder fq encodes the question and another fdencodes the evidence document (to a vector). The retrieval score is deﬁned as the dot product between the two resulting vectors: score (q,di; )= ), (1) where  =[   q, d]denotes the retriever parameters. We select top- Kdocuments for the question q from Dbased on the retrieval scores. We denote the set of retrieved documents by Z={z1,..., zK}. We use transformer encoders ( Vaswani et al. ,2017 ) as our fqandfd. Our transformer architecture is similar to BERT with 12 layers and 768 hidden size ( Devlin et al. ,2019 ). We use the ﬁnal representation of the ﬁrst token (i.e., the standard [CLS] token from BERT’s tokenization) as our question (and similarly document) embedding. Initializing fqandfdwith BERT weights has been shown to lead to a poor retrieval accuracy ( Leeet al. ,2019 ,Sachan et al. ,2021 ). Therefore, we initialize the retriever with an unsupervised training procedure. We discuss our initialization technique in detail in § 3.2. 2.2 Neural Reader: The reader takes as input a question qand a set of retrieved documents (to be read) Zto generate an answer. Our reader is based on the (FiD; Izacard and Grave ,2021b ) model, which is built on top of T5 ( Raffel et al. ,2020 ). T5 is a pretrained transformer that consists of an encoder geand a decoder gd. In FiD, each retrieved document zkis ﬁrst appended with its title ( tzk) and the question: xk=[CLS] q[SEP] tzk[SEP] zk[SEP] , where [CLS] is used to indicate the start of a document and [SEP] is used as a separator for the different parts of the document as well as the ﬁnal token. Each xkis then independently given as an input to the T5 encoder ge. The output corresponding to all of the retrieved documents are concatenated as: , where Nis the number of tokens in each xk3andHis the hidden size of the T5 encoder ge. In this work, we use the T5- base conﬁguration with N= 512 andH= 768 . XZis then given as an input to the T5 decoder gd. When generating an answer token, the decoder attends to both previously generated tokens (i.e., causal attention) as well as the tokens encoded in XZ(i.e., cross attention). Since XZcontains information from multiple documents, the decoder has the ability to aggregate useful signals contained in multiple documents and jointly reason over them. We deﬁne the probability of the answer as: p(a|q,Z;⇥)=TY ;⇥), (2) where ⇥denotes the reader parameters (i.e., T5 encoder and decoder) and Tis the number of answer tokens. We keep generating answer tokens until the decoder outputs a special EOStoken or a pre-speciﬁed maximum answer length is reached. 2.3 End-to-End Training of Reader and Retriever In contrast to previous work on generative question answering, we train both the reader and the retriever jointly in an end-to-end differentiable fashion. Denote our latent variable which represents a set of retrieved documents by Zand let Zbe a possible value of Z. The marginal likelihood of an answer (marginalizing over all the possible values of Z) 3We truncate and pad as necessary such that every xkhas the same length N. See § 3.2for details. 3 T5 EncodeUT5 DecodeUT5 DiVWUibXWed EYidenceDoc Inde[ oYeU GPUVWop-k MIPS¬ Top-k PUodXcWReadeU TUaining TUaining SWale EYidenceDoc Inde[ RefUeVhFigure 1: An illustration of the different components of EMDR2. Colored blocks indicate components which contain trainable parameters. is:p(a|q;⇥, ) =P ; ). The goal of our training procedure is to ﬁnd and⇥that would maximize the above objective. Exactly optimizing Eq. 3is intractable as it is combinatorial in nature.4For one particular value Z, the log-likelihood is simpler to compute: ;  ) = log p(a|q,Z; ⇥) + log p(Z|q; ). (EM) algorithm ( Dempster et al. ,1977 ) offers a solution to learning this latent variable model. In classical EM, we iteratively compute the posterior of Zgiven all observed variables and use it to update ⇥and . We propose using two estimates of Z—Zreader andZretriever —for updating the two components of the model (reader parameters ⇥and retriever parameters  ): ;⇥)| {z } reader+ log p(Zretriever |q; )|{z } retriever. (3) In the ﬁrst term, we set the value of the latent variable Z=Zreader based on the prior scores. In the second term, we seek to maximize an approximate posterior of Z=Zretriever . We discuss them in more detail below. Reader parameters ⇥.For updating ⇥(the ﬁrst term of Eq. 3), we use the top- Kdocuments with the highest individual scores (as computed by Eq. 1based on the current value of  ) to construct Zreader. This is equivalent to relying on the prior p(Z|q; ) to estimate Zreader (without using information from the answer a). We choose to use the prior to train reader parameters since the prior scores are also used at evaluation time to obtain the top- Kdocuments. As a result, there is no mismatch between training and test computations when computing p(a|q,Z;⇥) (i.e., Zthat is used at test time is obtained in exactly the same way as Zreader =Ztop-K). Retriever parameters  .For updating  (the second term of Eq. 3), we propose to use the posterior estimate. In other words, we use additional information from awhen evaluating Zretriever to train  . Using the posterior allows our retriever to learn from richer training signals as opposed to relying only on the prior. We need to be able to compute p(Zretriever |q,a;⇥, )to maximize the retriever parameters. However, computing this quantity is difﬁcult since it is a probability of a set.5Consider a set of Kdocuments (e.g., Ztop-K), where zkdenotes a document in the set. We approximate the maximization of the probability of the set by assuming that its probability is maximized if the sum of the probability of 4Contrast our objective with REALM ( Guu et al. ,2020 ), where the reader only conditions on one retrieved document zkwhen generating an answer. In this case, the latent variable represents a document assignment instead of a set of retrieved documents. 5This is true whether we choose to use the posterior probability or the prior probability. 4 each document in the set is this approximation, we arrive at a simpler quantity:PK k=1p(zk|q,a;⇥, ). Note that using Bayes rule, we can rewrite:7 ; ). (4) The reader now only conditions on one document when computing the probability of an answer p(a|q,zk;⇥). This simpler reader uses the same parameters as the more sophisticated one ⇥, but it only uses one document zkinstead of a set of documents. To compute Eq. 4, we ﬁrst obtain Kdocuments with the highest scores as computed by Eq. 1based on the current value of  . We compute the probability of document zk2Z top-Kas: p(zk|q,Ztop-K; ) ⇡exp( score (q,zk)/⌧; )PK j=1exp( score (q,zj)/⌧; ), (5) where ⌧is a temperature hyperparameter and the approximation assumes that documents beyond the very small scores so we do not need to sum over all evidence documents Min the denominator (which is in the order of tens of millions in our experiments). We then compute p(a|q,zk;⇥) similarly to Eq. 2. Overall training objective of E MDR2.Combining the above derivations, our end-to-end training objective that we seek to maximize for a particular example becomes: L= log p(a|q,Ztop-K;⇥)| {z } reader+ logKX k=1SG(p(a|q,zk;⇥) ) p(zk|q,Ztop-K; ) | {z } retriever, (6) where SGis the stop-gradient operator so that the reader parameters ⇥are not updated to also perform well given a single document zk. The stop-gradient operator in the second term of EMDR2has several beneﬁts. First, the FiD reader is trained from the ﬁrst term of the EMDR2objective in which its likelihood is conditioned on all the retrieved documents, similar to how the reader is used at test time. Second, it also makes training faster since the backward pass which is more expensive than the forward pass is not needed, which in turn reduces the usage of GPU RAM as intermediate activations need not be saved. Given a training example, we update ⇥and by taking gradients of Eq. 6with respect to ⇥and in an end-to-end fashion. Intuitively, we train the reader to generate the correct answer given Khighest scoring documents Ztop-K. For the retriever, we train it to select Kdocuments which collectively has a high score of generating an answer (since the sum over Kis inside the log in the second term) while taking into account feedback from the reader. Algorithm 1summarizes our training algorithm. Algorithm 1: End-to-end training of multi-document reader and retriever. Input: Model parameters ⇥and , evidence documents D. while not converged do • Compute Ztop-Kusing the current retriever parameters  . //E-step • Compute p(a|q,zk)for each zkusing the current reader parameters ⇥.//E-step •Update model parameters ⇥and to maximize the log-likelihood in Eq. 6.//M-step end 3 Experiments 3.1 Datasets We experiment with three commonly used open-domain question answering datasets: 6The intuition is that each element of the set contributes independently, which greatly simpliﬁes the computa- tion to ﬁnd the maximum of the set. 7We choose not to normalize with p(a|q;⇥, ) since computing this quantity would require summing over all evidence documents M. While this makes the resulting objective that we optimize not correspond to a proper probability distribution anymore, we observe that our training method still behaves well in practice. 5 •Natural Questions (NQ; Kwiatkowski et al. ,2019).NQ contains questions asked by users of the Google search engine. Similar to Leeet al. (2019 ), we use the short answer subset. •TriviaQA ( Joshi et al. ,2017).TriviaQA is a collection of trivia pairs that were collected from multiple sources on the web. •WebQuestions (WebQ; Berant et al. ,2013).WebQ questions were collected using Google Suggest API and the answers were annotated using Mechanical Turk. We use the version from Chen et al. (2017 ) where Freebase IDs in the answers are replaced by entity names. Evidence documents D.We use the preprocessed English Wikipedia dump from December 2018 released by Karpukhin et al. (2020 ) as our evidence documents. Each Wikipedia article is split into 100 words long segments. Each segment corresponds to a document in our case. There are a total of 21,015,324 documents in total. We provide descriptive statistics and other preprocessing details in Appendix A. 3.2 Implementation Details Hardware and library. We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs. We use PyTorch ( Paszke et al. ,2019 ) to implement our proposed model and relevant baselines. Model conﬁgurations. For both the retriever and reader, we use the base conﬁguration that consists of 12 layers, 768 dimensional hidden size, and 12 attention heads. In all experiments, we retrieve 50 documents, unless stated otherwise. We only use the base conﬁguration in our experiments due to GPU memory constraints. However, we believe that our results would generalize to larger conﬁgurations as well. Retrieval. To support fast retrieval, we pre-compute evidence document embeddings and store them in a distributed fashion over all the GPUs. We refer to these document embeddings as the document index. For each question, we retrieve documents in an online (on-the-ﬂy) manner by performing exact maximum inner product search (MIPS), implemented using asynchronous distributed matrix multiplication over the document index. These documents are converted to subwords using BERT’s tokenization and are given as input to the T5 reader. If a tokenized document is shorter than 512 tokens, it is padded using the tokens from the neighboring documents until the maximum token limit is reached. Such padding additionally helps to provide an extended context for answer generation. Initialization and training details. We initialize the parameters of the model with unsupervised pre-training before performing supervised training using the training examples. Unsupervised pre-training is essential as it helps to warm-start the retriever so that it outputs relevant documents for a given question. We ﬁrst pre-train the retriever parameters with unsupervised Inverse Cloze Task training ( Leeet al. , 2019 ) for 100,000 steps. We then extract sentences containing named entities from the evidence documents. Next, we replace 15% of the named entity tokens with masked tokens, which are often referred to as masked salient spans (MSS; Guu et al. ,2020 ). The masked sentence can be considered as the question and its salient spans (i.e, named entities) can be considered as the answer to train the model with Eq. 6. We train the model on these (masked sentence-named entities) pairs for 82,000 steps with a batch size of 64 using Adam ( Kingma and Ba ,2015 ). We refer to this initialization method as unsupervised pre-training with masked salient spans . We provide further description in Appendix C. After MSS training, we ﬁnetune the model on the training examples with EMDR2. We perform training for 10 epochs on NQ and TriviaQA with a batch size of 64, and for 20 epochs on WebQ with a batch size of 16. During training, we save a checkpoint every 500 steps and select the best checkpoint based on its performance on the development set. During end-to-end training, since the parameters of the document encoder ( fd) are also updated at every step, the pre-computed document embeddings become stale as training progresses. We use the most recent document encoder checkpoint to compute fresh document embeddings asynchronously with which the document index is updated after every 500 training steps to prevent staleness. 6 Model top-K NQ TriviaQA WebQ # of dev test dev test dev test params Closed-Book QA Models T5-base (Roberts et al. ,2020 ) 0 - 25.7 - 24.2 - 28.2 220M T5-large (Roberts et al. ,2020 ) 0 - 27.3 - 28.5 - 29.5 770M T5-XXL (Roberts et al. ,2020 ) 0 - 32.8 - 42.9 - 35.6 11B GPT-3 ( Brown et al. ,2020 ) 0 - 29.9 - - - 41.5 175B Open-Book QA Models BM25 + BERT ( Leeet al. ,2019 ) 5 24.8 26.5 47.2 47.1 27.1 21.3 220M ORQA ( Leeet al. ,2019 ) 5 31.3 33.3 45.1 45.0 36.8 30.1 330M REALM ( Guu et al. ,2020 ) 5 38.2 40.4 - - - 40.7 330M DPR ( Karpukhin et al. ,2020 ) 25 - 41.5 - 56.8 - 34.6 330M RECONSIDER (Iyeret al. ,2021 )† 30 - 43.1 - 59.3 - 44.4 440M RAG-Sequence ( Lewis et al. ,2020b )† 50 44.0 44.5 55.8 56.8 44.9 45.2 626M Individual Top- K(Sachan et al. ,2021 ) - - 45.9 - 56.3 - - 440M Joint Top- K(Sachan et al. ,2021 ) 50 - 49.2 - 64.8 - - 440M FiD ( Izacard and Grave ,2021b ) 100 - 48.2 - 65.0 - - 440M FiD-KD ( Izacard and Grave ,2021a ) 100 48.0 49.6 68.6 68.8 - - 440M Our Implementation (Base Conﬁguration) FiD / T5- base 0 26.0 25.1 26.7 27.8 31.0 32.4 220M FiD (DPR retriever, T5 reader) 1 37.3 38.4 50.8 50.4 40.2 38.3 440M FiD (DPR retriever, T5 reader) 50 47.3 48.3 65.5 66.3 46.0 45.2 440M FiD (MSS + DPR retriever, T5 reader) 50 48.8 50.4 68.0 68.8 43.5 46.8 440M FiD (MSS retriever, MSS reader) 50 38.5 40.1 60.0 59.8 39.1 40.2 440M EMDR2(MSS retriever, MSS reader) 50 50.4 52.5 71.1 71.4 49.9 48.7 440M Table 2: Exact match scores on three evaluation datasets. Top- Kdenotes the number of retrieved documents that are used by the reader to produce an answer. To provide a fair comparison with our , we show results from other papers with the base conﬁguration, except for RAG-Sequence that uses BART- large (Lewis et al. ,2020a ).†indicates that their results on WebQ use NQ training data to pretrain the model. Inference. We use greedy decoding for answer generation at inference time. 3.3 Baselines We compare our model to other approaches for OpenQA that can be categorized under the following two classes: •Closed-book QA models. Large-scale language models capture a lot of world knowledge in their parameters derived from the corpus they have been trained on ( Petroni et al. ,2019 ). We compare with the work of Roberts et al. (2020 ) who show that larger T5 models—when ﬁnetuned with pairs—can perform remarkably well. We also compare with the few-shot results of GPT-3 ( Brown et al. ,2020 ).8 •Open-book QA models. Similar to this work, these models consist of retriever and reader components and adopt the retrieve then predict approach for answering questions given a collection of evidence documents. These models mainly differ in how the retriever is initialized (ORQA; Leeet al. ,2019 , DPR; Karpukhin et al. ,2020 ), whether the reader processes a single document (ORQA, DPR, RAG; Lewis et al. ,2020b ) or multiple documents (FiD; Izacard and Grave ,2021b ), or whether the reader and retriever are trained jointly or in a multistage process (REALM; Guu et al. ,2020 , FiD-KD; Izacard and Grave ,2021a ). 7 3.4 Results We follow standard conventions and report exact match (EM) scores using the reference answers included in each dataset. Table 2shows our main results. We divide the table into three main sections: closed-book QA models, open-book QA models, and our implementation. The ﬁrst two sections contain results from other papers, which we include for comparisons. The last section includes results from our proposed model, as well as our of relevant baselines to control for our experimental setup. Our of T5-base provides strong baselines when the number of retrieved documents is set to 0 (no retrieval) and 1. From Table 2, we see that the setting of top- 1vastly improves performance over the setting with no retrieved documents, signifying the importance of retrieval for OpenQA tasks. When further increasing the top- kdocuments to 50, the performance of the FiD models substantially improves over the top- 1retrieval, verifying the observation from ( Izacard and Grave ,2021b ) about the importance of modeling the retrieved documents as a set . Comparing EMDR2with our of FiD illustrates the beneﬁt of our end-to-end training approach. The underlying model is similar in both cases, but the training method is different. FiD adopts a two-stage approach to ﬁrst train the retriever and then the reader. We have three variants of FiD: (i) the reader and retriever are initialized with MSS training, (ii) the retriever is initialized with DPR training, which is the setting used in the original paper ( Izacard and Grave ,2021b ), and (iii) the retriever is initialized with MSS + DPR training from ( Sachan et al. ,2021 ), as it further improves DPR recall. E all the variants by large margins on all the datasets. The current best approach for training multi-document reader and retriever is FiD-KD ( Izacard and Grave ,2021a ). FiD-KD is a complex training procedure that requires multiple training stages and performs knowledge distillation with scores. We take the results from the original paper when comparing our model with FiD-KD. the reported numbers of FiD-KD by more than 2.5 points on NQ and TriviaQA to obtain new results on these benchmarks. In addition to better performance, EMDR2also has three other advantages compared to FiD-KD: (i)EMDR2is more efﬁcient since it only uses 50 evidence documents, whereas FiD-KD leverages 100 documents; (ii) FiD-KD is based on a distillation approach which requires multiple cycles of retriever and reader training, while EMDR2only requires one cycle of end-to-end training; and (iii) FiD-KD relies on supervised initialization of the retriever to achieve its best performance. EMDR2 is more robust to the retriever initialization, as demonstrated by results even with unsupervised initialization of the retriever. For the WebQ dataset, the training set size is much smaller compared to the other datasets (Table 5). Previous approaches such as RAG rely on supervised transfer (i.e., they ﬁnetune a model pre-trained on NQ) to obtain good results. In contrast, EMDR2improves over the results from this RAG model by 3.5 points without the supervised transfer step . This result demonstrates the applicability of our approach to the low-resource setting where we only have a limited number of training examples. We also perform qualitative analysis of the model outputs, which is included in Appendix E. 3.5 Ablations Number of retrieved documents. We investigate the performance of EMDR2and FiD as we vary the number of retrieved documents Kin Figure 2. We observe that when the number of retrieved documents is increased, both EMDR2and FiD improve in performance. When Kis small, the gap between EMDR2and FiD is larger. This indicates the efﬁcacy of EMDR2in a more constrained setting where we can only retrieve a small number of documents (e.g., due to memory limitations). Retriever initialization. We explore the effect of different parameter initialization strategies when training with EMDR2: (i) unsupervised MSS pre-training, (ii) supervised retriever training (DPR), and (iii) MSS pre-training followed by supervised retriever training (MSS + DPR; Sachan et al. (2021 )). Table 3shows our results. We can see that on NQ, MSS pre-training being unsupervised leads to a lower initial retriever recall than DPR. After EMDR2training, the recall improves by 20% (highlighted in yellow cells). Training with DPR initialization leads to the same ﬁnal recall as obtained by MSS 8We note that GPT-3 is not trained on the full training examples that we use, so the results are not directly comparable. 8 05102050number of top-k Match scoreNatural Questions (dev)EMDR2FiD 05102050number of top-k Match scoreTriviaQA (dev)EMDR2FiD 05102050number of top-k Match ( 2:Performance on NQ, TriviaQA, and WebQ as we vary the number of retrieved documents.NQ (dev) TriviaQA (dev) WebQ ( EM R@50 EM R@50 EMB.T. A.T. B.T. A.T. B.T. A.T.MSS pre-training MSS 50.4 74.8 86.2 71.1 59.8 88.649.9MSS pre-training T5 66.4 86.3 50.3 74.8 86.3 70.9 59.8 88.647.7DPR training T5 82.3 86.3 50.0 83.2 86.2 70.5 84.2 88.6 49.0MSS + DPR MSS pre-training 84.5 86.3 50.5 85.3 86.3 71.2 85.0 88.6 49.9Table 3:R@50 denotes the retrieval recall from the documents. B.T. and A.T.indicates R@50 score Before Training and After Training the model, that DPR initialization of the retriever may not be an essential componentto obtain good performance in OpenQA tasks.Similar trends are also observed on TriviaQA andWebQ. Similarly, MSS + DPR initialization has a better initial recall but leads to a marginal or noimprovements in answer extraction performance over MSS pre-training. Finally, we also observethat MSS pre-training also provides an improvement of 2 points in answer extraction on WebQ whencompared to the T5 reader (shown in orange cells), highlighting its importance in the tasks.3.6 Alternative End-to-End Training ObjectivesWe compare EMDR2objective (Eq.6) to two alternative formulations for end-to-end TriviaQA WebQFiD 50 47.3 65.5 71.1 49.9Lalt-150 14.1 11.9 28.0Lalt-250 49.9 69.6 28.8Table 4:EM scores on the developmentset for alternative training objectives.In the ﬁrst alternative formulation, when training the re-triever parameters , we simply arrive at the following = second term in this objective is maximised by a uni-form retrieval, in other words, byremovingany between documents in the retriever. We include itto show the impact of an adversarial objective.In the second formulation, for each retrieved document, we approximate its posterior under the as-sumption that we have a uniform prior over the set of retrieved . We use this to train reader and retriever parameters as follows:Lalt-2= ;⇥) )||p(zk|q,Z; ) ).Intuitively, we try to match the probability of retrieving a documentzkwith the “contribution” ofthat document to the generated answera, regardless of whether the retriever is relatively more or lesslikely to retreieve the documenta our results on the development set of NQ. We observe that training with the diverges, leading to poor performance, as expected. This shows that harming theretriever during training can signiﬁcantly harm performance of the QA system. In contrast, althoughit disregards the estimated prior, still improves over the FiD baseline for NQ and9 TriviaQA. However, it still lags behind EMDR2. On WebQ, the diverges and leads to a poor performance. We leave further analysis on the convergence of as a part of future work. 4 Related Work Our work is based on end-to-end training of neural readers and retrievers, which we discuss in § 1,§2, and § 3. Here we instead focus on discussing previous work related to standalone neural retrievers, neural readers, and their application in other natural language processing tasks. Neural retrievers. There are two broad classes of neural retrievers based on the number of embed- dings computed for a document: dual encoders ( Yihet al. ,2011 ,Leeet al. ,2019 ) and multivector encoders ( Khattab and Zaharia ,2020 ,Luan et al. ,2021 ). Dual encoders store one embedding for each evidence document. Multivector encoders require multiple embeddings, which can be computa- tionally expensive for large-scale retrieval. Due to the large size of the evidence document collection in OpenQA, our work uses the more efﬁcient dual-encoder. Sachan et al. (2021 ) show that the performance of supervised dual encoders in OpenQA can be improved when pre-training with the Inverse Cloze Task for the high-resource setting or masked salient spans for the low-resource setting. Neural readers. Neural readers output an answer given retrieved documents as its input. There are also two broad classes of neural readers: extractive and generative. Extractive readers ( Clark and Gardner ,2018 ,de Masson d’Autume et al. ,2019 ,Wang et al. ,2019 ,Guu et al. ,2020 ,Karpukhin et al. ,2020 ) extract a span from a retrieved document to produce an answer. Generative readers (Izacard and Grave ,2021b ), on the other hand, generates an answer conditioned on the retrieved documents. Other application areas. In addition to question answering, methods have been successfully applied to other natural language processing tasks. In left-to-right language modeling, retrieving similar words from an external memory has been shown to improve perplexity ( Khandelwal et al. ,2020 ,Yogatama et al. ,2021 ). In machine translation, retrieving domain-speciﬁc target language tokens has improved performance in domain adaptation ( Khandelwal et al. ,2021 ). Finally, in dialog modeling, retrieving text has helped improve factual correctness in the generated conversations ( Fanet al. ,2021 ). We provide a detailed comparison of EMDR2with some of the previous work in Appendix CandD. 5 Discussion Summary of contributions. We presented EMDR2, an end-to-end training method for retrieval- augmented question answering systems. We showed how to arrive at our training objective using the algorithm. We demonstrated that EMDR2achieves performance on three benchmark OpenQA datasets. Technical limitations. EMDR2shares a few limitations with other question answering models. In particular, as evidence documents are stored in an uncompressed format, maintaining them and searching for relevant documents can be expensive (both in terms of compute and memory consumption). In our experiments, we only focused on open-domain question answering. It would be interesting to see how EMDR2performs for other text generation models as well. We also note that training is relatively resource-heavy (requiring 16 GPUs), potentially having environmental concerns. Potential negative societal impacts. While EMDR2has the potential to improve language models in the low-resource setting (as demonstrated by our results on WebQ in § 3.4), it could exhibit typical biases that are associated with large language models. For example, our model does not have an explicit mechanism to generate answers that are calibrated for fairness across all spectra. As a method, it also could be more prone to generating fake answers if an attacker manages to have access and modify information in the collection of evidence documents. 10  The authors would like to thank the DeepMind Language team, Mila’s students, and anonymous reviewers for providing us valuable feedback and useful suggestions about this work that helped us improve the paper. Funding Statement DSS was supported by the Canada CIFAR AI Chair held by Prof. William Hamilton. References Berant, J., Chou, A., Frostig, R., and Liang, P. (2013). Semantic parsing on Freebase from question- answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing . Bromley, J., Guyon, I., LeCun, Y ., Säckinger, E., and Shah, R. (1994). Signature veriﬁcation using a "siamese" time delay neural network. In Advances in Neural Information Processing Systems . Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language models are few-shot learners. In Advances in Neural Information Processing Systems . Chen, D., Fisch, A., Weston, J., and Bordes, A. (2017). Reading Wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Clark, C. and Gardner, M. (2018). Simple and effective reading comprehension. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . de Masson d’Autume, C., Ruder, S., Kong, L., and Yogatama, D. (2019). Episodic memory in lifelong language learning. In Advances in Neural Information Processing Systems . Dempster, A., Laird, N., and Rubin, D. (1977). Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B ,1(39), 1–38. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirec- tional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Fan, A., Gardent, C., Braud, C., and Bordes, A. (2021). Augmenting Transformers with KNN-Based Composite Memory for Dialog. Transactions of the Association for Computational Linguistics ,9. Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. (2020). Retrieval augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning . Iyer, S., Min, S., Mehdad, Y., and Yih, W. (2021). Reconsider: Re-ranking using span-focused for open domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Izacard, G. and Grave, E. (2021a). Distilling knowledge from reader to retriever for question answering. In International Conference on Learning . Izacard, G. and Grave, E. (2021b). Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume . 11 Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. (2017). TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Karpukhin, V., O ˘guz, B., Min, S., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. (2020). Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. (2020). Generalization through memorization: Nearest neighbor language models. In International Conference on Learning . Khandelwal, U., Fan, A., Jurafsky, D., Zettlemoyer, L., and Lewis, M. (2021). Nearest neighbor machine translation. In International Conference on Learning . Khattab, O. and Zaharia, M. (2020). Colbert: Efﬁcient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In The 2015 International Conference for Learning . Kwiatkowski, T., Palomaki, J., Redﬁeld, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin, J., Lee, K., Toutanova, K. N., Jones, L., Chang, M.-W., Dai, A., Uszkoreit, J., Le, Q., and Petrov, S. (2019). Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics . Lee, K., Chang, M.-W., and Toutanova, K. (2019). Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. (2020a). BART: Denoising pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., Riedel, S., and Kiela, D. (2020b). generation for nlp tasks. In Advances in Neural Information Processing Systems . Luan, Y., Eisenstein, J., Toutanova, K., and Collins, M. (2021). Sparse, Dense, and Attentional for Text Retrieval. Transactions of the Association for Computational Linguistics , 9. Min, S., Chen, D., Hajishirzi, H., and Zettlemoyer, L. (2019). A discrete hard EM approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , Hong Kong, China. Association for Computational Linguistics. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). Pytorch: An imperative style, high- performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems . Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. (2019). Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Qi, P., Zhang, Y., Zhang, Y., Bolton, J., and Manning, C. D. (2020). Stanza: A Python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations . 12 Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research ,21(140), 1–67. Roberts, A., Raffel, C., and Shazeer, N. (2020). How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Robertson, S. and Zaragoza, H. (2009). The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval . Sachan, D. S., Patwary, M., Shoeybi, M., Kant, N., Ping, W., Hamilton, W. L., and Catanzaro, B. (2021). End-to-end training of neural retrievers for open-domain question answering. In Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP) . Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019). Megatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint . Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems . Wang, S., Yu, M., Guo, X., Wang, Z., Klinger, T., Zhang, W., Chang, S., Tesauro, G., Zhou, B., and Jiang, J. (2018). R3: Reinforced ranker-reader for open-domain question answering. In AAAI . Wang, Z., Ng, P., Ma, X., Nallapati, R., and Xiang, B. (2019). Multi-passage BERT: A globally normalized BERT model for open-domain question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Yih, W.-t., Toutanova, K., Platt, J. C., and Meek, C. (2011). Learning discriminative projections for text similarity measures. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning . Yogatama, D., de Masson d’Autume, C., and Kong, L. (2021). Adaptive Semiparametric Language Models. Transactions of the Association for Computational Linguistics ,9, 362–373. 13 Checklist 1.For all authors... (a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] Please see the model (§ 2) and result (§ 3) sections that solidify the claims made in the abstract and introduction sections. (b)Did you describe the limitations of your work? [Yes] Please see limitations in § 5. (c)Did you discuss any potential negative societal impacts of your work? [Yes] Please see negative societal impact in § 5. (d)Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2.If you are including theoretical results... (a)Did you state the full set of assumptions of all theoretical results? [N/A] (b)Did you include complete proofs of all theoretical results? [N/A] 3.If you ran experiments... (a)Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] We include the code, data, and instructions in the supplemental material and § 3.2. (b)Did you specify all the training details (e.g., data splits, , how they were chosen)? [Yes] We specify these details in the appendix included in the supplementary material. (c)Did you report error bars (e.g., with respect to the random seed after running exper- iments multiple times)? [No] Our experiments are compute expensive and it is not feasible to perform multiple runs of the same experiment with different seeds. All our training runs use the same seed value (1234). As an alternative to running multiple seeds, we perform a number of ablation studies (§ 3.5). (d)Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Please see § 3.2under hardware and library. 4.If you are using existing assets (e.g., code, data, models) or new assets... (a)If your work uses existing assets, did you cite the creators? [Yes] Please see § 3.1for the details. (b)Did you mention the license of the assets? [Yes] Our work is based on open-source data and framework. When applicable, we describe the license information in the appendix. (c)Did you include any new assets either in the supplemental material or as a URL? [Yes] We include our code in the supplementary material. (d)Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e)Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5.If you used crowdsourcing or conducted research with human subjects... (a)Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b)Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c)Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14 
fixes that fail self defeating improvements in machine learning systems 	Fixes That Fail: Self-Defeating Improvements in Systems Ruihan Wu∗ Cornell University Guo Awni Hannun Laurens van der Maaten Facebook AI Research { Abstract systems such as self-driving cars or virtual assistants are com- posed of a large number of models that recognize image content, transcribe speech, analyze natural language, infer preferences, rank options, etc. Models in these systems are often developed and trained independently, which raises an obvious concern: Can improving a model make the overall system worse? We answer this question afﬁrmatively by showing that improving a model can deteriorate the performance of downstream models, even after those downstream models are retrained. Such self-defeating improvements are the result of entanglement between the models in the system. We perform an error decomposition of systems with multiple models, which sheds light on the types of errors that can lead to self-defeating improvements. We also present the results of experiments which show that self-defeating improvements emerge in a realistic stereo-based detection system for cars and pedestrians. 1 Introduction Progress in machine learning has allowed us to develop increasingly sophisticated artiﬁcially intel- ligent systems, including self-driving cars, virtual assistants, and complex robots. These systems generally contain a large number of models that are trained to perform modular tasks such as recognizing image or video content, transcribing speech, analyzing natural language, eliciting user preferences, ranking options to be presented to a user, etc. The models feed each other information: for example, a pedestrian detector may use the output of a model as input. Indeed, systems can be interpreted as directed acyclic graphs in which each vertex corresponds to a model, and models feed each other information over the edges in the graph. In practice, various models in this graph are often developed and trained independently [ 22]. This modularization tends to lead to more usable APIs but may also be motivated by other practical constraints. For example, some of the models in the system may be developed by different teams (some models in the system may be developed by a cloud provider) ; models may be retrained and deployed at a very different cadence ( models may be updated very regularly but models may not) ; new downstream models may be added continuously in the graph (user-speciﬁc models may need to be created every time a user signs up for a service) ; or models may be (gradient- boosted decision trees are commonly used for selection of discrete features) . This can make through the models in the graph infeasible or highly undesirable in many practical settings. The scenario described above leads to an obvious potential concern: Can improving a machine- learning model make the system worse? We answer this question afﬁrmatively by showing how improvements in an upstream model can deteriorate the performance of downstream models, even if all the downstream models are retrained after updating the upstream model. Such ∗Work performed during internship at Facebook AI Research. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). self-defeating improvements are caused by entanglement between the models in the system. To better understand this phenomenon, we perform an error decomposition of simple systems. Our decomposition sheds light on the different types of errors that can lead to self- defeating improvements: we provide illustrations of each error type. We also show that self-defeating improvements arise in a realistic system that detects cars and pedestrians in stereo images. Our study opens up a plethora of new research questions that, to the best of our knowledge, have not yet been studied in-depth in the community. We hope that our study will encourage the community to move beyond the study of models in isolation, and to study systems more holistically instead. 2 Problem Setting We model a system as a static directed acyclic graph (DAG) of models ,G= (V,E), where each vertex v∈V corresponds to a model and each edge (v, w)∈Erepresents the output of model vbeing used as input into model w. For example, vertex vcould be a model and vertex wa model that operates on the depth estimates produced by vertex v. A model is upstream ofvif it is an ancestor of v. A downstream model of vis a descendant of v. An example of a system with three models is shown in Figure 1 (we study this model in Section 3.2). ) Figure 1: Example of a system with three models, V={u, v, w}, and two edges, E={(u, w),(v, w)}. The system receives an example xas input, of which parts x(u),x(v), and x(w)are fed into u,v, andw, respectively. Model w’s input is concatenated with the outputs of its parents, vertex in the system G corresponds to a model function, v(·), that op- erates on both data and outputs from its up- stream models. Denote the output from model vbyfv(x). The output of a source model v isfv(x) =v(x(v)), where x(v)represents the part of input xthat is relevant to model v. The output of a non-source model wis given by fw(x) = w([ x(w), fv(x) :v∈Pw]) , where Pw={v: (v, w)∈E} are the parents of w. Training. To train model v, we as- sume access to a training set Dv= {(x1,y1), . . . , ( xn∈X and corresponding targets yn∈Yv. For each model, we also assume a training al- gorithm Av(Dv)that selects a model v∈H v from hypothesis set Hvbased on the training set. The learning algorithm, Av, the hypothesis set, Hv, and the training set, Dv, are ﬁxed during training but they may change each time model vis re-trained or updated. The data space X, the target spaceYv, and the way in which inputs x(v)are obtained do not change between model updates. We assume that models in Gmay be trained separately rather than jointly, that is, the learning signal obtained from training a downstream model, v, may not be backpropagated into its upstream dependencies, w∈Pv. This assumption is in line with constraints that commonly arise in real-world machine learning systems; see our motivation below. Note that this assumption also implies that a downstream model cannot be trained before all its upstream models are trained. Evaluation. To be able to evaluate the performance of the task at v, we assume access to a ﬁxed test set ¯Dvthat is deﬁned analogously to the training set: it contains Mvtest examples ¯xm∈X and corresponding targets ¯ym∈Yv. In contrast to the training set, the test set is ﬁxed and does not change when the model is updated. However, we note that the input distribution into a non-root model is not only governed by the test set ¯Dvbut also by upstream models: if an upstream model changes, the inputs into a downstream model may change as well even though the test set is ﬁxed. We also assume access to a test loss function ℓv(v,¯Dv)for each model v, for example, classiﬁcation error (we assume lower is better). Akin to the test set ¯Dv, all test loss functions ℓvare ﬁxed. Hence, we always measure the generalization ability of a model, v, in the exact same way. The test loss function may not have trainable parameters, that is, it must be a “true” loss function. 2 Updating models. The system we deﬁned can be improved by updating a model. In such an update, an individual model vis replaced by an alternative model v′. We can determine if such a model update constitutes an improvement by evaluating whether or not its test loss decreases: ). When vis not a leaf in G, the update to v′may affect the test loss of downstream models as well. In practice, the improvement due to v′often also improves downstream models ( e.g., [19]), but this is not guaranteed. We deﬁne a self-defeating improvement as a situation in which replacing vbyv′improves that model but deteriorates at least one of the downstream models w, even after all the downstream models are updated ( i.e., re-trained using the same learning algorithm) to account for the changes incurred by v′. Deﬁnition 2.1 (Self-defeating improvement) .Denoting the set of all downstream models (descen- dants) of vinGasCv, improvement arises when: ∃)), (1) where w′represents a new version of model wthat was trained after model vwas updated to v′to account for the input distribution change that the update of vincurs on w. Motivation of problem setting. In the problem setting described above, we have made two assump- tions: (1) models may be (re-)trained separately rather than jointly and (2) the training algorithm, hypothesis set, and training data may change between updates of the model.2Both assumptions are motivated by the practice of developing large systems that may comprise thousands of models [22, 28]. Joint model training is often infeasible in such systems because: •Some of the models may have been developed by cloud providers [34] and cannot be changed. • may be technically infeasible or inefﬁcient, e.g., when models are implemented in incompatible learning frameworks or when their training data live in different data centers. •Some models may be re-trained or updated more often than others. For example, models may be updated every few hours to adapt to changes in the data distribution, but re-training large language models [8] at that same cadence is not very practical. •Upstream models may have hundreds or even thousands of downstream dependencies, complicating appropriate weighting of the downstream learning signals that multi-task learning [10] requires. •Some models may be , e.g., decision trees are commonly used for feature selection in large sets of features. We assume that changes in the training algorithm, hypothesis set, and training data may occur because model owners (for example, cloud providers) constantly seek to train and deploy improved versions of their models with the ultimate goal of improving the system(s) in which the models are used. 3 Understanding Self-Defeating Improvements via Error Decompositions Traditional software design relies heavily on modules : independent and components that are combined to form complex software systems [ 7, §7.4]. Well-designed modules provide an explicit speciﬁcation of their inputs, outputs, and functionality. In traditional software engineering, good modularization allows developers to modify modules independently because the effects of those modiﬁcations on the rest of the system are relatively predictable. models do not resemble traditional software modules because they lack an explicit speciﬁcation of their functionality [ 14]. As a result, there is often a signiﬁcant degree of entanglement between different models in the system [ 22]. This makes the system-wide effects of model modiﬁca- tions unpredictable, and can lead to situations where improving a model does not improve the entire system. To understand such self-defeating improvements, we study Bayes error decompositions of simple systems. Speciﬁcally, we study a system with two models in Section 3.1 and a system with three models in Section 3.2. 3.1 Error Decomposition of System with Two Models We adapt standard Bayes error decomposition [ 6,33] to study a simple system that has two vertices, V={v, w}, and a dependency between upstream model vand downstream model w, that is, E={(v, w)}. We denote the Bayes-optimal downstream model by w∗; the optimal downstream 2The training data is ﬁxed during training, i.e., we do not consider online learning settings in our setting. 3 Downstream model w’ Downstream model optimal downstream model Upstream model vUpstream model v’Upstream 2: Illustration of a self-defeating improvement due to an increase in the downstream approx- imation error. Upstream model vpredicts a 2D point for each example. Downstream model wis a linear classiﬁer separating the points into two classes (red and green). Left: Ground-truth target distribution for the upstream model. Middle: Point predictions by upstream model v, the optimal v-conditioned downstream classiﬁer w†, and the downstream classiﬁer wlearned. Right: Point predictions by improved upstream model v′, the optimal v′-conditioned downstream classiﬁer w†, and the downstream classiﬁer w′learned. Note how the predictions of v′better match the ground-truth, butw′cannot exploit the improvement because it is restricted to be linear. model conditioned on an upstream model by w†; and the optimal downstream model in the hypothesis set Hwbyw† Hw. Using these deﬁnitions, we can decompose the downstream riskof model w, given upstream model v, as follows: E[ℓw(w◦v)−ℓw(w∗)] = ∗)] upstream error+E[ℓw(w† Hw◦v)−ℓw(w†◦v)]   downstream approximation † Hw◦v)]  downstream estimation error,(2) where◦denotes function composition, and the expectations are under the data distribution. The error decomposition is similar to standard decompositions [ 6,33] but contains three terms instead of two.3 Speciﬁcally, the upstream error does not arise in prior error decompositions, and the downstream approximation anddownstream estimation errors differ from the standard decomposition. Barring variance in the error estimate due to the ﬁnite size of test sets ¯Dw(which is an issue that can arise in any problem), a self-defeating improvement occurs when a model update from (v, w)to(v′, w′)reduces the upstream risk but increases the downstream risk. An increase in the downstream risk implies that at least one of the three errors terms in the composition above must have increased. We describe how each of these terms may increase due to a model update: •Upstream error measures the error that is due to the upstream model vnot being part of the Bayes- optimal solution w∗. It increases when an improvement in the upstream loss does not translate in a reduction in Bayes error of the downstream model, i.e., when: ℓw(w†◦v′)> ℓw(w†◦v). An upstream error increase can happen due the loss mismatch : a setting in which the test loss functions, ℓ, of the upstream and downstream model optimize for different things. For example, the upstream test loss function may not penalize errors that need to be avoided in order for the downstream test loss to be minimized, or it may penalize errors that are irrelevant to the downstream model. Alternatively, the upstream error may increase due to distribution mismatch : situations in which the upstream loss focuses on parts of the data-target space X×Y vthat are unimportant for the downstream model (and vice versa). For example, suppose the upstream model vis an model that aims to learn a feature representation that separates cats from dogs, andfv(x)is a feature representation obtained from that model. A downstream model, w, that distinguishes different dog breeds based on fvmay deteriorate when the improvement of model v collapses all of dog images in fv. Examples of this were observed in, e.g., [19]. •Downstream approximation error measures the error due to the optimal w†not being in the hypothesis setHw. The approximation error increases when the downstream model, w, is unable to exploit improvements in the upstream model, v, because exploiting those improvements would require selection of a model that is not in that hypothesis set. 3For brevity, we do not include optimization error of [5] in our Bayes error decomposition. 4 Upstream model vUpstream model v’Upstream model w’Downstream model optimal model in hypothesis set optimal downsteam model v1v2v1v2v2v1 Figure 3: Illustration of a self-defeating improvement due to an increase in the downstream estimation error. Upstream model vpredicts a 2D point for each example. Downstream model wis a quadratic classiﬁer separating the points into two classes (red and green). Left: Ground-truth target samples and distribution for the upstream model. Middle: Point predictions by upstream model v, the corresponding optimal v-conditioned downstream model in Hw,w† Hw, and the downstream classiﬁer wlearned based on the training examples. Right: Point predictions by improved upstream model v′, the corresponding optimal v′-conditioned downstream model in Hw,w† Hw, and the downstream classiﬁer w′learned based on the training examples. Note how the predictions of v′better match the ground-truth, but w′deteriorates because it only receives Nw= 6training examples. Figure 2 illustrates how this situation can lead to a self-defeating improvement. In the illustration, the upstream model, v, predicts a 2D position for each data point. The downstream model, w, separates examples into the positive (green color) and negative (red color) class based on the prediction of vusing a linear classiﬁer. The predictions of the improved upstream model, v′, better match the ground-truth targets: the predictions of v′(right plot) match the ground-truth targets (left plot) more closely than those of v(middle plot). However, the re-trained downstream model, w′, deteriorates because the resulting classiﬁcation problem is more non-linear: the downstream model cannot capitalize on the upstream improvement because it is linear. The resulting increase in approximation error leads to the self-defeating improvement in Figure 2. •Downstream estimation error measures the error due to the model training being performed on a ﬁnite data sample with an imperfect optimizer, which makes ﬁnding w† Hwdifﬁcult in practice. The estimation increases, for example, when the upstream model improves but the downstream model requires more than Nwtraining samples to capitalize on this improvement. Figure 3 shows an example of a self-defeating improvement caused by an increase of the downstream estimation error. As before, the upstream model, v, predicts a 2D position for each data point in the example. The downstream model wis selected from the set, Hw, of quadratic classiﬁers. It is tasked with performing binary classiﬁcation into a positive class (green color) and negative class (red color) based on the 2D positions predicted by the upstream model. To train the binary classiﬁer, the downstream model w(andw′) is provided with Nw= 6labeled training examples (the green and red markers in the plot). In the example, the upstream model v′performs better than its original counterpart v: the predictions of v′(right plot) match the ground-truth targets (left plot) more closely than those of v(middle plot). However, the upstream improvement hampers the downstream model even though the optimal downstream model w†is in the hypothesis setHwboth before and after the upstream improvement. After the upstream improvement, the optimal downstream model that can be selected from the hypothesis set, w† Hw, is more complex, which makes it harder to ﬁnd it based on the ﬁnite number of Nwtraining examples. This results in an increase in the estimation error, which causes the self-defeating improvement in Figure 3. 3.2 Error Decomposition of System with Two Upstream Models More complex types of self-defeating improvements may arise when a downstream model depends on multiple upstream models that are themselves entangled. Consider the system of Figure 1 that has three models, V={u, v, w}, and dependencies between upstream models uandvand downstream model w, that is,E={(u, w),(v, w)}. The error decomposition for this system is 5 Upstream model uUpstream model vDownstream model w -+-+ Upstream model u’Upstream model vDownstream model w’ -+-++ +-++-++++Model after update (u’, v, w’)Model before update (u, v, w)Positive (ground truth)Negative (ground truth) Upstream model uUpstream model vDownstream model w -+--++-+ Upstream model u’Upstream model vDownstream model w’ -+-++--++(a) errors(b) Correlated 4: Illustrations of self-defeating improvement due to an increase in the upstream compatibility error. Green areas should be labeled as positive; red areas as negative. Left (a and b) :Original upstream model u(top; blue line and +,−symbols) and its improved version u′(bottom; red). Middle (a and b) :Upstream model v.Right (a and b) :Original downstream model w(top) and its improved version w′(bottom). In (a), the errors of uandvare . As a result, the improved upstream model u′negatively impacts the re-trained downstream model w′. In (b), the improved upstream model u′is identical to the other upstream model v. As a result, w′looses the additional information previously provided by u, negatively impacting its performance. similar to Equation 2, but we can further decompose the upstream error. Denoting the Bayes-optimal downstream model by w∗, the optimal model given upstream models uandvbyw† u,v, and the optimal upstream model vgiven upstream model ubyv† u, we decompose the upstream error as follows: E[ℓw(w† u,v◦(u, v))−ℓw(w∗)]   upstream error= E[ℓw(w† u,v◦(u, v))−ℓw(w† u,v◦(u, v† u))]    upstream compatibility error+E[ℓw(w† u,v◦(u, v† u))−ℓw(w∗)]    excess upstream error. (3) Theexcess upstream error is similar to the upstream error in Equation 2: upstream model umay be suboptimal for the downstream task, for example, because of loss mismatch or distribution mismatch. The key observation in the error decomposition of a system with two upstream models is that the optimal upstream model vis a function of upstream model u(and vice versa). The upstream compatibility error captures the error due to upstream model vnot being identical to the optimal v† u. A self-defeating improvement can occur when we update upstream model utou′because it may be thatv† u̸=v† u′, which can cause the upstream compatibility error to increase. We provide two examples of this in Figure 4. The ﬁrst example (left pane) shows a self-defeating improvement due to upstream models uandvmaking errors that cancel each other out. The second example (right pane) is a self-defeating improvement due to u′making more correlated predictions withv. We note that, in both examples, the optimal vdepends on uand the self-defeating improvement arises because of an increase in the upstream compatibility error. In the examples in Figure 4, all three models aim to distinguish two classes: green (positive class) and red (negative class). Upstream models uandvdo so based the (x, y)-location of points in the 2D plane. The downstream model operates on the (hard) outputs of the two upstream models. In Figure 4(a), the original upstream models uandvmake errors that are . The original downstream model wexploits this to make perfect predictions. When upstream model uis replaced by an improved model u′that makes no errors, a self-defeating improvement arises: downstream model wno longer receives the information it needs to separate both classes perfectly. In Figure 4(b), upstream model uis improved to u′, which makes the exact same predictions as the other upstream model v. As a result, downstream model w′no longer has access to the complementary information that uwas providing in addition to v, producing the self-defeating improvement. 6 Disparity estimation model uStereo image (xleft, map d Detected cars (in 3D)Detected pedestrians (in 3D)Point cloud generator r Pseudo-LiDAR Car detection model vPedestrian detection model w Figure 5: Overview of the car and pedestrian detection system used in our case study in Section 4. The system consists of three models: (1) a model uthat generates a pseudo-LiDAR representation of the environment, (2) a car-detection model vthat operates on 3D point clouds, and (3) a model wthat performs 3D pedestrian detection. While the examples in Figure 4 may seem contrived, errors and correlated predictions can arise in practice when there are dependencies in the targets used to train different models. For example, suppose the upstream models are trained to predict ad clicks ( will a user click on an ad? ) and ad conversions ( will a user purchase the product being advertised? ), respectively. Because conversion can only happen after a click happens, there are strong dependencies between the targets used to train both models, which may introduce correlations between their predictions. 4 Case Study: Pseudo-LiDAR for Detection We perform a case study on self-defeating improvements in a system that may be found in self-driving cars. In both cases, the system uses a pseudo-LiDAR [ 37] as the basis for 3D detection of cars and pedestrians. Figure 5 gives an overview of the models in the system: •A model, u, predicts a map dwith disparities corresponding to every pixel in the left image of a stereo image (a pair of rectiﬁed images from two horizontally aligned cameras). •A point cloud generator, r, uses disparity map dto create a pseudo-LiDAR representation of the 3D environment. The point cloud generator, r, is a simple non-learnable module [37]. •The stereo image and disparity map are input into a model, v, that performs 3D detection of cars. •The same inputs are used in a separate model, w, that aims to perform 3D pedestrian detection. 4.1 System Design Car detection Pedestrian detection P-RCNN F-PTNET P-RCNN F-PTNET AP3Du 39.91 33.64 34.42 43.02 u′38.75 32.78 27.64 43.10 y 68.06 55.46 55.19 62.16 APBEVu 50.34 43.79 38.51 50.49 u′49.30 42.77 31.47 48.29 y 78.43 67.35 63.00 65.92 Table 1: Average precision of car detection and pedestrian detection measured for 3D box view (AP 3D) and 2D birds-eye view (AP BEV). Higher is better. Results are shown for baseline disparity- estimation model uand an improved disparity- estimation model u′that is trained using a differ- ent loss. The AP of oracle model yis shown for reference. Improving the model leads to a self-defeating improvement in both the car detection model and the pedestrian detection model.The and detection models used in our case study are described below. Disparity estimation. To perform disparity es- timation, we adopt the PSMNet model of Chang and Chen [11]. The model receives a stereo im- ), as input and aims to compute a disparity map, ), that con- tains a disparity estimate for each pixel in image xleft. We experiment with two training loss func- tions: (1) the depth mean absolute error and (2) the disparity mean absolute error (MAE). Given a ground-truth disparity map, y, and a function that maps disparities to depths, g(d) =C d, for some camera constant C, the disparity MAE is proportional to∥d−y∥1and the depth MAE is proportional to∥g(d)−g(y)∥1. To evaluate the quality of model u, we measure disparity MAE: ℓu(u,¯Du) =1 M∑M m=1∥d−y∥1. We expect that a model, u′, trained to minimize disparity 7 MAE will provide better disparity estimates (per test loss ℓu) than a model, u, that is trained to minimize depth MAE. This has downstream effects on performance of the detection models. Car and pedestrian detection. We experiment with two different models for performing detection of cars and pedestrians, viz., the Point-RCNN model (P-RCNN; Shi et al. [30]) and the Frustum PointNet detection model (F-PTNET; Qi et al. [26]). Both models take stereo image (xleft,xright) and point cloud r(d)as input. Before computing r(d), we perform winsorization on the prediction u(xleft,xright): we remove all points that are higher than 1meter in the point cloud (where the camera position is the origin). Both 3D car detection model and 3D pedestrian detection model are trained to detect their target objects at any distance from the camera. Following Geiger et al. [16], we evaluate the test loss of car-detection model, ℓv, using the negative average precision (AP) for an (IoU) of 0.7. The test loss of the pedestrian- detection model, ℓw, is the negative AP for an IoU of 0.5. The test-loss is only evaluated on pedestrians whose distance to the camera is ≤20meters. We measure both the car-detection and the test losses for both the 3D box view ( −AP3D) and the 2D bird-eye view (−APBEV); see Geiger et al. [16] for details on the deﬁnition of the test-loss functions. 4.2 Experiments Range 0-max 0-20 20-40 40-max Model u 1.28 1.32 1.12 1.11 Model u′1.21 1.21 1.20 1.20 Table 2: Disparity MAE test loss of models u(trained to minimize depth MAE) and u′(trained to minimize disparity MAE), split out per range of ground-truth depth values (in meters). Model ubetter predicts the disparity of nearby points, but u′works better on distant points.We evaluate our system on the KITTI dataset [ 16, CC BY-NC-SA 3.0], using the split of [ 12]. Our baseline model, u, that is trained to minimize depth MAE obtains a test loss of 1.28 (see Table 2). The improved version of that model, u′, is trained to minimize disparity MAE and obtains a test loss of 1.21, conﬁrming the model improvement. Table 1 presents the test losses of the downstream models for car and pedestrian detection, for both the baseline upstream model uand the improved model u′. We observe that the improvement in disparity estimation leads to self- defeating improvements on both the car-detection and the tasks. The AP 3Dof the car detector drops from 50.34to49.30in AP BEV). For pedestrian detection, AP 3Dis unchanged but AP BEVdrops from 50.49to48.29. Interestingly, these self-defeating improvements are due to increases in different error terms. P-RCNN F-PTNET AP3Du 30.53 44.10 u′34.20 46.14 y 55.19 62.16 APBEVu 34.09 51.84 u′38.83 53.99 y 64.25 65.23 Table 3: Average precision at IoU 0.5 of pedestrian detec- tion measured for 3D box view (AP 3D) and 2D birds-eye view (AP BEV) for two different detec- tion models (P-RCNN and F- PTNET). Pedestrian detector is trained by removing pedestrians farther than 20meters. Results are shown for the baseline dis- parity estimation model, u, the improved model, u′, and an ora- cle model, y. Higher is improvement in car detection. The self- defeating improvement observed for car detection is likely due to an increase in the upstream error. A model that is trained to minimize depth MAE ( i.e., model u) focuses on making sure that small disparity values (large depth values), are predicted correctly. By contrast, a model trained to minimize disparity MAE (model u′) aims to predict large disparity values (small depth values) correctly, sacriﬁcing accuracy in predictions of large depth values (see Table 2). This negatively affects the car detector because most cars tend to be relatively far away. In other words, the loss function that improves model udeteriorates the downstream model because it focuses on errors that are less relevant to that model, which increases the upstream error. Self-defeating improvement in pedestrian detection. Different from car detection, the self-defeating improvement in pedestrian detection is likely due to an increase in the downstream approxima- tion or estimation error. Table 2 shows that whereas the disparity MAE test loss of u′increases for large depth values, it decreases in the 0-20 meter range. As the evaluation focuses on pedestrians within 20 meters, we expect the pedestrian detector w′to improve along with model u′: that is, the upstream error likely decreases. 8 However, the point produced by model u′are likely noisier for far-away pedestrians than those produced by u(see Table 2). Because the downstream model is trained to (also) detect these far-away pedestrians, the noise in the pedestrian point clouds makes it harder for that model to learn a good model of the shape of pedestrians. This negatively affects the ability of w′to detect nearby pedestrians. Indeed, if the pedestrian detector is only trained on nearby pedestrians, the self-defeating improvement disappears; see Table 3. Hence, the observed self-defeating improvement was due to an increase in a downstream error term: the upstream model did improve for the downstream task, but the downstream model was unable to capitalize on that improvement. 5 Related Work This study is part of a larger body of work around engineering [9,21,23]. Sculley et al. [28] provides a good overview of the problems that engineering studies. The self-defeating improvements studied in this paper are due to the entanglement problem [ 1,22], where “changing anything changes everything” [ 28]. Entanglement has also been studied in work on “” learners [ 29,32,35]. Prior work has also studied how the performance of humans operating a system may deteriorate when the system is improved [3]. Some aspects of self-defeating improvements have been studied in other domains. In particular, the effect of an upstream model change on a downstream model can be viewed as domain shift [4,25]. However, approaches to domain shift such as importance weighting of training examples [ 31] are not directly applicable because they require the input space of both models to be identical. The study of self-defeating improvements is also related to the study of transfer learning ,e.g., [2,19,24]. Prior work on transfer learning has reported examples of self-defeating improvements: e.g., [19] reports that some regularization techniques improve the accuracy of convolutional networks on ImageNet, but negatively affect the transfer of those networks to other recognition tasks. Finally, work on differentiable programming [18,36] is relevant to the study of self-defeating improvements. In some situations, a self-defeating improvement may be resolved by learning signals from a model further upstream. However, such an approach is not a panacea as downstream tasks may have conﬂicting objectives. Moreover, technical or practical obstacles may prevent the use of differentiable programming in practice (see Section 2). 6 Conclusion and Future Work This study explored self-defeating improvements in modular systems. We presented a new error decomposition that sheds light on the error sources that can give rise to self-defeating improvements. The ﬁndings presented in this study suggest a plethora of directions for future work: •It may be possible to derive bounds on the error terms in Equation 2 that go beyond the current art. For example, it is well-known that the trade-off between approximation and estimation error leads to an excess risk that scales between the inverse and the inverse square root of the number of training examples [ 27,39]. New theory may provide more insight into the effect of the upstream hypothesis set and training set size on those errors, and may bound the upstream error term in 2. •Our study focused on ﬁrst-order entanglement between two models (either an upstream and a downstream model or two upstream models). Some settings may give rise to higher-order entanglements, for example, between three upstream models whose errors cancel each other out. •Our deﬁnition of different types of entanglement may pave the way for the development of diagnostic tools that identify the root cause of a self-defeating improvement. •We need to develop best practices that can help prevent self-defeating improvements from occurring. One way to do so may be by tightening the “API” of the models, e.g., by calibrating classiﬁer outputs [ 17] or whitening feature [ 20]. Such transforms constrain a model’s output distribution, which may reduce the downstream negative effects of model changes. Alternatively, we may train models that are inherently more robust to changes in their input distribution, e.g., using training [38] or meta-learning [15]. •We also need to study self-defeating improvements of other aspects. An important open question is [13]: Can making an upstream model fairer make its downstream dependencies less fair? We hope this paper will encourage the study of such questions, and will inspire the community to rigorously study systems in addition to individual models. 9  The authors thank Yan Wang, Yurong You, and Ari Morcos for helpful discussions and advice.  
one question answering model for many languages with cross lingual dense passage retrieval 	One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval Akari Asai†, Xinyan Yu†, Jungo Kasai†, Hannaneh Hajishirzi†‡ †University of Institute for AI {akari, xyu530, jkasai, Abstract We present Cross-lingual Open-Retrieval Answer Generation ( CORA ), the ﬁrst uniﬁed many-to-many question answering (QA) model that can answer questions across many languages, even for ones without annotated data or knowledge sources. We introduce a new dense passage retrieval algorithm that is trained to retrieve documents across languages for a question. Combined with a multilingual autoregressive generation model, CORA answers directly in the target language without any translation or in-language retrieval modules as used in prior work. We propose an iterative training method that automatically extends annotated data available only in high-resource languages to low-resource ones. Our results show that CORA substantially outperforms the previous state of the art on multilingual open QA benchmarks across 26 languages, 9 of which are unseen during training. Our analyses show the signiﬁcance of cross-lingual retrieval and generation in many languages, particularly under low-resource settings. Our code and trained model are publicly available at / CORA . 1 Introduction Multilingual open question answering (QA) is the task of answering a question from a large collection of multilingual documents. Most recent progress in open QA is made for English by building a pipeline based on a dense passage retriever trained on large-scale English QA datasets to ﬁnd evidence passages in English (Lee et al., 2019; Karpukhin et al., 2020), followed by a reader that extracts an answer from retrieved passages. However, extending this approach to multilingual open QA poses new challenges. Answering multilingual questions requires retrieving evidence from knowledge sources of other languages than the original question since many languages have limited reference documents or the question sometimes inquires about concepts from other cultures (Asai et al., 2021; Lin et al., 2020). Nonetheless, large-scale cross-lingual open QA training data whose questions and evidence are in different languages are not available in many of those languages. To address these challenges, previous work in multilingual open QA (Ture and Boschee, 2016; Asai et al., 2021) translates questions into English, applies an English open QA system to answer in English, and then translates answers back to the target language. Those pipeline approaches suffer from error propagation of the machine translation component into the downstream QA, especially for low-resource languages. Moreover, they are not able to answer questions whose answers can be found in resources written in languages other than English or the target languages. In this paper, we introduce a uniﬁed many-to-many QA model that can answer questions in anytarget language by retrieving evidence from anylanguage and generating answers in the target language. Our method (called CORA, Fig. 1) extends the approach of English open QA (Lewis et al., 2020; Izacard and Grave, 2021b) with a single cross-lingual retriever and a generator that do not rely on retrievers or machine translation modules. The multilingual 35th Conference on Neural Information Processing Systems (NeurIPS 2021). Question Encoder Maximum Inner Product Search mDPR : Multilingual Dense Passage Retriever mGEN : Multilingual Answer Generator Generator qL: quién escribió la canción eye of the tiger (who wrote the song eye of the tiger ) Eye of the Tiger «Eye of the Tiger» fue escrita Frankie Sullivan y Jim Peterik Eye of the Tiger Eye of the Tiger פזמון הוא הרוק להקת של בביצועה ו שכתבו, סורבייבר האמריקנית סאליבן פרנקי Passage Encoder Frankie Sullivan aL Figure 1: Overview of CORA (mDPR and mGEN). retrieval module ( mDPR ) produces dense embeddings of a question and all multilingual passages, thereby retrieving passages across languages. The generation module ( mGEN ) is trained to output an answer in the target language conditioned on the retrieved multilingual passages. To overcome the aforementioned data scarcity issue, we automatically mine training data using external language links and train mDPR and mGEN iteratively. In particular, each iteration proceeds over two stages of updating model parameters with available training data and mining new training data by Wikipedia language links and predictions made by the models. This approach does not require any additional human annotations or machine translation, and can be applied to many new languages with low resources. Our experiments show that CORA advances the state of the art on two multilingual open QA datasets, XOR-TYDIQA (Asai et al., 2021) and MKQA (Longpre et al., 2020), across 26 typologically diverse languages; CORA achieves gains of 23.4 and 4.7 F1 points in XOR-TYDIQA and MKQA respectively, where MKQA data is not used for training. Moreover, CORA achieves F1 scores of roughly 30 over 8 languages on MKQA that have no training data or even reference Wikipedia documents, outperforming the approach by 5.4 F1 points. Our controlled experiments and human analyses illustrate the impact of many-to-many cross-lingual retrieval in improving multilingual open QA performance. We further observe that through cross-lingual retrieval, CORA can ﬁnd answers to 20% of the multilingual questions that are valid but are originally annotated as unanswerable by humans due to the lack of evidence in the English knowledge sources. 2 Method We deﬁne multilingual open QA as the task of answering a question qLin a target language Lgiven a collection of multilingual reference passages Cmulti, where evidence passages can be retrieved from any language . These passages come from Wikipedia articles that are not necessarily parallel over languages. We introduce CORA, which runs a procedure to achieve this goal (Fig. 1). We further introduce a novel training scheme of iterative training with data mining (§ 2.2). 2.1 CORA Inference CORA directly retrieves evidence passages from anylanguage for questions asked in anytarget language, and then generates answers in the target language conditioned on those passages. More for- mally, the CORA inference consists of two steps of (i) retrieving passages Pmultiand (ii) generating an answer aLbased on the retrieved passages. Pmultican be in any language included in Cmulti. ), ). Multilingual Dense Passage Retriever (mDPR). mDPR extends Dense Passage Retriever (DPR; Karpukhin et al., 2020) to a multilingual setting. mDPR uses an iterative training approach to ﬁne-tune a pre-trained multilingual language model (e.g., mBERT; Devlin et al., 2019) to encode passages and questions separately. Once training is done, the for all passages from Cmultiare computed ofﬂine and stored locally. Formally, a passage encoding is obtained as follows: epL=mBERT p(p), where a passage pis a ﬁxed-length sequence of tokens from multilingual documents. At inference, mDPR independently obtains a d-dimensional ( d=768) encoding of the 2 Initial QA data mDPR1mGEN1 mGEN2 mDPR2 Negative Training Inference mDPR retrieved data WikiData Positive New training data 1. Initial mDPR Training 2. Training mGEN from mDPR data 3. Cross-lingual Data Mining 4. Automatic Labeling Wikidata-based data 5. Iteratively training mDPR & mGEN mGEN1 outputs: ✔ ❌ Figure 2: Overview of CORA iterative training and data mining. question eqL=mBERT q(qL). It retrieves kpassages with the khighest relevance scores to the question, where the relevance score between a passage pand a question qLis estimated by the inner product of their encoding vectors, ⟨eqL,ep⟩. Multilingual Answer Generator (mGEN). We use a multilingual model (e.g., mT5; Xue et al., 2021) to generate answers in the target language token-by-token given the retrieved multilingual passages Pmulti. We choose a generation approach because it can generate an answer in the target language Lfrom passages across different , the generator can be adapted to unseen languages, some of which may have little or no translation training data. Speciﬁcally, the generator outputs the sequence probability for aLas follows: 5 ip(aL i∣aL <i, qL,Pmulti), (1) where aL idenotes the i-th token in the answer, and Tis the length of the answer. We append a language tag to the question to indicate the target language. 2.2 CORA Training We introduce an iterative training approach that encourages cross-lingual retrieval and answer generation conditioned on multilingual passages (sketched in Fig. 2 and Alg. 1). Each iteration proceeds over two stages: parameter updates (§ 2.2.1) where mDPR and mGEN are trained on the current training data and cross-lingual data mining (§ 2.2.2) where training data are automatically expanded by Wikipedia language links and model predictions. Initial training data. The initial training data is a combination of multilingual QA datasets: XOR- TYDIQA and TYDIQA(Clark et al., 2020), and an English open QA dataset (Natural Questions, Kwiatkowski et al., 2019). Each training instance from these datasets comprises a question, a positive passage, and an answer. Note that annotations in the existing QA datasets have critical limitations: positive passages are taken either from English (Asai et al., 2021) or the question’s language (Clark et al., 2020). Further, most of the non-English languages are not covered. Indeed, when we only train mDPR on this initial set, it often learns to retrieve passages in the same languages or similar languages with irrelevant context or context without sufﬁcient evidence to answer. 2.2.1 Parameter Updates mDPR updates (line 3 in Alg. 1). Let D={⟨qL i, p+ i, p− i,1,⋯, p− i,n⟩}m i=1bemtraining instances. Each instance consists of a question qL i, a passage that answers the question (positive passage) p+ i, andnpassages that do not answer the question (negative passages) p− i,j. For each question, we use positive passages for the other questions in the training batch as negative passages ( in-batch negative , Gillick et al., 2019; Karpukhin et al., 2020). mDPR is updated by minimizing the negative log likelihood of positive passages: i,ep+ i⟩) exp(⟨eqL i,ep+ i⟩)+∑n j=1exp(⟨eqL i,ep− i,j⟩). (2) 1An alternative approach of answer extraction requires translation for all language pairs (Asai et al., 2021). 3 Algorithm 1: Iterative training that automatically mines training data. Data: Input QA pairs: (qL,aL) 1initialize training data }; 2whilet<Tdo 3 Θt mDPR */ ( passages */ 5θt mGEN */ 6 ( data using Wikidata */ 7 ( (pi) )/*Add new training data */ 9t←t+1 10end mGEN updates (lines 4-5 in Alg. 1). After updating mDPR, we use mDPR to retrieve top kpassages Pmultifor each qL. Given these pairs of the question and the retrieved passages (qL,Pmulti)as input, mGEN is trained to generate answer (Eq. (1)) and minimize the cross- entropy loss. To train the model to generate in languages not covered by the original datasets, we translate aLto other languages using Wikipedia language links and create new synthetic answers.2 See Appendix § A.2 for more detail. 2.2.2 Cross-lingual Data Mining After the parameter updates, we mine new training data using mDPR and Wikipedia language links and label the new data by mGEN predictions. This step is skipped in the ﬁnal iteration. Mining by trained mDPR and language links (line 4, 6 in Alg. 1). Trained mDPR can discover positive passages in another language that is not covered by the initial training data. At each iteration, we use retrieved passages 4 in Alg. 1) as a source of new positive and negative passages. This enables expanding data between language pairs not in the original data. To cover even more diverse languages, we use language links and ﬁnd passages in other languages that potentially include sufﬁcient evidence to answer. Wikipedia maintains article-level language links that connect articles on the same entity over languages. We use these links to expand training data from the English QA dataset of Natural Questions (line 6 in Alg. 1). Denote a training instance by (qEn, aEn, pgold). We ﬁrst translate the English answer aEnto a target language aLusing language links. We use language links again to look up the English Wikipedia article that the gold passage pgoldcomes from. We then ﬁnd articles in non-English languages in the reference documents Cmulti that correspond to this article. Although the language link-based automatic translation cannot handle non-entity answers (e.g., short phrases), this helps us to scale to new languages without additional human annotation or machine translation. We add all passages from these articles to Pmultias positive passage candidates, which are then passed to mGEN to evaluate whether each of them leads toaLor not. Automatic labeling by mGEN predictions (lines 7-8 in Alg. 1). A passage not always provide sufﬁcient information to answer the question qLeven when it includes the answer string aL. To ﬁlter out those spurious passages (Lin et al., 2018; Min et al., 2019), we take instances generated from the two mining methods described above, and run mGEN on each passage to predict an answer for the question. If the answer matches the correct answer aL, then the passage piis labeled as a positive passage ; otherwise we label the input passage as a negative passage . We assume that when mGEN fails to generate a correct answer given the passage, the passage may not provide sufﬁcient evidence to answer; this helps us ﬁlter out spurious passages that accidentally contain an answer string yet do not provide any clue to answer. We add these new positive and negative passages to the training data, and in the next iteration, mDPR is trained on this expanded training set (§ 2.2.1). 2This automatic answer translation is only done after the third epoch of initial training to prevent the model from overﬁtting to synthetic data. 4 3 Experiments We evaluate CORA on two multilingual open QA datasets across 28 typologically diverse languages.3 CORA achieves performance across 26 languages, and greatly outperforms previous approaches that use components such as question or answer translation. 3.1 Datasets and Knowledge Sources Multilingual open QA datasets differ in covered languages, annotation schemes, and target application scenarios. We evaluate F1 and EM scores over the questions with answer annotations from two datasets, following the common evaluation practice in open QA (Lee et al., 2019). XOR-TYDIQA. XOR-TYDIQA (Asai et al., 2021) is a multilingual open QA dataset consisting of 7 typologically diverse languages, where questions are originally from TYDIQA(Clark et al., 2020) and posed by native speakers. The answers are annotated by extracting spans from Wikipedia in the same language as the question ( in-language data ) or by translating English spans extracted from English Wikipedia to the target language ( cross-lingual data ).XOR-TYDIQA offers both training and evaluation data. MKQA. MKQA (Longpre et al., 2020) is an evaluation dataset created by translating 10k Natural Questions (Kwiatkowski et al., 2019) to 25 target languages. The parallel data enables us to compare the models’ performance across typologically diverse languages, in contrast to XOR-TYDIQA. MKQA has evaluation data only; X OR-TYDIQA and MKQA have ﬁve languages in common. Collection of multilingual documents Cmulti.We use the February 2019 Wikipedia dumps of 13 diverse languages from all XOR-TYDIQA languages and a subset of MKQA languages.4We choose 13 languages to cover languages with a large number of Wikipedia articles and a variety of both Latin and non-Latin scripts. We extract plain text from Wikipedia articles using split each article into 100-token segments as in DPR (Karpukhin et al., 2020). We ﬁlter out disambiguation pages that distinguish pages that share the same article title6as well as pages with fewer than 20 tokens, resulting in 43.6M passages. See more details in Appendix § B.2. Language categories. To better understand the model performance, we categorize the languages based on their availability during our training. We call the languages with human annotated gold paragraph and answer data seen languages. XOR-TYDIQA provides gold passages for 7 languages. For the languages in Cmultiwithout passages, we mine new mDPR training data by our iterative approach (§ 2.2). We call these languages, which are seen during mDPR training, mDPR-seen . We also synthetically create mGEN training data as explained in § 2.2.1 by simply replacing answer entities with the corresponding ones in the target languages. The languages that are unseen by mDPR but are seen by mGEN are called mGEN-seen , and all other languages (i.e., included neither in mDPR nor mGEN training; 9 of the MKQA languages) unseen languages . 3.2 Baselines and Experimental Setting We compare CORA with the following strong baselines adopted from Asai et al. (2021). Translate-test (MT + DPR). As used in most previous work (e.g., Asai et al., 2021), this method translates a question to English, extracts an answer in English using DPR, and then translates the answer back to the target language. The translation models are obtained from MarianMT (Junczys- Dowmunt et al., 2018) and trained on the OPUS-MT dataset (Tiedemann, 2012). Monolingual baseline (BM25). This baseline retrieves passages solely from the target language and extracts the answer from the retrieved passages. Training neural network models such as DPR is infeasible with a few thousands of training examples. Due to the lack of training data in most of 3A full list of the language families and script types are in the appendix. 4Downloaded from = . . 5 Models Target Language LiF1 Macro Average Ar Bn Fi Ja Ko Ru Te F1 EM BLEU CORA 59.8 40.4 42.2 44.5 27.1 45.9 44.7 43.5 33.5 31.1 SER 32.0 23.1 23.6 14.4 13.6 11.8 22.0 20.1 13.5 20.1 GMT+GS 31.5 19.0 18.3 8.8 20.1 19.8 13.6 18.7 12.1 16.8 MT+Mono 25.1 12.7 20.4 12.9 10.5 15.7 0.8 14.0 10.5 11.4 MT+DPR 7.6 5.9 16.2 9.0 5.3 5.5 0.8 7.2 3.3 6.3 BM25 31.1 21.9 21.4 12.4 12.1 17.7 – – – – Closed-book 14.9 10.0 11.4 22.2 9.4 18.1 10.4 13.8 9.6 7.4 Table 1: Performance on XOR-FULL (test data F1 scores and macro-averaged F1, EM and BLEU scores). “GMT+GS” denotes the previous model, which combines Google Custom Search in the target language and Google Translate + English DPR for cross-lingual retrieval (Asai et al., 2021). Concurrent to our work, “SER” is a model, Single Encoder Retriever, submitted anonymously on July 14 to the ( . edu/xorqa/ ). We were not able to ﬁnd a BM25 implementation that supports Telugu. the target languages, we use a BM25-based lexical retriever implementation by Pyserini (Lin et al., 2021). We then feed the retrieved documents to a multilingual QA model to extract ﬁnal answers. MT+Mono. This baseline combines results from the translate-test method and the monolingual method to retrieve passages in both English and the target language. Following Asai et al. (2021), we prioritize predictions from the monolingual pipeline if they are over a certain threshold tuned on XOR-TYDIQA development set; otherwise we output predictions from the translate-test method.7 Closed-book baseline. This model uses an model that takes a question as input and generates an answer in the target language without any retrieval at inference time (Roberts et al., 2020). This baseline assesses the models’ ability to memorize and retrieve knowledge from its parameters without retrieving reference documents. CORA details. For all experiments, we use a single retriever (mDPR) and a single generator (mGEN) that use the same passage embeddings. mDPR uses multilingual BERT base uncased,9and the generator ﬁne-tunes mT5-base. We found that using other pre-trained language models such as mBART (Liu et al., 2020) for mGEN or XLM-R (Conneau et al., 2020) for mDPR did not improve performance and sometimes even hurt performance. We ﬁrst ﬁne-tune mDPR using gold passages from Natural Questions, and then further ﬁne-tune it using XOR-TYDIQA and TYDIQA’s gold passage data. We exclude the training questions in Natural Questions and TYDIQAthat were used to create the MKQA or XOR-TYDIQA evaluation set. We run two iterations of CORA training (§ 2.2) after the initial ﬁne-tuning. All are in Appendix § B.5. 4 Results and Analysis 4.1 Multilingual Open QA Results XOR-TYDIQA. Table 1 reports the scores of CORA and the baselines in XOR-TYDIQA. CORA, which only uses a single retriever and a single generator, outperforms the baselines and the previous model on XOR-TYDIQA by a large margin across all 7 languages. CORA achieves gains of 24.8 macro-averaged F1 points over the previous method (GMT+GS), which uses external black-box APIs, and 23.4 points over the concurrent anonymous work (SER). MKQA. Tables 2 and 3 report the F1 scores of CORA and the baselines on over 6.7k MKQA questions with short answer seen andunseen settings. CORA signiﬁcantly 7For the languages not supported by Pyserini, we always output predictions. 8We did not use larger-sized variants due to our computational budget. 9The alternative of XLM-RoBERTa (Conneau et al., 2020) did not improve our results. 10Following previous work in open QA but different from the ofﬁcial script of MKQA (Longpre et al., 2020), we disregard the questions labeled as “no answer”. As shown in our human analysis, it is difﬁcult to prove an answer does not exist in the millions of multilingual documents even if the annotation says so. 6 Setting – Seen (Included in X OR-TYDIQA) mDPR-seen Avg. over all L.En Ar Fi Ja Ko Ru Es Sv He Th CORA 21.8 40.6 12.8 26.8 19.7 12.0 19.8 32.0 30.9 15.8 8.5 MT+Mono 14.1 19.3 6.9 17.5 9.0 7.0 10.6 21.3 20.0 8.9 8.3 MT+DPR 17.1 43.3 16.0 21.7 9.6 5.7 17.6 28.4 19.7 8.9 6.9 BM25 – 19.4 5.9 9.9 9.1 6.9 8.1 14.7 10.9 – 4.9 Closed 4.5 8.0 4.6 3.6 6.5 3.8 4.1 6.6 4.8 3.8 2.1 Table 2: F1 scores on MKQA seen and mDPR-seen languages. Setting mGEN-seen Unseen Da De Fr It Nl Pl Pt Hu Vi Ms Km No Tr cn hk tw CORA 30.4 30.2 30.8 29.0 32.1 25.6 28.4 18.4 20.9 27.8 5.8 29.2 22.2 5.2 6.7 5.4 MT+Mono 19.3 21.6 21.9 20.9 21.5 24.6 19.9 16.5 15.1 12.6 1.2 17.4 16.6 4.9 3.8 5.1 MT+DPR 26.2 25.9 21.9 25.1 28.3 24.6 24.7 15.7 15.1 12.6 1.2 18.3 18.2 3.3 3.8 3.8 BM25 9.5 12.5 – 13.6 12.8 – 13.4 7.4 – – – 9.4 8.8 2.8 – 3.3 Closed 4.7 5.6 5.8 5.3 5.5 4.0 4.4 5.5 5.9 5.3 1.9 4.1 3.8 2.6 2.3 2.4 “cn”: “Zh-cn” (Chinese, simpliﬁed). “hk”: “Zh-hk” (Chinese, Hong Kong). “tw”:“Zh-tw” (Chinese, traditional). Table 3: F1 scores on MKQA in mGEN-seen and unseen languages. outperforms the baselines in all languages by a large margin except for Arabic and English. Note that Longpre et al. (2020) report results in a simpliﬁed setting with gold reference articles from the original Natural Questions dataset given in advance, and thus their results are not comparable. CORA yields larger improvements over the translate-test baseline in the languages that are distant from English and with limited training data such as Malay (Ms; 27.8 vs. 12.6) and Hebrew (He; 15.8 vs. 8.9). The performance drop of the translate-test model from English (43.3 F1) to other languages indicates the error propagation from the translation process. BM25 performs very poorly in some low-resource languages such as Thai because of the lack of answer content in the target languages’ Wikipedia. MT+Mono underpeforms the MT+DPR baseline in MKQA since it is challenging to rerank answers from two separate methods with uncaliberated conﬁdence scores. In contrast, CORA retrieves passages across languages, achieving around 30 F1 on a majority of the 26 languages. 4.2 Analysis Setting XOR-TYDIQA MKQA Avg. F1 Ar Ja Te Avg. F1 Fi Ru Es Th Vi CORA 31.4 42.6 33.4 26.1 22.3 25.9 20.6 33.2 6.3 22.6 (i) mDPR 1+ mGEN 1 27.9 36.2 29.8 21.1 17.3 23.1 13.1 28.5 5.7 18.6 (ii) DPR (trained NQ)+mGEN 24.3 30.7 29.2 19.0 17.9 20.1 16.9 29.4 5.5 18.2 (iii) CORA, Cmulti={En} 19.1 20.5 23.2 11.5 20.5 24.7 15.4 28.3 8.3 21.9 (iv) 11.2 11.8 10.8 5.6 12.2 16.1 10.9 25.2 1.2 12.7 Table 4: Ablation studies on X OR-TYDIQA development set and a subset of MKQA. Ablations: Impact of CORA components. We compare CORA with the following four variants to study the impact of different components. (i) mDPR 1+ mGEN 1only trains CORA using the initial labeled, annotated data and measures the impact of the iterative training. (ii) DPR (trained NQ) + mGEN replaces mDPR with a multilingual BERT-based DPR trained on English data from Natural Questions (NQ), and encodes all passages in Cmulti. This conﬁguration assesses the impact of cross-lingual training data. (iii) CORA, Cmulti={En} only retrieves from English during inference. This variant evaluates if English reference documents sufﬁce to answer multilingual questions. (iv) replaces mGEN with an extractive reader model (Karpukhin et al., 2020) followed by answer translation. This variant quantiﬁes the effectiveness of using a multilingual generation model over the approach that combines an extractive reader model with translation models. Note that for MKQA experiments, we sample the same 350 questions ( ∼5%) from the evaluation set for each language to reduce the computational cost over varying conﬁgurations. 7 Figure 3: Breakdown of the languages of retrieved reference pas- sages for sampled MKQA questions (%). The x and y axes indicate target (question) and retrieval reference languages Es retrieval errors 28 48 different lang 18 0 incorrect answer 22 36 annotation error 22 12 underspeciﬁed q 10 4 Table 6: Error categories (%) on 50 errors sampled from Japanese (Ja) and Spanish (Es) data. Results in Table 4 show performance drops in all variants. This supports the following claims: (i) the iterative learning and data mining process is useful, (ii) mDPR trained with cross-lingual data substantially outperforms DPR with multilingual BERT trained on monolingual data only, (iii) reference languages other than English are important in answering multilingual questions, and (iv) a multilingual generation model substantially boosts the model performance. Setting mDPR-Seen | Unseen Lang Es Fi Ja Ru Th Pt Ms Tr Zh-Cn Zh-Hk Km Script Latn | Jpan | Cyrl | Thai Latn | Hant | Khmr mDPRRL@10 53.7 52.8 32.9 42.3 14.9 50.0 49.4 42.0 12.6 16.6 15.7 Rmulti@10 63.4 60.9 42.0 54.0 28.0 62.6 63.4 55.4 40.6 42.3 25.1 DPR(NQ) RL@10 52.3 46.0 24.6 36.0 12.6 45.7 48.8 32.0 9.1 14.0 13.4 Rmulti@10 63.1 53.1 32.9 49.1 29.4 56.8 58.0 44.0 36.3 39.4 23.4 Table 5: Retrieval recall performance on MKQA as the percentage of the questions where at least one out of the top 10 passages includes an answer string in the target language ( RL@10), or in any language ( Rmulti@10). The same subset of the MKQA evaluation data are used as in the ablations. Retrieval performance and relationship to the ﬁnal QA performance. We evaluate CORA’s retrieval performance on MKQA using two recall metrics that measure the percentage of questions with at least one passage among the top 10that includes a string in an answer set in the target language (RL@10) or in the union of answer sets from all languages that are available in MKQA ( Rmulti@10). MKQA provides answer translations across 26 languages. Table 5 reports retrieval results for mDPR and multilingual BERT-based DPR trained on NQ: DPR (NQ) . This is equivalent to (ii) from the ablations. We observe that mDPR performs well in Indo- European languages with Latin script, even when the language is unseen. Interestingly, there is a signiﬁcant performance gap between RL@10 andRmulti@10 in languages with non-Latin script (e.g., Japanese, Russian, Chinese); this suggests that our model often uses relevant passages from other languages with Latin script such as English or Spanish to answer questions in those languages with non-Latin script. Our mDPR outperforms DPR (NQ) by a large margin in unseen languages with limited resources, which are consistent with the ﬁndings in Table 3. Nevertheless, we still see low performance on Khmer and Thai even with the Rmulti@10 metric. We also observe that passage and query embeddings for those languages are far from other languages, which can be further studied in future work. We provide a visualization of the encoded passage in the appendix. Breakdown of reference languages. Fig. 3 breaks down retrieved reference languages for each target language. Our multilingual retrieval model often retrieves documents from the target language (if its reference documents are available), English, or its typologically similar languages. For example, mDPR often retrieves Spanish passages for Portuguese questions and Japanese passages for Chinese questions; while they are considered distant, Japanese and Chinese overlap in script. To further evaluate this, we conduct a controlled experiment: we remove Spanish, Swedish and Indonesian document embeddings and evaluate CORA on related languages: Danish, Portuguese and 8 Example1: French Question: qFr: Quand est-ce que The life of pablo est sorti? (When was The life of pablo released?) aFr: 14 février 2016 The Life of Pablo – ru.wikipedia The Life of Pablo'' был выпущен 14 февраля 2016 года (The Life of Pablo '' was released on February 14, 2016 ) The Life of Pablo – sv.wikipedia The Life of Pablo … var planerat att släppas 11 februari 2016 … släpptes slutligen 14 februari 2016 (The Life of Pablo … was scheduled for release on February 11, 2016 … was finally released on February 14, 2016 ) Pred: 14 февраля 2016 ❌ Example3: Japanese Question qJa: 3 が 公開されたのはいつですか (when does the third season of rick and morty come out) aJa: No Answer in MKQA リック・アンド・モーティー – ja.wikipedia リック・アンド・モーティーの シーズン 3 は2017 年4 月1 日 から放送 されている。 (Rick and Morty's Season 3 has been airing since April 1, 2017 ) Pred: 2017 年4 月1 日 ✔ Example2: Norweigian Question qNo: hvem spiller black panther i filmen black panther (who plays black panther in the movie black panther) aNo: Chadwick Boseman Pantera Negra (película) – es.wikipedia es protagonizada por Chadwick Boseman como T'Challa / Black Panther (it stars Chadwick Boseman as T'Challa / Black Panther) Black Panther (film) – sv.wikipedia Huvudrollen som Black Panther spelas av Chadwick Boseman (The main role as Black Panther is played by Chadwick Boseman ) Pred: Chadwick Boseman ✔mDPR mGEN Figure 4: Cross-lingual retrieval and generation examples for three MKQA questions. Malay. We observe performance drops of 1.0 in Danish, 0.6 in Portuguese, and 3.4 F1 points in Malay. This illustrates that while CORA allows for retrieval from any language in principle ( many-to-many ), cross-lingual retrieval from closer languages with more language resources is particularly helpful. Error analysis and qualitative examples. Table 6 analyzes errors from CORA by manually inspecting 50 Japanese and Spanish wrong predictions from MKQA. We observe six major error categories: (a) retrieval errors, (b) generating correct answers in a different language (different lang), (c) incorrect answer generation (incorrect answer), (d) answer annotation errors (e.g., a correct alias isn’t covered by gold answers, or Wikipedia information is inconsistent with English.), and (e) ambiguous or underspeciﬁed questions such as “who won X this year ” (underspeciﬁed q). The table shows that both in Japanese and Spanish, the retrieval errors are dominant. In Japanese, CORA often generates correct answers in English, not in Japanese (different lang). Fig. 4 shows some qualitative examples. The ﬁrst example shows an error in (b): mGEN is generating an answer in Russian, not in French though the answer itself is correct. This type of error happens especially when retrieved passages are in languages other than the target and English. Human evaluation on cross-lingual retrieval results. To observe how cross-lingual retrieval between distant languages is actually helping, we sample 25 Norwegian questions for which Spanish passages are included among the top 10 retrieved results. As seen in Fig. 3, CORA retrieves Spanish (es) passages for 6.8% of the Norwegian (no) questions. A Spanish speaker judges if the retrieved Spanish passages actually answer the given Norwegian questions.11We found that in 96% of the cases, the retrieved Spanish passages are relevant in answering the question. One such example is presented in Fig. 4 (the second example). Human analysis on unanswerable questions. CORA retrieves passages from a larger multilingual document collection than the original human annotations. Thus, CORA may further improve the answer coverage over the original human annotations. MKQA includes questions that are marked as unanswerable by native English speakers given English knowledge sources. We sample 400 unanswerable Japanese questions whose top one retrieved passage is from a non-English Wikipedia article. Among these, 329 unanswerable questions are underspeciﬁed (also discussed in Asai and Choi, 2021). For 17 out of the 71 remaining questions, the answers predicted by CORA are correct. This ﬁnding indicates the signiﬁcance of cross-lingual retrieval and potential room for improvement in annotating multilingual open QA datasets. The third example in Fig. 4 shows one of these cases. 5 Related Work and Broader Impacts English and non-English open QA. Despite the rapid progress in open QA (Chen et al., 2017; Karpukhin et al., 2020), most prior work has been exclusively on English (Lewis et al., 2020; Izacard and Grave, 2021b). Several prior attempts to build multilingual open QA systems often rely on machine translation or retrieval models (Ture and Boschee, 2016; Asai et al., 2021). 11During evaluation, we provide the original English questions from MKQA. 9 Lewis et al. (2020) and Guu et al. (2020) introduce approaches. Izacard and Grave (2021a) introduce an iterative training framework that uses attention weights from a generator model as a proxy for text relevance scores. Tran et al. (2020) introduce CRISS, a pre-training approach consisting of a parallel sentence mining module and a model, which are trained iteratively. Several recent work such as Xiong et al. (2021) improves DPR by mining and learning with hard examples. Our work is the ﬁrst work that introduces a uniﬁed multilingual system for many-to-many open QA, which is a challenging task requiring massive-scale cross-lingual retrieval and has not been addressed in prior work. We introduce an iterative training and data mining approach guided by ﬁltering from an answer generation model to automatically extend annotated data available only in high-resource languages to low-resource. This approach contributes to signiﬁcant performance improvements in languages without annotated training data. models. Several recent work introduces single multilingual models for many languages using pre-trained multilingual models such as mBERT or mT5 in many NLP tasks (e.g., entity linking: Botha et al., 2020; De Cao et al., 2021; semantic role labeling: Mulcaire et al., 2019b; Lyu et al., 2019; Fei et al., 2020; syntactic parsing: Mulcaire et al., 2019a; Kondratyuk and Straka, 2019). This work conducts the ﬁrst large-scale study of a uniﬁed multilingual open QA model across many languages and achieves performance in 26 typologically diverse languages. Synthetic data creation for machine reading comprehension. Alberti et al. (2019) introduce a method of generating synthetic machine reading comprehension data by automatically generating questions and ﬁltering them out by a trained machine reading comprehension model. Several studies augment multilingual machine reading comprehension training data by generating new question- answer pairs from randomly sampled non-English Wikipedia paragraphs (Riabi et al., 2021; Shakeri et al., 2020). This work focuses on multilingual open QA, which involves not only machine reading comprehension but also cross-lingual retrieval. A similar augmentation method for machine reading comprehension can be applied to further improve the answer generation component in CORA. Societal impacts. Our code and data are publicly available. CORA can perform open QA in unseen languages and can beneﬁt society in building QA systems for low-resource languages, hence enabling research in that direction. Unlike previous models, CORA removes the necessity of external black-box APIs, and thus we can examine and address wrong answers due to model errors or misinformation present on Wikipedia. This would help us mitigate the potential negative impact from CORA or its subsequent models outputting a wrong answer when it is used by people who seek information. 6 Conclusion To address the information needs of many non-English speakers, a QA system has to conduct cross- lingual passage retrieval and answer generation. This work presents CORA, a uniﬁed multilingual many-to-many open QA model that retrieves multilingual passages in many different languages and generates answers in target languages. CORA does not require translation or retrieval components and can even answer questions in unseen, new languages. We conduct extensive experiments on two multilingual open QA datasets across 28 languages, 26 of which CORA advances the state of the art on, outperforming competitive models by up to 23 F1 points. Our extensive analysis and manual evaluation reveal that CORA effectively retrieves semantically relevant passages beyond language boundaries, and can even ﬁnd answers to the questions that were previously considered unanswerable due to lack of sufﬁcient evidence in annotation languages (e.g., English). Nonetheless, our experimental results show that the retrieval component still struggles to ﬁnd relevant passages for queries in some unseen languages. Our analysis also showed that CORA sometimes fails to generate an answer in the target language. In future work, we aim to address these issues to further improve the performance and scale our framework to even more languages. This research was supported by NSF IIS-2044660, ONR , gifts from Google, the Allen Distinguished Investigator Award, the Sloan Fellowship, and the Nakajima Foundation Fellowship. We thank anonymous reviewers, area chairs, Eunsol Choi, Sewon Min, David Wadden, and the members of the UW NLP group for their insightful feedback on this paper, and Gabriel Ilharco for his help on human analysis. 10 References Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. 2019. Synthetic QA corpora generation with roundtrip consistency. In ACL. Akari Asai and Eunsol Choi. 2021. Challenges in information seeking QA: Unanswerable questions and paragraph retrieval. In ACL. Akari Asai, Jungo Kasai, Jonathan H. Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. 2021. XOR QA: Cross-lingual open-retrieval question answering. In NAACL . Jan A. Botha, Zifei Shan, and Daniel Gillick. 2020. Entity Linking in 100 Languages. In EMNLP . Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open-domain questions. In ACL. Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for question answering in typologically diverse languages. TACL . Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In ACL. Nicola De Cao, Ledell Wu, Kashyap Popat, Mikel Artetxe, Naman Goyal, Mikhail Plekhanov, Luke Zettlemoyer, Nicola Cancedda, Sebastian Riedel, and Fabio Petroni. 2021. Multilingual autoregressive entity linking. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL . Hao Fei, Meishan Zhang, and Donghong Ji. 2020. Cross-lingual semantic role labeling with high- quality translated training corpus. In ACL. Daniel Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta, Jason Baldridge, Eugene Ie, and Diego Garcia-Olano. 2019. Learning dense for entity retrieval. In CoNLL . Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: language model pre-training. In ICML . Gautier Izacard and Edouard Grave. 2021a. Distilling knowledge from reader to retriever for question answering. In ICLR . Gautier Izacard and Edouard Grave. 2021b. Leveraging passage retrieval with generative models for open domain question answering. In EACL . Marcin , Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, André F. T. Martins, and Alexandra Birch. 2018. Marian: Fast neural machine translation in C++. In ACL (System Demonstrations) . Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In EMNLP . Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR . Dan Kondratyuk and Milan Straka. 2019. 75 languages, 1 model: Parsing Universal Dependencies universally. In EMNLP . Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A benchmark for question answering research. TACL . Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In ACL. 11 Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. generation for NLP tasks. In NeurIPS . Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: An easy-to-use python toolkit to support replicable ir research with sparse and dense . Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2020. Pretrained transformers for text ranking: BERT and beyond. Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. 2018. Denoising distantly supervised open-domain question answering. In ACL. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. TACL . Shayne Longpre, Yi Lu, and Joachim Daiber. 2020. MKQA: A linguistically diverse benchmark for multilingual open domain question answering. Chunchuan Lyu, Shay B. Cohen, and Ivan Titov. 2019. Semantic role labeling with iterative structure reﬁnement. In EMNLP . Sewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi Chen, Eunsol Choi, Michael Collins, Kelvin Guu, Hannaneh Hajishirzi, Kenton Lee, Jennimaria Palomaki, Colin Raffel, Adam Roberts, and Tom et al Kwiatkowski. 2021. NeurIPS 2020 EfﬁcientQA competition: Systems, analyses and lessons learned. In PMLR . Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. A discrete hard em approach for weakly supervised question answering. In EMNLP . Phoebe Mulcaire, Jungo Kasai, and Noah A. Smith. 2019a. Low-resource parsing with crosslingual contextualized . In CoNLL . Phoebe Mulcaire, Jungo Kasai, and Noah A. Smith. 2019b. Polyglot contextual improve crosslingual transfer. In NAACL . Arij Riabi, Thomas Scialom, Rachel Keraron, Benoît Sagot, Djamé Seddah, and Jacopo Staiano. 2021. Synthetic data augmentation for zero-shot cross-lingual question answering. In EMNLP . Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In EMNLP . Siamak Shakeri, Noah Constant, Mihir Sanjay Kale, and Linting Xue. 2020. Towards zero-shot multilingual synthetic question and answer generation for cross-lingual reading comprehension. Saku Sugawara, Kentaro Inui, Satoshi Sekine, and Akiko Aizawa. 2018. What makes reading comprehension questions easier? In EMNLP . Jörg Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In LREC . Jörg Tiedemann and Lars Nygaard. 2004. The OPUS corpus - parallel and free. In LREC . Jörg Tiedemann and Santhosh Thottingal. 2020. OPUS-MT — Building open translation services for the World. In EAMT . Chau Tran, Yuqing Tang, Xian Li, and Jiatao Gu. 2020. Cross-lingual retrieval for iterative self- supervised training. In NeurIPS . Ferhan Ture and Elizabeth Boschee. 2016. Learning to translate for multilingual question answering. InEMNLP . 12 Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: natural language processing. In EMNLP (System Demonstrations) . Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In ICLR . Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. InNAACL . 13 Checklist 1. For all authors... (a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] See Section § 4. (b) Did you describe the limitations of your work? [Yes] See Section § 4. (c)Did you discuss any potential negative societal impacts of your work? [Yes] See Section § 5. (d)Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a)Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] Our code and trained model are publicly available at . (b)Did you specify all the training details (e.g., data splits, , how they were chosen)? [Yes] All will be included in appendix. Data splits are from the original datasets. (c)Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [N/A] (d)Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] That information is included in Appendix. 4. If you are using existing assets (e.g., code, data, models) or new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section § 3. (b)Did you mention the license of the assets? [Yes] Both two datasets are under MIT license and we discuss that information in Appendix. (c)Did you include any new assets either in the supplemental material or as a URL? [Yes] Our code and trained model are publicly available at / AkariAsai/CORA . (d)Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [Yes] See Section § 4 (e)Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [Yes] As the dataset used in this work is originally created for general purpose QA and we believe that those datasets contain few examples that risk our participants. We will describe that in details in Appendix. 5. If you used crowdsourcing or conducted research with human subjects... (a)Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] We will include the instructions in Appendix. (b)Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes] As the dataset used in this work is originally created for general purpose QA and we believe that those datasets contain few examples that risk our participants. We will describe that in details in Appendix. (c)Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14 
revenue maximization via machine learning with noisy data 	Revenue maximization via machine learning with noisy data Ellen Vitercik Department of Electrical Engineering and Computer Sciences University of California, Berkeley Tom Yan Department of Machine Learning Carnegie Mellon University tyyan@cmu.edu Abstract Increasingly, copious amounts of consumer data are used to learn high-revenue mechanisms via machine learning. Existing research on mechanism design via machine learning assumes that there is a distribution over the buyers’ values for the items for sale and that the learning algorithm’s input is a training set sampled from this distribution. This setup makes the strong assumption that no noise is introduced during data collection. In order to help place mechanism design via machine learning on ﬁrm foundations, we investigate the extent to which this learning process is robust to noise. Optimizing revenue using noisy data is challenging because revenue functions are extremely volatile: an inﬁnitesimal change in the buyers’ values can cause a steep drop in revenue. Nonetheless, we provide guarantees when arbitrarily correlated noise is added to the training set; we only require that the noise has bounded magnitude or is sub-Gaussian. We conclude with an application of our guarantees to multi-task mechanism design, where there are multiple distributions over buyers’ values and the goal is to learn a high-revenue mechanism per distribution. To our knowledge, we are the ﬁrst to study mechanism design via machine learning with noisy data as well as multi-task mechanism design. 1 Introduction Revenue maximization in multi-item settings is one of the most important, long-standing open problems in mechanism design. In Bayesian Mechanism Design, there is a set of items for sale and an underlying distribution deﬁning a set of agents’ values for the items. A mechanism determines which buyers receive which items and what they pay. For decades, research in economics has assumed that the mechanism designer must know the exact distribution over buyers’ values. An explosion of research [e.g., ] has relaxed this strong assumption: instead, the distribution is unknown and the mechanism designer only has a training set of i.i.d. samples. Using the training set, the goal is to learn a mechanism with high expected revenue. Learning-based mechanism design is on the verge of taking over as the main tool for designing high-revenue mechanisms for selling items—a cornerstone of many modern enterprises. Motivated by recent literature on the brittleness of deep learning models in the face of imperceptible noise [ 32,41,52], an important question is whether adversarial noise has the same effect in learning- based mechanism design. Learning methods deployed in real-world settings to design mechanisms 35th Conference on Neural Information Processing Systems (NeurIPS 2021). Figure 1: We illustrate revenue function volatility using the single-item second-price auction with a reserver. Letv(1)be the highest bid and v(2)be the second-highest bid. Revenue as a function of the reserve ), and 0ifr>v (1), as illustrated by the blue solid line. Suppose we receive a noisy sample with highest bid w(1)>v(1)and second-highest bidw(2)=v(2). Revenue is illustrated by the dotted grey line. Setting the reserve equal to w(1) maximizes revenue on the noisy sample, but leads to zero revenue on the true values (v(1),v(2)). must be robust to minute noise in the data. We provide guarantees for learning mechanisms with high expected revenue in the face of an adversary that can add arbitrarily correlated noise to the training set; we only require that the noise has bounded magnitude or is sub-Gaussian. In contrast, classic results typically rely on the stronger assumption that the training samples are independent. 1.1 Summary of main contributions and overview of techniques We set out to determine whether imperceptible adversarial noise in the training set can cause a catastrophic loss in the learned mechanism’s expected revenue. Our main contribution is a sensitivity analysis of revenue with respect to the noise’s magnitude, which answers this question in the negative. Our results apply to empirical revenue maximization , the canonical approach to learning-based mechanism design. To our knowledge, we are the ﬁrst to study learning-based mechanism design under adversarial noise. We provide guarantees for optimizing revenue over several classic mechanism classes: second-price auctions with non-anonymous reserves under additive buyers, single-priced lottery mechanisms under unit-demand buyers, and item-pricing mechanisms under unit-demand buyers. These classes have been studied extensively [e.g., 6,18,24,46,51] and can be viewed as hypothesis classes , just as DNNs correspond to a particular (brittle) type of hypothesis class. In some settings, mechanisms from these classes have been shown to provide approximately optimal revenue [22, 23, 35, 39]. The key challenge we face is that revenue functions are extremely sensitive to small perturbations of the buyers’ values. In a second-price auction, for example, slightly shifting the highest bid from below to above the reserve price can cause an arbitrarily large drop in revenue, as illustrated in Figure 1. The set of bidders whose bids are above their reserve prices can completely change when even an inﬁnitesimal amount of noise is added, radically altering the analytical form of the revenue function. This is unlike most functions that we understand well from a perspective, which generally are smooth, continuous, or—more a connection between parameters and output value. Despite the volatility of these revenue functions, we nonetheless are able to prove bounds on the revenue loss incurred by optimizing over a noisy training set. Our second main contribution is an application of our guarantees to multi-task mechanism design . To our knowledge, we are the ﬁrst to study this problem. The existing literature on mechanism design via machine learning assumes that there is a single distribution deﬁning the buyers’ values. Often, however, the mechanism designer may be interested in designing high-revenue mechanisms for multiple related distributions. Each distribution thus deﬁnes a distinct learning task. The goal is to leverage the similarity between the distributions to learn a high-revenue mechanism per distribution. Multi-task learning has proven useful in ﬁelds such as Computer Vision and Natural Language Processing [ 53], and we demonstrate its value in mechanism design as well. Our main technical insight is that we can transform training instances from one task into slightly noisy training instances for another task and make use of our guarantees under noised samples. 2 1.2 Related research Adversarial Machine Learning: A prominent line of research on machine learning in the presence of noise studies the case where some fraction of the training data may be noisy (data poisoning). Research in this vein includes settings where every instance can be corrupted with probability η<1[e.g., 3] and where an η-fraction of the training data can be corrupted [e.g., 16,27]. Meanwhile, in our setting, every sample may be adversarially perturbed with probability 1, not just an η-fraction, though we require that the perturbation be bounded. Another line of research, especially prominent in Computer Vision, studies the case where the test data can be adversarially perturbed within, for example, an ℓp-ball [ 32,41,52]. We, on the other hand, are concerned with noisy training data and face the unique challenges imposed by mechanism design. Mechanism Design with Noised Distributions: While the robustness of mechanism design in the face of model uncertainty has been well studied (see for instance work by Bergemann and Morris [14] and Bergemann and Schlag [15]), to our knowledge, there has been relatively less work done on mechanism design via machine learning in the particular setting where samples are not necessarily drawn i.i.d. from the true distribution. The paper most closely related to ours is that by Cai and Daskalakis [20], who show that given access to a noisy distribution, it is possible to learn a mechanism with high expected revenue over the true distribution if the Kolmogorov distance between the noisy and true distributions are small. One might hope that the uniform distribution over the noisy training data could constitute the noisy distribution, but the Kolmogorov distance between this empirical distribution and the true distribution could be as large as 1, so these results do not apply to our setting. Moreover, we note that Cai and Daskalakis [20] makes the stronger assumption that the buyers’ values follow a product distribution, whereas our assumed noise model allows for arbitrarily correlated distributions. Multi- vs Single-item Mechanism Design: Further aﬁeld, Huang et al. [38] and Guo et al. [33] have also studied mechanism design under noisy settings, albeit under single-item settings. By contrast, we study a wider and different set of multi-item mechanism classes including multi-item lotteries and multi-item item-pricing mechanisms. The past few decades of research on multi-item mechanism design has demon- strated that results and intuition from the single-item setting do not always carry over to the multi-item setting. And we see that this is the case in our paper as well. For example, an item-pricing mecha- nism for a single item would have a stability analysis (similar to the analysis of a single-item anonymous second-price auction that we include for intuition in Appendix B.1). With multiple items and unit-demand buyers, however, stability does not even hold. Therefore, intuition from the single-item case does not carry over to the multi-item case, and we must use completely different analysis techniques. Dispersion: The technique needed is that of dispersion (section 3.4). Our bounds improve based on how “nice” the distribution over buyers’ values is, quantiﬁed by dispersion [ 5]. Dispersion has primarily been used to provide regret bounds [ 5,8]. Balcan et al. [7]also use dispersion in mechanism design, though for a very different problem: estimating how much utility an agent can gain by misreporting their value in a manipulable mechanism. The appendix summarizes additional related work on mechanism design with side information, multi-task learning, transfer learning, and approximate incentive compatibility. 2 Notation There arenbuyers andmitems for sale. Each buyer i∈[n]has a value for each item j∈[m], denoted asvij≥0. We analyze unit-demand and additive buyers. If buyer iis unit-demand, he is only interested in obtaining one item, so his value for a bundle b⊆[m]of goods is equal to the maximum value he has for any item in b,maxj∈bvij. If buyeriis additive, his value for a bundle b⊆[m]is∑ j∈bvij.We use the notation vi= (vi1,...,vim)to denote buyer i’s values for all m items andv= ( denote all nbuyers’ values. When there is only one item, we use the notation v= (v1,...,vn)∈Rn, wherevi∈Ris buyeri’s value for the item. We study mechanism classes parameterized by vectors r∈Rdfor somed(for example, the class of second-price auctions parameterized by non-anonymous reserves r). The mechanisms we analyze 3 are incentive compatible, so we assume the bids equal the buyers’ true values. We denote the revenue of the mechanism deﬁned by rwhen the buyers’ values equal vbyrevr(v). For simplicity of notation, we assume revr(v)is in[0,1]though our results can be extended to the case where rev r(v)∈[0,H]for someH(all bounds must simply be multiplied by H). 3 Learning under adversarial noise In this section, we study mechanism design via machine learning when the data is corrupted. Let S={ v(1),...,v(L)} ⊂Rnm ≥0be a set of valuation vectors drawn from an unknown distribution D. Our learning algorithm receives a poisoned training set S′={ w(1),...,w(L)} ⊂Rnm ≥0. Bounded Noise Model: To model the imperceptible noise that corrupts the bids, we assume the bounded noise model that is commonly assumed in adversarial defense literature. That is, for some knownϵ>0, all samples ℓ∈[L], all bidders i∈[n], and all items j∈[m],w(ℓ) ij∈[ v(ℓ) ij−ϵ,v(ℓ) ij] , or more succinctly, ). (If we only know thatv(ℓ)−w(ℓ) ∞≤ϵ, we can shift all bids in each vector w(ℓ)down byϵ, in which case ).). Our goal is to use the noisy setS′to learn a mechanism with high expected revenue over D. We provide some more motivating factors for our choice of the noise model in the context of mechanism design. Besides its prevalence in adversarial learning literature, (1) Such noise may arise when one is using estimated bids to learn auction parameters. We give a concrete example in Section 4 where we study multi-task mechanism design. (2) The noise may result from bounded rationality on the part of the bidders, leading to small, differences between the buyers’ true values and reported values. (3) Lastly, we note that our results can immediately be extended to cover unbounded standard noise models such as sub-Gaussian noise. In this case, suppose that each element of each vector v(i)∈S is perturbed by subG (σ2)to obtain the noisy vectorw(i). Then with probability 1−δ, for √ 2 log2Lnm δ. The noise need not be independent. Therefore, all of our results hold with (high) probability 1−2δover the draw of the true values Sand the noise for ϵ= 2σ√ 2 log2Lnm δ. Our Approach: We begin by ﬁxing a mechanism class parameterized by vectors r∈Rdfor some d(for example, the class of second-price auctions parameterized by non-anonymous reserves r). Given the training set S′, the most widely used learning algorithm is empirical revenue maximization (ERM) , which returns the parameter setting ˆr′that maximizes average empirical revenue over S′. This algorithm will be the subject of our study. Throughout this section, we analyze the following key question: what is the difference between the expected revenue of the mechanism deﬁned by ˆr′, Ev∼D[revˆr′(v)], and that of the optimal mechanism in the class, max r∈RdE[revr(v)]? Another key aspect of learning besides optimality is sample complexity. In machine learning beyond the context of mechanism design, prior research [e.g., 49] has shown that a large increase in sample complexity is needed to handle noise with bounded ℓ∞-norm. By contrast, under the same noise assumption, we show that empirical revenue maximization can achieve near-optimal revenue without signiﬁcantly higher sample complexity . This exposes a notable contrast between the two learning tasks. We provide lower bounds showing that ERM’s loss has an optimal dependence on the noise ϵ. 3.1 Robustness We begin by providing guarantees for any mechanism class that satisﬁes a notion of robustness, which helps us isolate exactly the properties we need to prove that robust revenue optimization is possible. A class is robust if it satisﬁes two properties. The ﬁrst is a stability property. Let ˆrbe the empirically optimal parameter setting over the set Sof true samples, which means that of all r∈Rd, average revenue overSis maximized when r=ˆr. Let ˆr′be the empirically optimal parameter vector over the noisy setS′. Stability is satisﬁed when the average revenues of close. Our second property relies on a classic notion of convergence. It is satisﬁed when, for every parameter vector r, average revenue over the set Sis close to expected revenue. Although convergence bounds have been derived in prior research for the mechanism classes we analyze [ 6,46], a convergence bound alone does not imply any guarantees whatsoever for optimization with noisy 4 Mechanism class Buyers’ valuesp-stable Error bound Single-item anonymous second-price p(ϵ,n,m ) = 2ϵ (Theorem B.2)˜O(ϵ+√ 1/L) (Corollary B.6) Multi-item non-anonymous second-price p(ϵ,n,m ) = 2mϵ (Theorem 3.3)˜O(mϵ+√ nm/L ) (Corollary 3.7) Multi-item non-anonymous lotteriesUnit- demandp(ϵ,n,m ) =nϵ (Theorem 3.9)˜O(nϵ+√ nm/L ) (Corollary 3.10) Item-pricing mechanisms Unit- demandDoes not satisfy (Lemma B.11)˜O(nm2(κϵ+√ 1/L))* (Theorem 3.13, Lemma 3.15) *The distribution over buyers’ values is κ-bounded (Deﬁnition 3.14). Table 1: Our p-stability guarantees together with the resulting error bounds. Item-pricing mechanisms do not satisfy p-stability, so we use alternative techniques to provide an error bound (Section 3.4). data. This is why we introduce the separate notion of stability, which has not been previously studied in the automated mechanism design literature. The challenge then lies in proving that a variety of mechanism classes satisfy stability. To begin, we deﬁne the two properties formally as follows: Deﬁnition 3.1. Given two functions p: [0,1]×Z2→Randq: [0,1]×Z3→R, we say that a mechanism class is p-stable if the following conditions hold: 1.p-stable. For anyL≥1, letS={ v(1),...,v(L)} ⊂Rnm ≥0andS′={ w(1),...,w(L)} ⊂Rnm ≥0 be two arbitrary sets of vectors such that allℓ∈[L]. Let ˆrandˆr′be the empirically optimal parameter vectors over ℓ=1revr( v(ℓ)) and ℓ=1revr( w(ℓ)) . We require that on average over S, the revenues of ˆrandˆr′ are close:1 L∑L ℓ=1revˆr( v(ℓ)) −revˆr′( v(ℓ)) ≤p(ϵ,n,m ). 2.q-convergent. For ), with probability 1−δover the drawS∼DL, for every vectorr∈Rd, the difference between the average revenue over Sand the expected revenue is at mostq(δ,L,n,m ). In other words,⏐⏐1 L∑ ). If a mechanism class is p-stable , then the expected revenue of the empirically optimal mechanism over the noisy samples S′is close to the expected revenue of the optimal mechanism in the class. For completeness, the proof is in Appendix B. Fact 3.2. Fix ap-stable mechanism class. Let S={ v(1),...,v(L)} andS′={ w(1),...,w(L)} be two sets of valuation vectors such that for all ). Letˆr′be empirically optimal over ℓ=1revr( w(ℓ)) .With probability 1−δ overS∼DL,max ) + 2q(δ,L,n,m ). We now prove that several mechanism classes are stable: second-price auctions with non-anonymous reserves and lotteries. (For intuition, we also analyze the simpler class of second-price auctions with anonymous reserves in Appendix B.1.) Table 1 summarizes our results. 3.2 Non-anonymous second-price auctions We prove that second-price auctions with non-anonymous reserves and additive bidders are robust. In the single-item setting, this auction is deﬁned by a vector r∈Rn, whereriis price . Each bidder submits a bid and the mechanism discards all bidders whose bids are smaller than their reserves. If there are bidders remaining, the highest bidder, say bidder i, wins and pays the maximum of the second-highest remaining bid and ri(orriif there are no other remaining bids). In the multi-item case, there is only one copy of each item and there is a separate auction per item. The mechanism is deﬁned by r= (r1,...,rm)∈Rnm, whererj∈Rnis the reserve vector for item j. 5 We begin by analyzing single-item auctions, which then implies guarantees for multiple items. The key challenge is that the set of bidders whose bids are above their reserves can completely change when even inﬁnitesimal noise is added, radically altering the analytical form of the revenue function. Theorem 3.3. Single-item non-anonymous second-price auctions are p-stable with p(ϵ,n,m ) = 2ϵ. Proof. For anyL≥1, letS={ v(1),...,v(L)} ⊂Rn ≥0andS′={ w(1),...,w(L)} ⊂Rn ≥0 be two sets of valuation vectors such that allℓ∈[L]. Let ˆr′be the empirically optimal reserve vector over S′and let ˆrbe empirically optimal over S. This proof relies on two key lemmas. The ﬁrst states that if we shift the reserve vector ˆr—which is empirically optimal overS—down by an additive factor of ϵ= (ϵ,...,ϵ )∈Rnand apply it to the valuations in S′, little revenue is lost. We use the standard notation ⟨x⟩= max{x,0}. The full proof is in Appendix B.2. Lemma 3.4. Letrϵ= (⟨( v(ℓ)) >0, then revrϵ( w(ℓ)) ≥ revˆr( v(ℓ)) −2ϵ. Proof sketch of Lemma 3.4. Suppose that revˆr( v(ℓ)) >0. Letibe the winner under the values v(ℓ) and reserve ˆr. Sincev(ℓ) i≥ˆri, we know that w(ℓ) i≥⟨v(ℓ) i−ϵ⟩≥⟨ ˆri−ϵ⟩,so under the values w(ℓ) and is at least one bidder whose bid is at least his reserve. Let i′be the winner under the valuation vector w(ℓ)and reserverϵ. We split this proof into four cases that depend on: 1. Whether or not i=i′, and is another bidder besides iwhose bid is at least his reserve (in other words, whether or not {t:v(ℓ) t≥ˆrt,t̸=i}=∅). In the ﬁrst case, i=i′and{t:v(ℓ) t≥ˆrt,t̸=i}̸=∅. Letkbe the index of the second-highest bidder inv(ℓ)whose bid is above his reserve in ) t:v(ℓ) t≥ˆrt}. This means thatrevˆr(v(ℓ)) = max{ˆri,v(ℓ) k}.Sincev(ℓ) k≥ˆrk, we have that w(ℓ) k≥⟨v(ℓ) k−ϵ⟩≥⟨ ˆrk−ϵ⟩.Since k̸=iandi=i′, it must be that k̸=i′, so{t:w(ℓ) particular, the set containsk). Letk′be the index of the second-highest bidder in w(ℓ)whose bid is above his reserve in ) t:w(ℓ) t≥⟨ˆrt−ϵ⟩}, which means that revrϵ(w(ℓ)) = max{ˆri′−ϵ,w(ℓ) k′}. Sincek∈{t:w(ℓ) t≥⟨ˆrt−ϵ⟩,t̸=i′}, we know that w(ℓ) k′≥w(ℓ) k≥v(ℓ) k−ϵ. Putting this all together, we prove that ) ) . We prove the other three cases in the appendix. We use Lemma 3.4 to continue Theorem 3.3’s proof. Let Ibe the set of indices ℓsuch that revˆr( v(ℓ)) >0. By Lemma 3.4,∑L ℓ=1revˆr( v(ℓ)) =∑ ℓ∈Irevˆr( v(ℓ)) ≤∑ ℓ∈Irevrϵ( w(ℓ)) + 2Lϵ≤∑L ℓ=1revrϵ( w(ℓ)) + 2Lϵ≤∑L ℓ=1revˆr′( w(ℓ)) + 2Lϵ. Next, we prove that for any reserve vector r, revenue under the samples v(ℓ)will only be higher than revenue under the samples w(ℓ), which intuitively makes sense since w(ℓ)≤v(ℓ).The proof, which has a similar structure as the proof of Lemma 3.4, is in Appendix B.2. Lemma 3.5. For all samples ℓ∈[L]and reserve vectors r∈Rn,revr( w(ℓ)) ≤revr( v(ℓ)) . Since∑L ℓ=1revˆr( v(ℓ)) ≤∑revˆr′( w(ℓ)) + 2Lϵ, Lemma 3.5 implies that the theorem holds. We next use Theorem 3.3 to prove stability guarantees in the case where there are multiple items and nadditive buyers. It follows from the fact that the mechanism’s revenue decomposes additively over the items. The proof is in Appendix B.2. Theorem 3.6. The set of multi-item non-anonymous second-price auctions under nadditive buyers isp-stable with p(ϵ,n,m ) = 2mϵ. Theorem 3.6 and the fact that the class is q-convergent with q(δ,L,n,m ) = O(√ 1 L( nmlog(nm) + log1 δ)) [46] implies that optimization under noise is possible: 6 Corollary 3.7. LetS={ v(1),...,v(L)} ⊂Rnm ≥0andS′={ w(1),...,w(L)} ⊂Rnm ≥0be two sets such that for all ). Let ℓ=1revr( w(ℓ)) . With high probability over S∼DL,max )] = ˜O( mϵ+√nm L) . This dependence on mϵis tight: no algorithm has better error than mϵ.Therefore, empirical revenue maximization provides an optimal dependence on the error term ϵ. The proof is in Appendix B.2. Proposition 3.8. Fix an arbitrary error term ϵ. For any deterministic algorithm Athat takes as input a training set S ⊆Rnmand returns a vector of non-anonymous reserves A(S)∈Rnm, there exists a distribution Dsuch that with probability 1over the drawS={ v(1),...,v(L)} ∼ DL,max [ revA(S′)(v)] = Ω(mϵ)for some noisy training set S′={ w(1),...,w(L)} such thatv(ℓ)−w(ℓ) ∞≤ϵfor allℓ∈[L]. 3.3 Lotteries We next prove that single-priced lottery mechanisms satisfy our stability and convergence conditions. We analyze a setting where there are nunit-demand buyers with values for mitems. Unlike the previous section, we assume there are at least nunits of each good available. A single-priced lottery is deﬁned byn(m+1) parameters: for each buyer i, there is a price ri0∈R≥0and a set of probabilities j=1rij= 1. If the buyer chooses to pay ri0, she will receive one item J∈[m], and Pr[J=j] =rij. Therefore, her expected utility is∑m j=1vijrij−ri0. She will choose to participate in the lottery so long as her expected utility is at least 0. We prove that this mechanism class satisﬁes our stability and convergence conditions. The full proof is in Appendix B.3. Theorem 3.9. The set of lotteries with nunit-demand buyers is p-stable with p(ϵ,n,m ) =nϵ. Proof sketch. We sketch the proof that p(ϵ,n,m ) =ϵin the single-buyer setting (n= 1) and for the sake of generality, we prove the guarantee for nbuyers in the appendix. For any L≥1, let S={ v(1),...,v(L)} ⊂Rm ≥0andS′={ w(1),...,w(L)} ⊂Rm ≥0be two arbitrary sets of valuation vectors such that allℓ∈[L]. Let ˆr= (ˆr0,ˆr1,..., ˆrm)be the empirically optimal parameter vector over the set Sand let ˆr′= (ˆr′ 0,ˆr′ 1,..., ˆr′ m)be the empirically optimal parameter vector over the set S′. We prove that1 L∑L ℓ=1revˆr( v(ℓ)) −revˆr′( v(ℓ)) ≤ϵ. We ﬁrst prove that if we shift the price ˆr0down byϵand evaluate the resulting lottery over S′, little rev- enue is lost. Speciﬁcally, letting rϵ= (,..., ˆrn)we prove that if revˆr( v(ℓ)) >0, then revrϵ( w(ℓ)) ≥revˆr( v(ℓ)) −ϵ.This implies that∑L ℓ=1revˆr( v(ℓ)) ≤∑L ℓ=1revrϵ( w(ℓ)) +Lϵ. Since ˆr′is empirically optimal under S′,∑L ℓ=1revˆr( v(ℓ)) ≤∑L ℓ=1revˆr′( w(ℓ)) +Lϵ. Next, we prove that for any parameter vector r, revenue under the samples v(ℓ)will only be higher than revenue under the samples w(ℓ). Speciﬁcally, for every ℓ∈[L]and any parameter vector r∈Rm+1, revr( w(ℓ)) ≤revr( v(ℓ)) . This implies that∑L ℓ=1revˆr( v(ℓ)) ≤Lϵ+∑L ℓ=1revˆr′( v(ℓ)) . By a natural generalization of prior research [ 6],q(δ,L,n,m ) =O(√ 1 L( nmlog(nm) + log1 δ)) . Fact 3.2 and Theorem 3.9 imply our main result for this section—that the class of lotteries is robust: Corollary 3.10. LetS={ v(1),...,v(L)} ⊂Rnm ≥0andS′={ w(1),...,w(L)} ⊂Rnm ≥0be two sets such that for all ). ℓ=1revr( w(ℓ)) .With high probability over )] = ˜O( nϵ+√nm L) . This dependence on nϵis tight: no algorithm has better error than nϵ.The proof is in Appendix B.3. Proposition 3.11. Fix an arbitrary error term ϵ. For any deterministic algorithm Athat takes as input a training set S ⊆Rnmand returns a vector of lottery parameters A(S)∈Rn(m+1), there exists a distribution Dsuch that with probability 1over the drawS={ v(1),...,v(L)} ∼ [ revA(S′)(v)] = Ω(nϵ)for some noisy training set S′={ w(1),...,w(L)} such thatv(ℓ)−w(ℓ) ∞≤ϵfor allℓ∈[L]. 7 3.4 Guarantees via dispersion We now provide guarantees for the class of item-pricing mechanisms under unit-demand buyers, under which revenue is a particularly volatile function. In fact, the p-stability property does not hold for any non-trivial stability function p. We show that we can use another tool—called dispersion [5]— to obtain guarantees for learning with a noisy dataset. Therefore, we illustrate that p-stability is a sufﬁcient but not necessary condition for obtaining robustness guarantees for a wide range of mechanism classes. We showcase this for item-pricing mechanisms. Item-pricing mechanisms: Let there be nbuyers andmitems and as in Section 3.2, we assume there is only one unit of each item for sale. There is a non-anonymous price rij∈Rfor each buyer i∈[n]and each item j∈[m]. First, buyer 1 arrives and buys the item j∈[m]that maximizes his chooses not to buy if v1j<r1jfor all items j∈[m]). Next, buyer 2 arrives and selects the item among the remaining that maximizes his utility v2j−r2j(or chooses not to buy). This process continues for each buyer i∈[n].As in the previous section, we use the notation revr(v) to denote the revenue of the mechanism with prices r∈Rnmwhen the buyers have values v∈Rnm. Under item-pricing mechanisms, we face an immediate obstacle which is that p-stability does not necessarily hold for any non-trivial choice of the function p: the empirically optimal prices over the noisy setS′might have terrible revenue on average over the true set S(Lemma B.11 in Appendix B.4). The primary challenge in proving a stability guarantee is that an agent may have very different values for two items but very similar utilities given a set of prices r. If we add a little noise to the buyer’s values, their preference ordering over the items may ﬂip. In turn, this may cause the buyer to select a low-cost item instead of a high-cost item, thus triggering a sharp drop in revenue. As a result, average revenue over a noisy training set may be completely different from average revenue over the uncorrupted training set. If, however, given any price vector r, the buyers’ utilities across items are not too similar (they are “dispersed”), their preferences will not change if a bit of noise is added to their values. Below, we formally capture this notion of dispersion. Deﬁnition 3.12 (() .LetS={ v(1),...,v(L)} ⊂Rnm ≥0be a set of valuation vectors. We say if for any price vector r∈Rm, there are at most kvaluation vectors inSsuch that for some buyer i∈[n], either: 1. For some item pair j,j′∈[m], buyeri’s utility for item jis withinϵof her utility for item j′, or 2. Buyeri’s utility for some item jis between 0andϵ. The parameter kallows for some slack: for some—but not all—of the valuation vectors in S, the buyers’ utilities can concentrate. Later in this section, we demonstrate that dispersion holds under mild assumptions. First, we provide a guarantee based on dispersion for optimizing prices using a noisy training set. The full proof is in Appendix B.4. Theorem 3.13. LetS={ v(1),...,v(L)} ∼DLbe a set of ( vectors. Let S′={ w(1),...,w(L)} ⊂Rnm ≥0be another set such that for all ) ∞≤ϵ. Let ˆr′be empirically optimal over ℓ=1revr( w(ℓ)) .With probability 1−δover the draw ofS,max )] =O( k L+√ 1 L( nmlog(nm) + log1 δ)) . Proof sketch. We ﬁrst prove that for any price vector r, average revenue over Sis withink Lof average revenue over S′, for the following reason. Due to dispersion, for most of the valuation vectors v(ℓ)∈S, the buyers will choose to buy the same set of goods regardless of whether their values are deﬁned by v(ℓ)orw(ℓ), so the revenue remains constant. For at most kvaluation vectors in S, the allocation may change arbitrarily, but revenue is always bounded in the interval [0,1], which is why the bound contains the termk L.Finally, to relate average and expected revenue, we use a generalization bound from prior research [ 46] which equals the second summand of our bound. This result raises an important question: when will a set of valuations be dispersed? One example, also observed in prior research [ 5], is when the distribution over buyers’ values is relatively “smooth” (a lasmoothed analysis [50]), formalized as follows. Deﬁnition 3.14. For any distribution over an abstract set Xwith probability density function f: X→R≥0, the density function is κ-bounded ifmaxx∈Xf(x)≤κ. 8 The proof of the following lemma is in Appendix B.4. Lemma 3.15. Suppose that for every buyer i∈[n]and every pair of items j,j′∈[m], buyer i’s values for items jandj′have aκ-bounded joint density function. Then for any ϵ >0, with probability 1−δover the drawS={ v(1),...,v(L)} ∼DL, the with k= 4Lnm2κϵ+O( nm2√Llognm δ) . Theorem 3.13 and Lemma 3.15 imply learning guarantees for optimizing with noisy data when dispersion holds, even for these extremely volatile revenue functions. 4 Multi-task learning We now show how our results from the previous section apply to multi-task mechanism design. Setup: In this setting, there are Ttasks, and each task t∈[T]is deﬁned by a distribution D(t)over buyers’ values. Each distribution could represent, for example, buyers from different regions or market segments. The learning algorithm receives a training set sampled from each task’s distribution. Given a mechanism class parameterized by vectors r∈Rd(which equal, for example, reserve prices) and theTtraining sets, our goal is to learn a parameter setting r(t)for each task twith high expected revenue overD(t). In particular, max be small. Task relatedness: If the distributions are completely unrelated, there is no hope that sharing infor- mation across tasks could improve learning. Thus, we require some notion of “”. Under our model of , buyers’ values across tasks are similar up to additive shifts which represent, for example, differences in their income brackets. More formally, for each buyer i∈[n]and itemj∈[m], there is an underlying distribution the cross product of these . Each task t∈[T]is deﬁned by an unknown vector of common-base values b(t)∈Rn. Intuitively, when b(t) iis large, buyer itends to be willing to pay more for items, and when b(t) iis small, buyer itends to be frugal. This model is inspired by the well-studied model [ 9,24]. For each task t∈[T], our learning algorithm receives Lsamples . Each sample v(ℓ,t)is generated by sampling z(ℓ,t)∼D and deﬁning buyer i’s value for item jto bev(ℓ,t) ij=b(t) i+z(ℓ,t) ij. This deﬁnes a distribution D(t) ijover buyeri’s value for itemj. We use the notation D(t)=×i,jD(t) ijto denote the cross product of these for taskt. The learning algorithm does not know the vectors b(t)orz(ℓ,t); it only observes the valuation vectorsv(ℓ,t). Throughout this section, we use the (slight abuse of) notation b(t)+z(ℓ,t)to denote v(ℓ,t). In Appendix D, we supply empirical evidence that demonstrates that this is a reasonable model of on real-world auction data. Sample bootstrapping: Our multi-task approach follows from the observation that for each task t∈[T],St={ ]} is a training set of LTsamples from the tthtask’s each vector z(ℓ,τ)is sampled fromD. However, we cannot compile this training set since we do not know the base values b(t). Nonetheless, in Appendix C, we show that it is to compute a set of vectors S′ t={ ]} that closely approximate *the vectors inSt: with high probability, for all pairs of tasks t,τ∈[T]and indices ) ∞=˜O(√ 1 Lm) . From our results in Section 3, we know that it is possible to learn a mechanism with high expected revenue over the distribution D(t)using the noisy training setS′ t. We summarize the resulting guarantees below. All proofs are in Appendix C. Multi-item non-anonymous second-price auctions. Our results from Section 3.2 imply the fol- lowing guarantee for learning a high-revenue multi-item second-price auction with non-anonymous reserves under additive bidders. Theorem 4.1. For each task t∈[T], letˆr′ tbe the empirically optimal reserve vector over the set S′ t:ˆr′ ℓ=1∑T τ=1revr( w(ℓ,t,τ)) .With probability 1−δ, for every task t∈[T], max r∈RnmEv∼D(t)[ revr(v)−revˆr′ t(v)] =˜O(√ 1 LT( nm+ logT δ) +m LlognT δ) . *In some cases, we show that it is also possible to exactly recover the vectors in St(Appendix C.2). 9 This theorem implies that for any γ∈(0,1), whenL=˜Ω( m/γ2) andT=˜Ω(n), the rev- enue of the auction deﬁned by the reserves ˆr′ tis withinγof optimal: )]− Ev∼D(t)[ revˆr′ t(v)] ≤γ.In contrast, the best-known single-task sample complexity guarantee re- quiresL=˜Ω( nm/γ2) [46]†. Our per-task sample complexity is smaller by a multiplicative factor equalling the number of buyers. Lotteries. Our results from Section 3.3 imply that under unit-demand buyers, optimizing lottery parameters using the noisy training sets S′ tresults in nearly optimal revenue. Theorem 4.2. For each task t, letˆr′ tbe the empirically optimal lottery parameter vector over the set S′ t:ˆr′ ℓ=1∑T τ=1revr( w(ℓ,t,τ)) .With probability 1−δ, for everyt∈[T], )[ revr(v)−revˆr′ t(v)] =˜O(√ 1 LT( nm+ logT δ) +n2 LmlognT δ) . Theorem 4.2 implies that for any γ∈(0,1), whenL=˜Ω( n/γ2) ,T=˜Ω(m), and there are more items than buyers (m > n ), the revenue of the lottery deﬁned by ˆr′ tis withinγof optimal: )[ revˆr′ t(v)] ≤γ.Meanwhile, the best-known single-task sample complexity bound requires L=˜Ω( nm/γ2) [6]. Our approach’s per-task sample complexity is better by a multiplicative factor of m. 5 Conclusions We provided guarantees for learning high-revenue mechanisms with noisy data. Learning in the presence of noise is particularly challenging in mechanism design because revenue functions are volatile, exhibiting many jump . We were nonetheless able to provide revenue guar- antees only under the assumption that the magnitude of the noise is bounded or sub-Gaussian. Our guarantees apply to the dominant approach to learning-based mechanism design: empirical revenue maximization (ERM). We demonstrated the application of our guarantees to multi-task mechanism design. We thus initiated the study of both learning with noisy data and multi-task learning in mechanism design. Future directions: (1) We focused on learning with bounded noise rather than noise of arbitrary magnitude (the full adversarial game). This is an exciting future direction if any guarantees are indeed possible when bids may be arbitrarily altered, and may require the development of new learning algorithms beyond ERM. (2) A major bottleneck for empirical evaluations in learning- based mechanism design is the lack of public datasets—we are only aware of one, from eBay [ 44]. We focused on theoretical guarantees, but applied research on robust and multi-task learning in mechanism design is a great future direction. (3) Building on our results for multi-task learning, what other notions of permit strong multi-task sample complexity bounds? Negative societal impacts: This paper falls into the broader line of research on using machine learning for mechanism design. In this direction, collecting individuals’ data and using it to ﬁne-tune prices could have negative privacy implications. The development of approaches to mechanism design via machine learning is a great direction for future research.  
sequence to sequence learning with latent neural grammars 	 Learning with Latent Neural Grammars Yoon Kim MIT CSAIL Abstract learning with neural networks has become the de facto standard for sequence prediction tasks. This approach typically models the local distribution over the next word with a powerful neural network that can condition on arbitrary context. While ﬂexible and performant, these models often require large datasets for training and can fail spectacularly on benchmarks designed to test for compositional generalization. This work explores an alternative, hierarchical approach to learning with grammars, where each node in the target tree is transduced by a node in the source tree. Both the source and target trees are treated as latent and induced during training. We develop a neural of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. We apply this latent neural grammar to various domains—a diagnostic language navigation task designed to test for compositional generalization (SCAN), style transfer, and small-scale machine ﬁnd that it performs respectably compared to standard baselines. 1 Introduction learning with neural networks [ 62,22,106] encompasses a powerful and general class of methods for modeling the distribution over an output target sequence ygiven an input source sequence x. Key to its success is a factorization of the output distribution via the chain rule coupled with a neural network that models the local conditional distribution over the next word given the previous words and the input. While architectural innovations such as attention [8], convolutional layers [ 39], and Transformers [ 110] have led to signiﬁcant improvements, this word-by-word modeling remains core to the approach, and with good reason—since any distribution over the output can be factorized via the chain rule, this approach should be able to the true target distribution given large-enough data and model.1 However, despite their excellent performance across key benchmarks these models are often sample inefﬁcient and can moreover fail spectacularly on diagnostic tasks designed to test for compositional generalization [ 68,63]. This is partially attributable to the fact that standard models have relatively weak inductive biases (e.g. for capturing hierarchical structure [ 79]), which can result in learners that over-rely on surface-level (as opposed to structural) correlations. In this work, we explore an alternative, hierarchical approach to learning with latent neural grammars . This work departs from previous approaches in three ways. First, we model the distribution over the target sequence with a grammar [103] which assumes a hierarchical generative process whereby each node in the target tree is transduced by Much of the work was completed while the author was at MIT-IBM Watson AI. Code is available at . 1There are, however, weighted languages whose next-word conditional distributions are hard to compute in a formal sense, and these distributions cannot be captured by locally normalized autogressive models unless one allows the number of parameters (or runtime) to grow in sequence length [72]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). nodes in the source tree. Such node-level alignments provide provenance and a causal mechanism for how each output part is generated, thereby making the generation process more interpretable. We additionally ﬁnd that the explicit modeling of source- and target-side hierarchy improves composi- tional generalization compared to models. Second, in contrast the existing line of work on incorporating (often observed) tree structures into sequence modeling with neural networks [ alia ], we treat the source and target trees as fully latent and induce them during training. Finally, whereas previous work on synchronous grammars typically utilized log-linear models over features [ alia] we make use of neural features to parameterize the grammar’s rule probabilities, which enables efﬁcient sharing of parameters over the combinatorial space of derivation rules without the need for any manual feature engineering. We also use the grammar directly for end-to-end generation instead of as part of a larger pipelined system (e.g. to extract alignments) [122, 41, 14]. We apply our approach to a variety of learning tasks—SCAN language nav- igation task designed to test for compositional generalization [ 68], style transfer on the English Penn Treebank [ 78], and small-scale English-French machine ﬁnd that it performs respectably compared to baseline approaches. 2 Neural Synchronous Grammars for Learning We use denote the source/target strings, and further use s,tto refer to source/target trees, represented as a set of nodes including the leaves (i.e. yield( s) =xand yield( t) =y). 2.1 Grammars grammars, introduced by Smith and Eisner [103] , deﬁne a monolingual grammar over target strings conditioned on a source tree, where the grammar’s rule set depends dynamically on the source tree s. In this paper we work with probabilistic context-free grammars (QCFG), which can be represented as a tuple G[s] = ( the distinguished start symbol, Nis the set of nonterminals which expand to other nonterminals, Pis the set of nonterminals which expand to terminals (i.e. preterminals), Σis the set of terminals, and R[s] is a set of context-free rules conditioned on s, where each rule is one of S→A[αi], A ∈N ,αi⊆s ], A∈N ,B,C∈N∪P ,αi,αj,αk⊆s D[αi]→w, D ∈P,w∈Σ,αi⊆s. We useθto parameterize the rule probabilities pθ(r)for eachr∈R[s]. In the above, αi’s are subsets of nodes in the source tree s, and thus a QCFG tranduces the output tree by aligning each target tree node to a subset of source tree nodes. This monolingual generation process differs from that of classic synchronous context-free grammars [ 118] which jointly generate source and target trees in tandem (and therefore require that source and target trees be isomorphic), making QCFGs appropriate tools for tasks such as machine translation where syntactic divergences are common.2Since theαi’s are elements of the power set of s, the above formulation as presented is completely intractable. We follow prior work [ 103,112] and restrict αi’s to be single nodes (i.e. αi,αj,αk∈s), which amounts to assuming that each target tree node is aligned to exactly one source tree node. In contrast to standard, “ﬂat” models where any hierarchical structure necessary for the task must be captured implicitly within a neural network’s hidden layers, synchronous grammars explicitly model the hierarchical structure on both the source and target side, which acts as a strong source of inductive bias. This tree transduction process furthermore results in a more interpretable generation process as each span in the target aligned to a span in the source via node- level generally, the grammar’s rules provide a symbolic interface to the model with which operationalize constraints and imbue inductive biases, and we show how this mechanism can be used to, for example, incorporate phrase-level copy mechanisms (section 2.4). 2It is also possible to model syntactic divergences with richer grammatical formalisms [ 101,81]. However these approaches require more expensive algorithms for learning and inference. 3Similarly, latent variable attention [ ] provides for more a interpretable generation process than standard soft attention via explicit word-level alignments. 2 2.2 Since each source tree node αiis likely to occur only a few times (or just once) in the training corpus, parameter sharing becomes crucial. Prior work on QCFGs typically utilized log-linear models over handcrafted features to share parameters across rules [ 103,42]. In this work we instead use a neural which allows for easy parameter sharing without the need for manual feature engineering. Concretely, we represent each target nonterminal and source node combination A[αi]as an embedding, eA[αi]=uA+hαi, where uAis the embedding for A, and hαiis the representation of node αigiven by running a TreeLSTM over the source tree s[107,134]. These embeddings are then combined to produce the probability of each rule, pθ(S→A[αi]) ∝exp( u⊤ SeA[αi]) , ( ]) +f3(eC[αk]))) , pθ(D[αi]→w) ∝exp( ) , feedforward networks with residual layers (see Appendix A.1 for the exact ). Therefore the learnable parameters in this model are the nonterminal embeddings ( ), terminal (i.e. uw,bwforw∈Σ), and the parameters of the TreeLSTM and the feedforward networks. 2.3 Learning and Inference The QCFG described above deﬁnes a distribution over target trees (and by , target strings) given a source tree. While prior work on QCFGs typically relied on an off-the-shelf parser over the source to obtain its parse tree, this limits the generality of the approach. In this work, we learn a probabilistic source-side parser along with the QCFG. This parser is a monolingual PCFG with parameters φthat deﬁnes a posterior distribution over binary parse trees given source strings, i.e. pφ(s|x). Our PCFG uses the neural from Kim et al. [64]. With the parser in hand, we are now ready to deﬁne the log marginal likelihood, logpθ,φ(y|x) = log ∑ s∈T(x)∑ ) . the sets of trees whose yields are . Unlike in synchronous context-free grammars, it is not possible to efﬁciently marginalize over both T(y) andT(x)due to the non-isomorphic assumption. However, we observe that the inner sum- mation∑ t∈T(y)pθ(t|s) =pθ(y|s)can be computed with the usual inside algorithm [ 9] in ), whereSis the source length and Tis the target length. This motivates the following lower bound on the log marginal likelihood, )], which is obtained by the usual application of Jensen’s inequality (see Appendix A.2).4 An unbiased Monte Carlo estimator for the gradient with respect to θis to compute given a sample from pφ(s|x), since we can just backpropagate through the inside algorithm. For the gradient respect to φ, we use the score function estimator with a self-critical baseline [92], ∇), 4As is standard in variational approaches, one can tighten this bound with the use of a variational distribution qψ(s|x,y), which results in the following evidence lower bound, )]. This is equivalent to our objective if we set qψ(s|x,y) =pφ(s|x). Rearranging some terms, we then have, )] = )]. Hence, our use of pφ(s|x)as the variational distribution is militating towards learning a model which achieves good likelihood but at the same time has a posterior distribution is close to the prior pφ(s|x) (i.e. learning a model where most of the uncertainty about sis captured by xalone). This is arguably reasonable for many language applications since parse trees are often assumed to be task-agnostic. 3 where s′is a sample from pφ(s|x)andˆsis the MAP tree from pφ(s|x). We also found it important to regularize the source parser by simultaneously training it as a monolingual PCFG, and therefore the gradient expression the sample tree s′, the argmax tree ˆs, and scoring the sampled tree logpφ(s′|x)all programs. Hence the runtime is still dominated by the O(S3T3)dynamic program to compute We found this to be manageable on modern GPUs with a vectorized implementation of the inside algorithm. Our implementation uses the Torch-Struct library [93]. Predictive inference For decoding, we ﬁrst run MAP inference with the source parser to obtain ˆs= argmaxspφ(s|x). Givenˆs, ﬁnding the most probable sequence . the consensus string of the grammar G[ˆs]) is still difﬁcult, and in fact NP-hard [ 102,16,77]. We therefore resort to an approximate decoding scheme where we sample Ktarget trees ], rescore the yields of the sampled trees, and return the tree whose yield has the lowest perplexity. 2.4 Extensions Here we show that the formalism of synchronous grammars provides a ﬂexible interface with which to interact with the model. Phrase-level copying Incorporating copy mechanisms into models has led to signiﬁcant improvements for tasks where there is overlap between the source and target sequences []. These models typically deﬁne a latent variable at each time step that learns to decide to either copy from the source or generate from the target vocabulary. While useful, most existing copy mechanisms can only copy singletons due to the word-level contrast, the hierarchical generative process of QCFGs makes it convenient to incorporate phrase-level copy mechanisms by using a that always copies the yield of the source subtree that it is combined with. Concretely, letting ACOPY∈N be a COPY nonterminal, we can expand the rule set R[s]to include rules of the form +, and deﬁne the probabilities to be, = 1{v= yield(αi)}. (The preterminal copy mechanism is similarly deﬁned.) Computing pθ(y|s)in this modiﬁed grammar requires a modiﬁcation of the inside algorithm.8In our style transfer experiments in section 3.2 we show that this phrase-level copying is important for obtaining good performance. While not explored in the present work, such a mechanism can readily be employed to incorporate external rules (e.g. from bilingual lexicons or tables) into the modeling process, which has been previously investigated at the [88, 3]. Adding constraints on rules For some applications we may want to place additional restrictions on the rule set to operationalize domain-speciﬁc constraints and inductive biases. For example, ( αi)for rules of the form constrain the target tree hierarchy to respect the source tree hierarchy, while restricting αito source terminals (i.e. αi∈yield( s)) for rules of the form D[αi]→wwould enforce that each target terminal be aligned to a source terminal. We indeed make use of such restrictions in our experiments. Incorporating autoregressive language models Finally, we remark that simple extensions of the QCFG can incorporate standard autoregressive language models. Let pLM(w|γ)be a distribution over the next word given by a (potentially conditional) language model given arbitrary context γ(e.g. γ=y<tfor a monolingual language model and γ=x,y<tfor a model). One way to embed this language model into a QCFG would be to use a special that is not combined with any source node, and deﬁne the emission probability to be, ). (The nonterminal probabilities computed with the associated symbol embedding uDLM.) Both the QCFG and the language model can then be trained jointly. 5This motivates our use of a generative rather than a discriminative parser on the source side. 6This runtime is incidentally is the same as that of the bitext inside algorithm for marginalizing over both source and target trees in rank-two synchronous context-free grammars. 7However see Zhou et al. [132], Panthaplackel et al. [87], and Wiseman et al. [114]. ] =pθ(N∗→ys:t)be the inside variable for N’s being the root of the subtree over ys:t, we can simply set β[s,t,A COPY[αi]] = 1{ys:t= yield(αi)}. 4 Approach Simple Jump A. Right Length RNN [68] 99.7 1.7 2.5 13.8 CNN [29] 100.0 69.2 56.7 0.0 Transformer [38] − 1.0 53.3 0.0 T5-base [38] − 99.5 33.2 14.4 Syntactic Attn [94] 100.0 91.0 28.9 15.2 Meta Seq2Seq [67] − 99.9 99.9 16.6 CGPS [70] 99.9 98.8 83.2 20.3 Equivar. Seq2Seq [44] 100.0 99.1 92.0 15.9 Span-based SP [54] 100.0 − 100.0− LANE [76] 100.0 100.0 100.0 100.0 Program Synth. [85] 100.0 100.0 100.0 100.0 NeSS [19] 100.0 100.0 100.0 100.0 NQG-T5 [98] 100.0 100.0 − 100.0 GECA [6] − 87.0 82.0− R&R Data Aug. [4] − 88.0 82.0− Neural QCFG (ours) 96.9 96.8 98.7 95.7 Table 1: Accuracy on the SCAN dataset splits compared to previous P0[look]→LOOK P0[walk]→WALK P0[jump]→JUMP N4[look left ]→] N4[look right ]→] N4[walk left ]→] N4[walk right ]→] N1[look right twice ]→N4[look right ]N4[look right ] N1[walk left twice ]→N4[walk left ]N4[walk left ] N1[look thrice ]→N8[look thrice ]P0[look] N1[look right thrice ]→N8[look right thrice ]N4[look right ] N8[look right thrice ]→N4[look right ]N4[look right ] N1[walk left thrice ]→N8[walk left thrice ]N4[walk left ] N8[walk left thrice ]→N4[walk left ]N4[walk left ] Table 2: rules from MAP tar- get trees on the add primitive (jump) train set. For some cases we may want to make use of a conditional language model that condition on subparts of the source sentence. This may be appropriate for learning to translate phrases whose translations cannot be obtained by stitching together independent translations of subparts (e.g. idioms such as “kicked the bucket”). In this case we can make use of a special nonterminal ALM∈N which is combined with source tree nodes to produce rules of the form +. The associated probabilities are then deﬁned to be, )). While these extensions can embed ﬂexible autoregressive models within a QCFG,9they also inherit many of the issues attendant with such models (e.g. over-reliance on surface form). In preliminary experiments with these variants, we found the combined model to quickly degenerate into the uninteresting case of always using the conditional language model, and hence did not pursue this further. However it is possible that modiﬁcations to the approach (e.g. posterior regularization to penalize overuse of the conditional language model) could lead to improvements. 3 Experiments We apply the neural QCFG described above to a variety of learning tasks. These experiments are not intended to push the on these tasks but rather intended to assess whether our approach performs respectably against standard baselines while simulatenously learning interesting and interpretable structures. 3.1 SCAN We ﬁrst experiment on SCAN [ 68], a diagnostic dataset where a model has to learn to translate simple English commands to actions (e.g. jump twice after walk →WALK JUMP JUMP ). While conceptually simple, standard models have been shown to fail on splits of the data designed to test for compositional generalization. We focus on four commonly-used splits: (1)simple , where train/test split is random, (2) add primitive (jump) , where the primitive command jump is seen in isolation in training and must combine with other commands during template (around right) , where the template around right is not seen during training, and (4) length , where the model is trained on action sequences of length at most 22 and tested on action sequences of length between 24 and 48. 9Alternatively, we can also embed a QCFG within an autoregressive language model with a binary latent distribution pLM(zt|x,y<t)) at each time step. This over during between ), where the latter next-word probability distri- bution in the QCFG can be computed with a probabilistic Earley parser [ 105]. The difference between the approaches stems from whether the switch decision is made by the QCFG or by the language model. 10The QCFG deﬁned in this paper places zero probability on length-one target strings, which presents an issue for this split of SCAN where jump→JUMP is the only context in which “ JUMP ” occurs in the training set. To address this, in cases where the target string is a singleton we simply replicate the source and target, i.e. jump→JUMP becomes jump jump→JUMP JUMP . 5 α12 α11 α9 α6 twiceα7 α5 rightα4 jumpα3 afterα10 α2 twiceα8 α1 leftα0 runN0[α12] N1[α10] N4[α8] P0[α1] RUNP0[α1] TURN-LEFTN4[α8] P0[α1] RUNP0[α1] ] N4[α7] P0[α4] JUMPP0[α5] ] P0[α4] JUMPP0[α5] TURN-RIGHT Figure 1: Generation from the neural QCFG on a test example from the add primitive (jump) split of SCAN. The induced tree from the learned source parser is shown on the left, and the target tree derivation is shown on the right. We do not show the initial root-level node (i.e. S→N0[α12]). While the model does not distinguish between preterminals and terminals on the source tree, we have shown them separately for additional clarity. We also show some of the node-level alignments with dashed lines. In these experiments, the nonterminals A∈N are only combined with source nodes that govern at least two nodes, and the preterminals P∈P are only combined with source terminals. We set|N|= 10 and|P|= 1, and place two additional restrictions on the rule set. First, for rules of the form S→A[αi]we restrictαito always be the root of the source tree. Second, for rules of the form be a descendant of αi, orαiitself (i.e. ( αi)∪{αi}). These restrictions operationalize the constraint that the target tree hierarchy respects the source tree hierarchy, though still in a much looser sense than in an isomorphism. We found these constraints to be crucial in learning models that perform well on the compositional splits of the dataset. See Appendix A.3.1 for the full experimental setup and . Results Table 1 shows our results against various baselines on SCAN. While many approaches are able to solve this dataset almost perfectly, they often make use of SCAN-speciﬁc knowledge, which precludes their application to non-synthetic domains. The neural QCFG performs respectably while remaining . In Table 2 we show some examples of frequently- occurring rules based on their MAP target tree counts on the training set of the add primitive (jump) split. Many of the rules are sensible, and they furthermore illustrate the need for multiple nonterminal symbols. For example, in order to deal with source phrases of form “ xthrice ” in a grammar that only has unary and binary rules, the model uses the nonterminals N1andN8in different ways when combined with the same phrase. Figure 1 shows an example generation from the test set of the add primitive (jump) split, where we ﬁnd that node-level alignments provide explicit provenance for each target span and thus makes the generation process more interpretable than standard attention mechanisms. These alignments can also be used to diagnose and rectify systematic errors. For example, we sometimes found the model to incorrectly split “ x{and,after}y” to “xx” (or “yy”) at the root node. When we manually disallowed such splits during decoding, performance increased by 1%-2% across the board, showcasing a beneﬁt of grammar-based models which makes it possible to directly manipulate model generations by intervening on the set of derivation rules. 3.2 Style Transfer We next apply our approach on style transfer on English utilizing the StylePTB dataset from Lyu et al. [78]. We focus on the three hard transfer tasks identiﬁed by the original paper: (1) active to passive , where a sentence has to be changed from active to passive voice (2808 examples), (2) adjective emphasis , where a sentence has to be rewritten to emphasize a particular adjective (696 examples) (3) verb/action emphasis , where a sentence has to be rewritten to emphasize a particular verb/action (1201 main difﬁculty with these tasks stems from the small training set combined with the relative complexity of these tasks. For these experiments we set |N|=|P|= 8 and use the same restrictions on the rule set as in the SCAN experiments. We also found it helpful to contextualize the source embedding with a bidirectional LSTM before feeding to the TreeLSTM encoder.12We further experiment with the 11To encode information about which word to be emphasized in the adjective/verb emphasis tasks, we use a binary variable whose embedding is added to the word embedding on the encoder side. 12A drawback of using contextualized word embeddings as input to the TreeLSTM is that since the repre- sentations hαifor each node αiare now a function of the entire sentence (and not just the leaves), we can no 6 Transfer Type Approach BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr Active to 0.476 0.329 0.238 0.189 0.216 0.464 1.820 Seq2Seq 0.373 0.220 0.141 0.103 0.131 0.345 0.845 Retrieve-Edit 0.681 0.598 0.503 0.427 0.383 0.663 4.535 Human 0.931 0.881 0.835 0.795 0.587 0.905 8.603 Seq2Seq 0.505 0.349 0.253 0.190 0.235 0.475 2.000 Neural QCFG 0.431 0.637 0.548 0.472 0.415 0.695 4.294 Seq2Seq + copy 0.838 0.735 0.673 0.598 0.467 0.771 5.941 Neural QCFG + copy 0.836 0.771 0.713 0.662 0.499 0.803 6.410 Adj. 0.263 0.079 0.028 0.000 0.112 0.188 0.386 Seq2Seq 0.187 0.058 0.018 0.000 0.059 0.179 0.141 Retrieve-Edit 0.387 0.276 0.211 0.164 0.193 0.369 1.679 Human 0.834 0.753 0.679 0.661 0.522 0.811 6.796 Seq2Seq 0.332 0.333 0.051 0.000 0.142 0.27 0.845 Neural QCFG 0.348 0.178 0.062 0.000 0.162 0.317 0.667 Seq2Seq + copy 0.505 0.296 0.184 0.119 0.242 0.514 1.839 Neural QCFG + copy 0.676 0.506 0.393 0.316 0.373 0.683 3.424 Verb 0.309 0.170 0.095 0.041 0.140 0.292 0.593 Seq2Seq 0.289 0.127 0.066 0.038 0.098 0.275 0.300 Retrieve-Edit 0.416 0.284 0.209 0.148 0.223 0.423 1.778 Human 0.649 0.569 0.493 0.421 0.433 0.693 5.668 Seq2Seq 0.355 0.152 0.083 0.043 0.151 0.320 0.530 Neural QCFG 0.431 0.250 0.140 0.073 0.219 0.408 1.097 Seq2Seq + copy 0.526 0.389 0.294 0.214 0.294 0.464 2.346 Neural QCFG + copy 0.664 0.512 0.407 0.319 0.370 0.589 3.227 Table 3: Results on the hard style transfer tasks from the StylePTB dataset [ 78]. For each transfer type, the top four rows are from Lyu et al. [78], while the bottom four rows are from this paper. Metrics such as BLEU and ROUGE are normally scaled to [0, 100] (as in Table 4), but here we keep them at [0, 1] as in the original paper. phrase-level copy mechanism as described in section 2.4. The original paper provides several strong baselines: ﬁnetuned GPT2, a standard model, and the model from Hashimoto et al. [53]. We also train our own baseline model with a word-level copy mechanism. See Appendix A.3.2 for more details. Results Table 3 shows the results where we observe that the neural QCFG performs well compared to the various baselines.13We further ﬁnd that incorporating the copy mechanism improves results substantially for both the baseline LSTM and the neural QCFG.14Figure 2 shows a test example from the task, which shows the word- and phrase-level copying mechanism in action. In this example the source tree is linguistically incorrect, but the grammar is nonetheless able to appropriately transduce the output. Given that linguistic phrases are generally more likely to remain unchanged in these types tasks, incorporating this knowledge into the learning process could potentially improve results.15For example in Figure 2 the ideal case would be to copy the phrase “ a 2-for-1 stock split ” but this is not possible due to the incorrectly predicted source tree. Finally, although our approach ostensibly improves upon the baselines according to many of the n-gram-based metrics, we observed the generated sentences to be often ungrammatical, highlighting the limitations of automatic metrics for these tasks while at the same time indicating opportunities for further work in this area. longer guarantee that target derivations such as depend on αi. This somewhat hinders the of the node-level alignments. 13As in the original paper we calculate the automatic metrics using the nlg-eval library, available at . 14Even for models that do not explicitly use the copy mechanism, we indirectly allow for copying by replacing the⟨unk⟩token with the source token that the preterminal is combined with in the neural QCFG case, or the source token that had the maximum attention weight in the LSTM case. This explains the outperformance of our baseline models compared to the baselines from Lyu et al. [78], which roughly uses the same architecture. 15There are many ways to do this. For example, one could identify the longest overlap between the source and target, and use posterior regularization on the source PCFG to encourage it to be a valid constituent. 7 α12 α11 α6 splitα5 stockα10 α4 2-for-1α9 α3 aα8 α2 declaredα7 α1 corp.α0 uniﬁrstN5[α12] NCOPY[α7] uniﬁrst corp.N4[α12] N1[α8] P2[α0] byN3[α8] PCOPY[α2] declaredP6[α2] isN2[α12] NCOPY[α11] stock splitN2[α10] PCOPY[α4] 2-for-1P7[α3] a Figure 2: A test example from the active to passive style transfer task on the Penn Treebank. The induced tree from the learned source parser is shown on the left, and the target tree derivation is shown on the right. The source tree is linguistically incorrect but the model is still able to correctly tranduce the output. Some examples ofCOPY and their aligned source nodes are shown with dashed arrows. 3.3 Machine Translation Our ﬁnal experiment is on a small-scale English-French machine translation dataset from Lake and Baroni [68]. Here we are interested in evaluating the model in two ways: ﬁrst, to see if it can perform well as a standard machine translation system on a randomly held out test set, and second, to see if it can systematically generalize to unseen combinations. To assess the latter, Lake and Baroni [68] add 1000 repetitions of i am daxy→je suis daxiste to the training set and test on 8 new sentences that use daxy in novel combinations (e.g. he is daxy→il est daxiste and i am not daxy→je ne suis pas daxiste ). As the original dataset does not provide ofﬁcial splits, we randomly split the dataset into 6073 examples for training (1000 of which is the “ i am daxy ” example), 631 examples for validation, and 583 for test.16 For these experiments, we set |N|=|P|= 14 and combine all source tree nodes with all nonter- . We place two restrictions on the rule set: for rules of the form S→A[αi]we restrictαito be the root of the source tree (as in previous experiments), and for rules of the form be the direct children of αisuch ifαihas no children).17As in the style transfer experiments, we also experiment with a bidirectional LSTM encoder which contextualizes the source word embeddings before the TreeLSTM layer. Our baselines here include standard models as well as approaches that explicitly target compositional generalization [70, 19]. Approach BLEU daxy acc. LSTM 25.1 12.5% Transformer 30.4 100% CGPS [70] 19.2 100% NeSS [19]− 100% Neural QCFG 23.5 100% + BiLSTM 26.8 75.0% Table 4: Results on English- French machine Table 4 shows BLEU on the regular test set of 583 sen- tences and accuracy on the 8 daxy the neural QCFG performs nontrivially, it is soundly outperformed by a well- tuned Transformer model, which performs impressively well even on the daxy test set.19We thus consider our results on machine trans- lation to be largely negative. Interestingly, the use of contextualized word embeddings (via the bidirectional LSTM) improves BLEU but hurts compositional generalization, highlighting the potential pitfalls of using ﬂexible models which can sometimes entangle in undesirable ways.20Figure 3 shows several ex- amples of target tree derivations from the neural QCFG that does 16The original dataset has 10000 examples (not including the daxy examples), but many of them involve duplicate source sentences. We removed such duplicates in our split of the data. 17These restrictions are closer to the strict isormorphic requirement in synchronous context-free grammars than in previous experiments. However they still allow for trees since αican be inherited if it has no children. 18For CGPS and NeSS, the original papers only assess accuracy on the daxy test set, and furthermore do not provide the splits. To obtain BLEU for CGPS, we run the publicly available code ( ) on our split of the data. For NeSS, the code is not publicly available but the authors provided a version of their implementation. However, the provided were tailored for the SCAN dataset, and despite our best efforts to adapt the to our setup we were unable obtain sensible results on the machine translation dataset. 19The Transformer did, however, require some hyperparameter tuning given the small size of our dataset. Similar ﬁndings have been reported by Wu et al. [120] in the context of applying Transformers to moderately sized transduction datasets. 20This variant of the neural QCFG also does poorly on the compositional splits of SCAN. 8 N13[i m as tall as my father . ] P2[.] .N2[i m as tall as my father ] N12[m as tall as my father ] N13[as tall as my father ] N2[my father ] P4[father ] pereP1[my] monN2[as tall as ] P5[as] queN13[as tall ] P8[tall] grandP8[as] aussiP8[m] suisP8[i] jeN0[i m not a cat . ] P0[.] .N5[i m not a cat ] N2[m not a cat ] N0[not a cat ] N11[a cat ] P5[cat] chatP13[a] unP0[not] pasN3[m] P9[m] vaisP13[m] neP8[i] je N13[i am very daxy . ] P2[.] .N2[i am very daxy ] N12[am very daxy ] N12[very daxy ] P2[daxy ] daxisteP8[very ] tresP8[am] suisP8[i] jeN13[he is daxy . ] P2[.] .N2[he is daxy ] N12[is daxy ] P2[daxy ] daxisteP8[is] estP2[he] ilN13[he is not daxy . ] P2[.] .N2[he is not daxy ] N12[is not daxy ] N12[not daxy ] P2[daxy ] daxisteP8[not] pasN6[is] P8[is] estP13[is] nP5[he] il Figure 3: Target tree derivations from the English-French machine translation experiments. Top left is an example from the regular test set, top right is a made-up example (which is incorrectly translated by the model), and the bottom three trees are from the daxy test set. We do not explicitly show the source trees here and instead show the source phrases as arguments to the target tree . not use contextualized word embeddings. The induced source trees are sometimes linguistically incorrect (e.g. in the top left example “ as tall as ” would not be considered a valid linguistic phrase), but the QCFG is still able to correctly transduce the output by also learning unconventional trees on the target side as well. This is reminiscent of classic hierarchical phrase-based approaches to machine translation where the extracted phrases often do not correspond to linguistic phrases [ 20]. Finally, although the model does well on the daxy test set, it still incorrectly translates simple but unusual made-up examples such as “ i m not a cat ” (Figure 3, top right). This is despite the fact that examples of the form i {am,m} not a x→je ne suis pas {un,une} yoccur multiple times in the training set.21We speculate that while the probabilistic nature of the grammar and the use of distributed enables easier training, they contribute to the model’s being (still) vulnerable to spurious correlations. 4 Discussion While we have shown that neural grammars can perform well for some sequence- to-sequence learning tasks, there are several serious limitations. For one, the O(|N|(|N|+ | program will likely pose challenges in scaling this approach to larger datasets with longer inference was also much more expensive since we found it necessary to sample and score a large number of target trees to perform well on the non-synthetic datasets (see Appendix A.3). The conditional independence assumptions made by the QCFG may also be too strong for some tasks that involve complex dependencies, and the approach may furthermore be inappropriate for domains where the input and output are not naturally . The models were quite sensitive to and some datasets needed training over multiple random seeds to perform well. These factors make our approach much less “off-the-shelf” than standard models, although this may be partially attributable to the availability of a robust set of for existing approaches. 21The other models were also unable to correctly translate this sentence. 22Indeed, on realistic machine translation datasets with longer sequences we quickly ran into memory issues when running the model on just a single example, even with a multi-GPU implementation of the inside algorithm distributed over four 32GB GPUs. To apply the model on longer sentences, an interesting future direction might involve working with a “soft” version of the grammar, where the nonterminals embeddings are contexualized against source elements via soft attention. The runtime and memory for marginalizing over target trees in this soft QCFG would have a linear (instead of cubic) dependence source length. 9 At the start of this project, our initial hope was to show that classic, grammar-based approaches to sequence transduction had been unfairly overlooked in the current deep learning era, and that revisiting these methods with contemporary would prove to be more than just an academic exercise. , this seems not to be the case. While we did observe decent performance on niche datasets such as SCAN and StylePTB where inductive biases from grammars were favorably aligned to the task at hand, for tasks like machine translation our approach was thoroughly steamrolled by a well-tuned Transformer. What role, then, can such models play in building practical NLP systems (if any)? It remains to be seen, but we venture some guesses. Insofar as grammars and other models with symbolic components are able to better surface model decisions than standard approaches, they may have a role in developing more controllable and interpretable models, particularly in the context of collaborative human-machine systems [ 40]. Alternatively, inﬂexible models with strong inductive biases have in the past been used to guide (overly) ﬂexible neural models in various ways, for example by helping to generate additional data [ 58,75] or inducing structures with which to models [24,74,3,127]. In this vein, it may be interesting to explore how induced structures from grammars (such as the tranduction rules in Table 2) can be used in conjunction with ﬂexible neural models. 5 Related Work Synchronous grammars Synchronous grammars and tree transducers have a long and rich history in natural language processing [ alia ]. In this work we focus on the formalism of grammars, which relaxes the requirement that source trees be isomorphic to target trees. grammars have enjoyed applications across a wide range of domains including in machine translation [ 103,42,43], question answering [112], paraphrase detection [ 27], sentence simpliﬁcation [ 117,116], and parser projection [ 104]. Prior work on grammars generally relied on pipelined parse trees for the source and only marginalized out the target tree, in contrast to the present work which treats both source and target trees as latent. Compositional learning Lake and Baroni [68] proposed the inﬂuential SCAN dataset for assessing the compositional generalization capabilities of neural sequence-to- sequence models. There has since been a large body of work on compositional learning through various approaches including modiﬁcations to existing architectures [ 70,94,44, 17,26], grammars and neuro-symbolic models [ 86,98,85,19,76], meta-learning [ 67,25], and data augmentation [ 6,49,50,4]. Our approach is closely related to NQG-T5 [ 98] which uses a rules-based approach to induce a QCFG and then backs off to a ﬂexible model during prediction if the grammar cannot parse the input sequence. Deep latent variable models There has much work on neural of classic proba- bilistic latent variable models including hidden Markov models, [ 109,113,21], ﬁnite state transducers [91,71] topic models [ 82,30,31], dependency models [ ], and context-free gram- mars [ ]. These works essentially extend feature-based unsupervised learning [ 10] to the neural case with the use of neural networks over embedding , which makes it easy to share parameters and additionally condition the generative model on side information such as auxiliary latent variables [ 52,64], images [ 130,60,55], video and audio [ 129], and source-side context [ 113,99]. Since we marginalize over unobserved trees during learning, our work is also related to the line of work on marginalizing out latent for sequence transduction tasks [46, 33, 11, 66, 128, 90, 57, 69, 108, 111, inter alia ]. 6 Conclusion In this paper we have studied learning with latent neural grammars. We have shown that the formalism grammars provides a ﬂexible tool with which to imbue inductive biases, operationalize constraints, and interface with the model. Future work in this area could consider: (1) revisiting richer grammatical formalisms (e.g. synchronous tree-adjoining grammars [ 101]) with contemporary , (2) conditioning on other modalities such as images/audio for grounded grammar induction [ 100,130,60,129], (3) adapting these methods to other structured domains such as programs and graphs, and (4) investigating how grammars and symbolic models can be integrated with pretrained language models to solve practical tasks. 10  
decoupling knowledge from memorization retrieval augmented prompt learning 	Decoupling Knowledge from Memorization: Prompt Learning Xiang Chen1,2∗, Lei Li1,2∗, Ningyu Zhang1,2†, Xiaozhuan Liang1,2, Shumin Deng1,2, Chuanqi Tan3, Fei Huang3, Luo Si3, Huajun Chen1,2† 1Zhejiang University & AZFT Joint Lab for Knowledge Engine, China 2Hangzhou Innovation Center, Zhejiang University, China 3Alibaba Group, China {xiang_chen, leili21, zhangningyu, liangxiaozhuan, 231sm, , {chuanqi.tcq, f.huang, Abstract Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during training or over- fit shallow patterns with low-shot data. To alleviate such limitations, we develop RETRO PROMPT with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RETRO PROMPT constructs an open-book from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhance- ment. Extensive experiments demonstrate that RETRO PROMPT can obtain better performance in both few-shot and zero-shot settings. Besides, we further illustrate that our proposed RETRO PROMPT can yield better generalization abilities with new datasets. Detailed analysis of memorization indeed reveals RETRO PROMPT can reduce the reliance of language models on memorization; thus, improving generalization for downstream tasks3. 1 Introduction Large parametric language models [ 45,7,22,31] have achieved dramatic empirical success in natural language processing (NLP). Notably, pre-trained language models (PLMs) have learned a substantial amount of in-depth knowledge from data, and have archived tremendous promise in learning ability with the natural language prompts [ 12,50,57]. However, Recent studies [ 37,39,59] observe that prompt learning with PLMs usually generalizes unstably in an extremely low-resource setting or emerging domains. One potential reason is that, it is non-trivial for parametric models to learn rare or hard patterns well with rote memorization , thus, resulting in inefficient generalizable performance. Intuitively, if we regard the whole training set as a book and the test phase as the examination , the current training-test procedure of prompt learning (based on batch data training) can be viewed as ∗Equal contribution. †Corresponding Author. 3Code is available in / RetroPrompt . 36th Conference on Neural Information Processing Systems (NeurIPS 2022). page-by-page memorization andclosed-book examination [42]. During training, vanilla prompt learning may struggle to memorize atypical instances in a setting or overfit shallow patterns with low-shot data [ 62,9]. Specifically, recent studies[ 10,11] have proposed a long- tail theory, which notes that when the training set has a long-tail distribution and contain small “” with atypical instances, then PLMs indeed predict on the test data through rote memorizing these atypical instances rather than learning the common patterns [62, 56]. The limitations of rote memorization remind us of the human learning process of “learn by analogy” and the proverb that “the palest ink is better than the best memory” . Note that humans can perform associative learning to recall relevant skills in deep memories for reinforcing each other, thus, owning the extraordinary abilities to solve few-shot and zero-shot tasks. Motivated by these, we endeavor to improve the generalization ability of prompt learning with retrieval and association. Our intuition is that the difficulty of resolving the above limitations can be substantially alleviated if we can decouple the knowledge from memorization by constructing an open-book from the training data; thus, referring to related knowledge could provide a strong enhancement signal to help the model strike a balance between generalization and memorization. Book1 Book2 Book3 Demonstration NN Incorporated Pr ediction Open-book Knowledge-stor e negative NN Guided T vs. Generalization Decoupling Knowledge Figure 1: Decoupling knowledge from , we introduce a novel retrieval- augmented framework based on prompt learning ( RETRO PROMPT ) as shown in Figure 1. The open-book knowledge store (K,V), defined as the set of key: prompt- based example embeddings andvalue: cor- responding label words constructed from the training data, are served as additional references for the model to decouple knowl- edge from pure memorization to some ex- tent. Specifically, to integrate retrieved knowledge into the input, Firstly , we de- sign to incorporate neural demonstrations into the input sequences as in-context augmentation, where the demonstration is retrieved from the . Then , we apply a non-parametric algorithm kNN over the input query and knowledge store, and regard kNN results as an indication of easy vs. hard instances. Moreover, we automatically force the model to emphasize the hard instances identified bykNN by assigning a scaling during training. Lastly , thekNN results are further employed at the output of the PLM head to participate in masked prediction. The model conducts inference through linearly interpolating the non-parametric nearest neighbor distribution with the output of prompt learning, which regards the Top- knearest reference instances as cues from (K,V). The considerable performance gains on nine tasks in few-shot and zero-shot settings demonstrate that our systemic retrieval mechanism helps the model generalize better with scarce data. Experiments in the setting with long-tail distribution illustrate that our RETRO PROMPT can deal with atypical instances more robustly. We further adopt self-influence [ 27] as our memorization scoring function to analyze the memorization process between fine-tuning, prompt learning and our RETRO PROMPT . The final analysis results show that 1) the training samples having the highest memorization scores are mostly atypical, 2) RETRO PROMPT generalize better than fine-tuning and convention prompt-tuning with decoupling knowledge from memorization to alleviate the rote of PLMs. In a nutshell, our work may open up new avenues to improve the generalization of prompting PLMs by decoupling knowledge from memorization. 2 Preliminaries of Prompt Learning Assuming that denotes the PLM and the template function for prompt tuning. Formally, the text classification task takes a query sentence x= (x0, x1, ..., x n)as input. Then, classify it into the label y∈ Y. While prompt learning converts the task into a MLM problem with cloze-style objectives. Specifically, the template function Tinserts text pieces into xasˆx=T(x), where ˆxrefers to the input of Mwith a [MASK] token. For instances, when we have to classify the textx=“The movie makes absolutely no sense.” into label NEGATIVE (labeled as 0) or POSITIVE (labeled as 1), we wrap it into ˆx=[CLS] xIt was [MASK][SEP] (1) 2 Transformer Layers great Label words gr eat[CLS] The movie ... sense. It Layers gr eatgr eatterribleKey ValueOpen-book Knowledge-stor e Update Original input Template [CLS] The movie ... sense. It was[MASK][SEP] Original input Template e(t) e(g) Demonstrations ···Nearest Representation h(M)MLM Head for label:positive Demonstrations for label:negative prompt-based representation Open-book knowledge-stor e Retrieve Nerual NNterrible terrible gr eat 100 60 65 ···Normalization terrible terrible great0.7 0.1 0.15 ··· ···Aggregation terrible great 0.2terrible great Label words ✘✔ N EGA TIVE b. Creation and refresh of open-book a. prompt learningstale refresh Asynchr onously refresh ··· e(M) ··· 0.8 ×(1+ ( ))NN Incorporated Pr ediction NN Guided T rainingFigure 2: Overview of RETRO PROMPT . Note that e(·)denotes word embedding function in the PLM M, while “M”,“t” and “g” in refers to “[MASK]”, “terrible” and “great”. The verbalizer f:Y 7→ V is defined as a mapping from the label space Yto those words in the vocabulary, which constructs the label word setV. The base component of Mproduces the sequence representation over ˆx, and we choose the hidden vector at the [MASK] position as the contextual representation hˆx∈Rd, where dis the dimension of hidden states. Then the MLM head of M can operate on hˆxto calculate each word v’s probability in the vocabulary being filled in [MASK] PM([MASK] =v|ˆx). We let Vyto represent the subset of Vwhich is connected with a unique label y,∪y∈YVy=V. Finally, the probability distribution over the label yis calculated as: ] =v|T(x))|v∈ Vy), (2) where grefers to the function converting the probability of label words to the probability of classes. 3 R ETRO PROMPT : Prompt Learning We introduce a simple and general framework for prompt learning, named RETRO PROMPT , whose basis is the dense retriever (§3.1) with an open-book to decouple knowledge from memorization. As shown in Figure 2, RETRO PROMPT consists of three components: retrieval of neural demonstration for enhancing input (§3.2), the kNN guided training (§3.3) and the kNN-based probability for cloze-style prediction (§3.4). 3.1 Dense Retriever Open-book The first step of our proposed framework is to build a knowledge- store for retrieval that can decouple from memorization and captures the semantics of the instance from the training set C. Specifically, we leverage the encoder to embed instance representation over the Cto construct the . Given the i-th example (ci, yi)in the training data C, we obtain the key-value pair (hˆci, vi), in which the embedding of the[MASK] token in the last layer of the PLM, and the label word of the i-th example. Compared with kNN-LM [ 26] that constructing with sliding generative corpus and tokens, our is more suitable for prompt learning. We store all pairs (hˆc, v)in a key-value datastore (K,V)where hˆcserves as keyandvasvalue as follows: (K,V) ={(hˆci, vi)|(ci, yi)∈ C} (3) The maye be flexible to edit, add or delete any instances and can be asynchronously updated during the training procedure. Note that our is constructed from few-shot trainsets in the corresponding few-shot settings rather than the whole available training data. Efficient Searching Considering that the size of the training data Ccan be enormous, we must ensure an efficient retrieval process. As shown in the above creation of open-book , 3 we can build the matrix D∈R|C|×das the index of training examples. Given a query set Q, we first encode each query example with template mapping function T(·)to get a set of prompt-based query vectors hˆqfor retrieval augmentation on the fly. Then, we utilize query vectors to search for the closest examples over the index Dvia maximum inner product search (MIPS). For the retrieval process, we choose FAISS [ 21] to query the open-book efficiently. FAISS is an excellent open-sourced toolkit for fast nearest neighbor retrieval. Asynchronous Refresh of the Since the neural demonstration may lead to the variable contextual representation of instance as the parameters of the PLM are continually updated, we thus propose to “refresh” the index of retrieval by asynchronously re-embedding and re-indexing all embeddings in an open-book every jtraining epochs4. In § 4.6, we empirically demonstrate that this procedure results in performance improvement. 3.2 Retrieval of Neural Demonstration To enhance the PLMs with the ability to learn by analogy through the , we further propose neural demonstrations that can be concatenated with input instance at the embedding layer to improve the generalization ability of our RETRO PROMPT . For the t-th query instance qt, we first utilize prompt-based representation hˆqtto query the cached of open-book knowledge- store. Then we retrieve mnearest neighbors {{c(1) 1, ...,c(1) m}, ...,{c(L) 1, ...,c(L) m}}ofqtfor each class, where the superscript Ldenotes the total number of the classes and the c(l) iis retrieved as the i-th nearest neighbor in the l-th class. After the model retrieves the Top- mcandidates for each class, their corresponding representation h(l) ˆciand label word v(l)from will be incorporated into the encoder to act as a demonstration learning. Since the h(l) ˆciis already vector, we intuitively aggregate the mneighbor vectors for each class according to their similarity and incorporate the demonstration into the input representation of ˆxafter the word embedding layer of the Mas follows: I=e(ˆx)⊕[X i∈[1:m]α(1) ih(1) ˆci, i∈[1:m]α(L) ih(L) ˆci, e(v(L))];α(l) i=ehˆq·h(l) ˆci P ) ˆci(4) where e(·)represents the word embedding layer of M,⊕denotes the concatenation of input se- quences, α(l) iis the softmax score for the i-th retrieval belonging to l-th class label to denote their relevance with ˆq, andIis the sequence features for inputting the next layer of PLM. As shown in the above equation, we encode demonstration representation with the weighted sum of the retrieval representation. Thus, retrieval scores are directly used in the final representation, making the frame- work differentiable. To this end, we denote this style of demonstration as neural demonstration , significantly different from prior work of discrete demonstration [12]. Neural vs. Discrete Demonstration Compared with prior discrete demonstrations described in [12,35,49,28], retrieving weighted neural demonstrations from the to augment prompt learning has advantages in the following three major aspects: (1) neural demonstrations could be more tolerant of the model’s maximum input length than discrete demonstrations, while the discrete demonstration is usually not suitable for multi-class classification tasks due to the limitation of input length, such as relation extraction, etc. (2) the model needs to deal with large retrieval tokens for discrete demonstration, making it time-consuming and intensive to perform operations due to the quadratic attention complexity. In contrast, dealing with much shorter instance as neural demonstrations unleashes the potential of and accelerates the inference. (3) when sampling examples based on the similarity between instances, ourcloze-style contextual representation is more informative and consistent than the contextual representation from [CLS] of Sentence-BERT [47] (adopted in LM-BFF). 3.3 Retrieve kNN for Guiding Training Eager learners (e.g., PLMs) are optimized to learn a global function that maps from the text to semantic label space. Lazy learners such as k-nearest neighbor classifiers, on the contrary, aims to 4Specifically, we refresh the for each epoch in our experiments. 4 approximating the neighborhoods around those test examples [ 2]. Since kNN can easily predict for each encountered query instance based on pre-trained representation without an extra classifier, it is intuitively to leverage the kNN’s classification results as the prior external knowledge to guide the PLMs’ parameters attending to hard examples (hard samples usually refer to atypical samples) during the training process (also referred as kNN-train for the abbreviation). Particularly, our intuition is to differentiate between easy and hard examples according to the prediction of kNN. Given the t-th query instance qt, we leverage the hqtquerying the open-book (K,V)to retrieve the k-nearest neighbors Nofqtaccording to a similarity function d(·,·), where adopt the inner product similarity. Then, we compute the distribution over neighbors according to the softmax of their similarities and aggregate probability mass for each label word across its occurrences in the retrieved targets: PkNN(y|qt)∝X ( () Given the probability pkNNof the query instance qtbeing predicted as the gold class (also as the probability value of the gold class in the PkNN), we propose to retrieve the kNN for guiding the training process of prompt learning. The kNN guider reweights the cross-entropy loss LCEby adjusting the relative loss for the or misclassified instances identified by kNN, respectively. Specifically, we apply the negative log-likelihood as the modulating factor F(pkNN). The final loss Lis defined as: F(pkNN) =−log (pkNN),L= (1 + βF(pkNN))LCE, (6) where βdenotes a scalar to determine the proportion of each loss term. Note that pkNNis computed using the leave-one-out distribution on the training set due to the fact that each example in the training set cannot retrieve itself. The motivation of modulating factor is inspired by Focal-loss [ 34], while we focus on exploit leveraging k-NN’s results for calibrating the training of LMs. 3.4 kNN based probability for Cloze-style Prediction Apart from the neural demonstration on the input side and kNN guided training process (also referred askNN-test for the abbreviation), we further present kNN based probability for Cloze-style prediction on the inference process, providing the PLM ability to retrieve nearest neighbors for decisions rather than making predictions only based on memorized parameters. Given the non-parametric knearest neighbor distribution PkNNof the query instance qtbeing predicted as y, we follow [ 13,26,16] to reformulate the P(y|qt)by interpolating the PkNNwith the base PLM’s MLM prediction PMusing parameter λto produce the final probability of the label: P(y|qt) =λPkNN(y|qt) + (1 −λ)g(PM([MASK] =v|T(qt))). (7) Different from kNN-LM [ 26,16] that mainly retrieve tokens to augment the language modeling, we focus on leveraging prompt-based kNN’s distribution for reference at test time, which can unlock the model prediction process as an open-book examination for prompt learning. 4 Experiments 4.1 Datasets and Baselines Datasets We evaluate RETRO PROMPT on several types of natural language understanding tasks, including single sentence classification tasks (SST-2 [ 54], MR [ 43], and CR [ 19]) and sentence pair classification tasks (MNLI [ 58], QNLI [ 46], and QQP5). To further evaluate the effectiveness of the proposed approach with multi-class classification, we also conduct experiments on the information extraction tasks, including FewNERD [ 8], SemEval 2010 Task 8 (SemEval) [ 17], and TACRED [ 61]. The detailed statistics of the datasets are shown in Appendix A. Baselines We compare with LM-BFF [ 12] for single sentence and sentence pair classification tasks and adopt SOTA prompt learning model KnowPrompt [ 6] as the baseline for information extraction tasks. Note that the discrete demonstration method cannot be applied to multi-class classification tasks due to the input length limitations; thus, we leave out the experimental table about the results of KnPr (D-demo). We also compare our RETRO PROMPT with the prompt learning / . 5 Table 1: Results across 9 NLU datasets in the few-shot and zero-shot setting. We report mean (and standard deviation) results over five different few-shot splits. “D-demo” refers to discrete demonstration, and “KnPr” is the abbreviation of KnowPrompt. LOTClass [ 41] is the SOTA model in unsupervised text classification with self-training. †donates the model uses extra knowledge and ♣means they train the PLM on the whole unlabeled trainset, while we and the other baselines only leverage the vanilla PLM to test without training. The average scores with∗denote that we reuse the results of the “non-demo” version of the related model to fill in the default values. St. ModelSingle Sentence Sentence Pair Extraction Avg.SST-2 MR CR MNLI QNLI QQP FewN SemEval TACRED (acc) (acc) (acc) (acc) (acc) (F1) (acc) (acc) (F1) 16FT 81.4 (3.8) 76.9 (5.9) 75.8 (3.2) 45.8 (6.4) 60.2 (6.5) 60.7 (4.3) FT 52.7 (2.2) 66.1 (1.2) 25.8 (2.8) 60.6 LM-BFF (man) 91.6 (1.2 ) 87.0 (2.0) 90.3 (1.6) 64.3 (2.5) 64.6 (5.4 ) 65.4 (5.3) KnPr 65.3 (1.1) 80.9 (2.5) 33.2 (2.0) 71.4 LM-BFF (D-demo) 91.8 (1.2 ) 86.6 (1.8) 90.2 (1.4) 64.8 (2.3) 69.2 (5.4) 68.2 (3.2) KnPr (D-demo) — — — 72.2∗ KPT † 90.3 (1.6) 86.8 (1.8) 88.8 (3.7) 61.4 (2.1) 61.5 (2.8) 71.6 (2.7) KPT † 65.9 (1.5) 78.8 (2.1) 32.8 (1.7) 70.9 Ours 93.9 (0.4) 88.0 (0.8) 91.9 (0.7) 71.1 (1.8) 71.6 (1.8) 74.0 (2.0) Ours 67.3 (0.9) 81.5 (1.3) 40.7 (0.7) 75.6 4FT 60.2 (2.8) 57.6 (1.4) 66.4 (5.5) 35.0 (0.3) 54.2 (3.9) 52.8 (4.7) FT 32.7 (2.9) 38.8 (2.0) 14.7 (2.8) 45.8 LM-BFF (man) 90.7 (0.8) 85.2 (2.8) 89.9 (1.8) 51.0 (2.5) 61.1 (6.1) 48.0 (4.9) KnPr 52.5 (1.5) 58.4 (3.7) 28.8 (2.5) 62.8 LM-BFF (D-demo) 90.2 (1.5) 85.5 (2.1) 89.7 (0.6) 56.1 (1.0) 61.7 (7.6) 63.2 (5.6) KnPr (D-demo) — — — 65.1∗ KPT † 88.2 (5.7) 83.4 (1.5) 87.2 (2.5) 53.7 (2.7) 59.2 (2.8) 54.9 (7.9) KPT † 58.8 (2.2) 57.2 (3.2) 27.5 (2.2) 63.3 Ours 91.5 (1.8) 87.4 (0.5) 91.4 (0.6) 57.6 (5.5) 62.2 (6.0) 66.1 (4.1) Ours 60.9 (1.9) 59.2 (3.0) 32.1 (2.0) 67.6 0LOTClass♣71.8 81.7 50.1 50.4 36.5 55.9 LOTClass♣11.5 9.8 2.5 41.1 FT 49.1 50.0 49.8 34.4 49.5 31.6 FT 10.0 6.2 0.5 31.2 LM-BFF (man) 83.5 80.3 78.4 49.7 50.5 49.7 KnPr 15.9 10.3 2.3 46.7 LM-BFF (D-demo) 82.9 80.7 81.4 52.2 53.5 44.0 KnPr (D-demo) — — — 47.0∗ KPT † 78.4 81.9 71.4 37.1 55.3 47.5 KPT † 24.6 11.6 0.8 45.7 Ours 86.8 83.5 79.7 53.7 56.2 56.7 Ours 41.3 12.2 2.8 52.5 method KPT [ 20] since KPT leverages the external knowledge base for enhancing prompt learning while we focus on utilizing internal trainsets as a . You can refer to Appendix B for the detailed introduction of baseline methods. 4.2 Evaluation protocols and details The experiments are implemented on 1 NVIDIA V100 and utilize Pytorch [ 44] as the base library. We adopt RoBERTa large[38] as the PLM and employ AdamW as the optimizer for all experiments. To mitigate the influence of diverse templates, we conduct baselines and RETRO PROMPT with the same templates for each dataset. We list the specific experimental settings and tuning retrieve parameters in Appendix C and D. As for few-shot and zero-shot experiments, we leverage different settings, respectively. Figure 3: Performance on Setting. We follow the few-shot setting of LM-BFF [ 12] to conduct 4-shot and 16-shot experiments and evaluate the average performance with a fixed set of seeds, Sseed, across several differ- ent sampled Dtrainfor each task. Note that our is constructed with the few-shot training set in this setting. Zero-shot Setting6.We leverage vanilla RoBERTa largefor all base- lines (except LOTClass [ 41]) to directly inference on the test set. To take advantage of retrieval mechanism, RETRO PROMPT follows LOTClass [ 41] to utilize unlabeled trainsets for retrieval. Specifi- cally, we take the vanilla RoBERTa largeto tag the pseudo labels on unlabeled trainset and create the open-book with the unlabeled trainsets and pseudo labels. Lastly, RETRO PROMPT make predictions on the test set based on the constructed datastore without tuning any of the model parameters . 4.3 Experimental Results Few-shot Results. As shown in Table 1, we find RETRO PROMPT consistently outperforms baseline method LM-BFF and KnowPrompt, both in 4-shot and 16-shot experiments. Especially for informa- tion extraction tasks with multiple classes, discrete demonstrations cannot be applied to the input due to the limited input sequence length, while our neural demonstration can also work and achieves 6Note that it is not a strict zero-shot sense. 6 improvement on these multi-class datasets. Moreover, RETRO PROMPT obtain better performance compared with KPT. Compared with KPT with external knowledge, we only focus on referencing the internal few-shot trainsets without visiting the external knowledge base. Besides, we observe that RETRO PROMPT has a relatively lower standard deviation than the baselines. The reason may lie that the retrieval mechanism can compensate for instabilities in parametric predictions. Table 2: Results of model generalization to new domains. Model Source Target Domain 16-shot MR SST-2 CR FT 76.9 71.4 64.7 LM-BFF (man) 87.0 88.9 86.9 LM-BFF (D-demo) 86.6 89.3 87.5 KPT 86.8 86.8 86.7 RETRO PROMPT 88.0 91.4 88.8 16-shot QQP MRPC RTE FT 60.7 43.7 48.0 LM-BFF (man) 65.4 20.9 65.5 LM-BFF (D-demo) 68.2 38.8 66.2 KPT 71.6 42.3 65.8 RETRO PROMPT 74.0 49.4 67.3Zero-shot Results. From Table 1, we also ob- serve that RETRO PROMPT achieves improvements in the zero-shot setting. Another notable point is that RETRO PROMPT performs even better than KPT in the zero-shot setting, revealing that exploring own data to decouple knowledge from memorization has more potential than leveraging external knowledge. More- over, we achieve superior performance to LOTClass even though we utilize the vanilla RoBERTa largewith- out any training. Results. As shown in Figure 3, the experiments in settings with long-tail distribution illustrate that RETRO PROMPT achieves improvement compared with baselines. This indicates that our retrieval mechanism extends the LM’s ability to learn hard examples in the fully- supervised datasets. 4.4 Model Generalization to New Domains The scarce data may bring the overfitting problem for the lots of memory parameters of PLMs, even though prompt learning. Thus, we conduct cross-domain experiments to validate the generalization of our RETRO PROMPT . Specifically, we utilize the model trained on the source datasets and directly test on the other target datasets. From Table 2, we can find that our method consistently outperforms baselines. This finding illustrates that RETRO PROMPT achieves great model generalization to new domains. 4.5 Analysis of Memorization It is necessary and interesting to further explore the memorization mechanism to help us better understand the utility of retrieval for memorization in NLP. Definition of Memorization Measurement. Inspired by the idea of [ 10] in the computer vision area, we define memorization measures as to how the classification varies when a training instance z is deleted from the trainset. We follow [ 27,62] to define and derive the memorization score for a training instance zas follows: ) dξ dξ ˆθ∇θL(z,ˆθ), (8) where ˆθξ,−zdenotes the parameters trained with the instance zdown-weighted by ξ,ˆθrefers to the parameters of the model trained with all instances and Hˆθ=1 nPn i=1∇2 θL(zi,ˆθ). Thus Sdelate(z) refers to the amount of change of P(y|x;θ)when the instance zis down-weighted by ξ. Top-memorized Instances: Typical or Atypical? Since the SST-2 dataset provides the annotations of phrase-level sentiment polarity labels, we adopt SST-2 to analyze the memorization by judging the atypical of an instance by checking the percentage of positive phrases. We achieve such statistics from SST-2 and observe that a typical positive instance has a relatively high percentage of positive phrases, and a typical negative instance should have a relatively low percentage of positive phrases. Based on the above observation, we apply the memorization score defined in Eq. 8 to select Top-10% and Bottom-10% memorized instances from the trainset and collect the average percentage of positive phrases in these instances. As shown in Table 3,we can conclude following findings: (1) The PLM tends to give atypical samples deeper memory attention. Specifically, no matter LM-BFF or our method, the top- 7 Table 3: The upper part shows the average percentage of positive phrases over different memory groups of instances. The lower part denotes the mean values of memorization score on the SST-2 dataset. Mem GroupNegative Postive FT LM-BFF OURS FT LM-BFF OURS Top-10% 34.29 32.78 30.23 68.75 69.71 75.67 ALL 23.40 86.39 Bottom-10% 17.63 16.25 14.42 95.92 95.08 94.53 FT LM-BFF OURS MEMSCORE 4.597 0.121 0.032 10% memorized negative instances have a higher percentage of positive phrases than the average percentage of positive phrases of all negative instances. 2) LM-BFF has lower memorization scores on hard samples than fine-tuning. We think it owns to prompt learning can help PLMs recall what they learned from pre-training without strengthening memory for downstream data. 3) RETRO PROMPT further has lower average memorization scores than fine-tuning and LM-BFF, which illustrates that our method is less memory dependent. This result may be attributed to decoupling knowledge from memorization through retrieval to alleviating the rote of PLMs. Table 4: Detailed ablation experiments in few-shot settings. “N-demo” donates the neu- ral demonstration, and “refresh” refers to the asynchronous refresh of the knowledge-tore. Model16-shot SST-2 CR MNLI QQP TACRED OURS 93.9 91.9 71.1 74.0 40.7 w/okNN-test 93.2 91.2 70.4 73.0 38.2 w/okNN-train 92.0 90.2 68.8 71.3 36.5 w/o N-demo 92.4 91.0 70.1 72.7 37.9 w/o refresh 93.5 91.5 70.7 73.6 39.9Case Analysis. As shown in Table 6, we manu- ally list the bottom-ranked and top-ranked training instances of SST-2 according to our model. It re- veals that the top-ranked memorized instances seem to show universal opinions indirectly. Thus, we in- spect them as atypical/hard for sentiment classifica- tion. While those instances with 0 memorization scores are to show their opinion for sentiment classification, representing the typical in- stance. Note that F(pkNN)is defined to represent the difficulty of the sample discriminated by kNN dis- tribution. And the Table 6 also shows that F(pkNN) indeed reflect atypicality of examples, which vali- dates the effectiveness of the kNN guided training. 4.6 Ablation Study Component Ablation. As shown in Table 4, the performance of component ablation experiments with four variants has a clear drop, which validates the power of our retrieval component. We also find that neural demonstration and kNN-train have more improvement in the few-shot setting than kNN-test. Note that kNN-test is similar to kNN-LM [ 26,16] and the results reveals that simply incorporate kNN in the test process of prompt learning has little influence in a few-shot setting. Table 5: Performance on 16-shot CR and TACRED with different represen- tations of key and calculate function ofkNN distribution. Key Repres. kNN Acq. CR TAC. Prompt Rep-similar 91.9 40.7 [CLS] Rep-similar 89.0 37.2 Prompt BM25 89.5 38.8 [CLS] BM25 88.7 36.1Key Representation and kNN Acquisition. We study the effect of using different of the key in the . We experiment with two types of repre- sentations: (1) prompt-based representation, which is the default setting, and (2) [CLS] based representation of current LM. We also experiment with two types of calculation of kNN distribution: (1) representation based similarity score (refer as rep-similar), which is the default setting, and (2) BM25 based score , which calculates the correlation score between the query and each key examples with BM25 [ 48] algorithm. Results in Table 5 show that using prompt-based for key and representation based similarity scores for kNN leads to the best performance. It suggests that prompt learn better for context similarity and the representation similarity based kNN distribution is better than BM25 based scores. 8 Table 6: Case examples of Top-3 and Bottom-3 memorized instance of ours from trainset of SST-2. Negative Positive Content Mem F(pkNN)Content Mem F(pkNN) Although godisgreat addressed interesting matters ofidentity andheritage, it’shard to shake thefeeling thatitwas intend tobea different kind offilm.0.066 1.17Ab-movie , enjoy ona certain level andthen forget.0.020 0.18 Astandard drama that, were participation, would have likely wound 1.48Afilm thatwillbebest appreciated bythose willing toendure itsextremely languorous rhythms, waiting forhappiness isultimately thoughtful without having much dramatic impact.0.010 0.43 Ahitandmiss affair, consistently amusing orfunny aschomay have intended asonemight have hoped.0.010 2.74What’s invigorating about isthat itdoesn’t give adamn.0.003 0.06 It’saloathsome movie, itreally isand it makes absolutely nosense.0.00 0.00Afunfamily movie that’s suitable forall ages– amovie thatwillmake youlaugh, cry andrealize, ‘it’s never in your dreams.’0.00 0.00 Itisthat rare combination ofbad writing, baddirection andbadacting –thetrifecta of badness.0.00 0.00 It’sacool event forthewhole family. 0.00 0.00 This thing isvirtually unwatchable. 0.00 0.00Good fun, good action, good acting, good dialogue, good pace, good 0.00 5 Related Work PLMs. Our pipeline is partly inspired by discrete demonstration methods such as [ 12,35,28,29] that retrieves few training examples in a natural language prompt, while we propose neural demonstration for enhancing the input to alleviate the limitations of input length. Another line researches of retrieval augmentation [ 14,23,32,49,3] retrieve useful information from a external knowledge corpus (e.g., Wikipedia) for a particular task (e.g., an open-domain question). Unlike these works, we focus on retrieving examples from the internal training data. Besides, semi- parametric methods [ ] have risen to leverage k-nearest neighbor classifier, a classic non-parametric algorithm that makes the prediction based on representation similarities, to enhance pre-trained language models in various tasks However, unlike these models using nearest neighbors only for augmenting the process of prediction, we aim to develop a comprehensive retrieval mechanism for input, training and test process. Prompt learning for PLMs. With the birth of GPT-3 [ 4], prompt learning [ 36] has recently arisen to fill the gap between masked LM objective of PLMs and downstream fine-tuning objective. Prompt learning has achieves very impressive performance on various tasks, such as text classification [ 51,53], named entity recognition [ 5,40], relation extraction [ 15,6], event extraction [ 18,60], machine translation [ 55] and language generation [ 52], especially under the setting of few-shot learning. Moreover, continuous prompts have also been proposed [ 33,30,37] to reduce prompt engineering, which directly appends a series of learnable continuous embeddings as prompts into the input sequence. Our work is orthogonal to previous prompt learning approaches, which aim to optimize prompts, while we focus on the systematic study of retrieving related examples from training data to enhance prompt learning. 6 Conclusion and Future Work We propose RETRO PROMPT that decouples knowledge from memorization by introducing retrieval augmentation to further improve the generalization ability of prompt learning on the input side and the whole process of model training and prediction. RETRO PROMPT , is a yet effective retrieval method that combines both neural demonstrations, kNN guider for training and prediction. Our extensive results show that it outperforms other prompt methods and prompt methods in few-shot, zero-shot and settings. Analyzing the essence of memorization validates the effectiveness of decoupling knowledge from memorization. Interesting future directions include: 1) apply to other tasks, such as QA and NLG, 2) explore the noise data mining for unsupervised learning, 3) further improve the retrieve efficiency for large datasets, etc. 9  We want to express gratitude to the anonymous reviewers for their kind comments. This work was supported by National Natural Science Foundation of China (No.62206246, 91846204 and U19B2027), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), and Yongjiang Talent Introduction Programme (2021A-156-G).  
fuzzy learning machine 	Fuzzy Learning Machine Junbiao Cui1 Liang1∗ ljy@sxu.edu.cn 1Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, School of Computer and Information Technology, Shanxi University, Taiyuan 030006, Shanxi, China. Abstract Classiﬁcation is one of the most important problems in machine learning and the nature of it is concept cognition. So far, dozens of different classiﬁers have been designed. Although their working mechanisms vary widely, few of them fully consider concept cognition. In this paper, a new learning machine, fuzzy learning machine (FLM), is proposed from the perspective of concept cognition. Inspired by cognitive science, its working mechanism is of strong . At the same time, FLM roots in set theory and fuzzy set theory, so FLM has a solid mathematical foundation. The systematic experimental results on a large number of data sets show that FLM can achieve excellent performance, even with the simple implementation. 1 Introduction Given an input space X, an output space (a ﬁnite set) Y, and an unknown function ϕ:X →Y , the goal of classiﬁcation is ﬁnding the ϕor an approximation to it. Classiﬁcation is one of the most important problems in machine learning, dating back to the origins of machine learning. In classiﬁcation, each member of the output space Ycorresponds to a concept. In essence, the process of classiﬁcation is the process of concept cognition. Concept contains our knowledge about the world, and we use concept to understand and organize the world. Without it, there will be no human intelligence at all. That is why classiﬁcation plays an important role in machine learning even in artiﬁcial intelligence. Over the past decades, a large number of classiﬁers have been designed and achieved success on different tasks. The literature [ 1] systematically summarizes 179 classiﬁers from 17 different families, which contains almost all existing classiﬁers. Usually, generalization performance is regarded as an important dimension for evaluating classiﬁers. With the increasing depth and breadth of machine learning applications, classiﬁers are expected to possess more and more good properties, such as [2, 3, 4], robustness [5, 6, 7], and so on. The existing classiﬁers follow different design principles (see Appendix A.2.1 for detailed analysis), but they are designed without full consideration for the nature of classiﬁcation and how humans do it. For a speciﬁc scenario, the existing classiﬁers often perform well on one evaluation dimension but poorly on others. For example, deep neural network has exceed the human level in very speciﬁc settings [ 8]. Unfortunately, its working mechanism is difﬁcult to be understood due to the complexity of the network structure [ 9,10], and it is also very vulnerable to be attacked [ 11,12]. Instead, human can perform well on a variety of tasks and on different evaluation dimensions. To implement a classiﬁer that is easy to understand and interpretable, one effective approach is to draw on relevant research in cognitive science. As mentioned above, classifying is essentially the ∗Corresponding author 36th Conference on Neural Information Processing Systems (NeurIPS 2022). process of concept cognition. In cognitive science, there is a lot of valuable theories about how human learn, represent and use concept [ 13]. However, few of the existing classiﬁers are designed based on these theories, which leads to that the existing classiﬁers are unable to take into account the performance on multiple evaluation dimensions. Inspires by this, we are encouraged to design a new classiﬁer based on concept cognition, which can simultaneously perform well on different evaluation dimensions. In this paper, a new learning machine is designed based on the following principles. Similarity is the basis of concept representation. Similarity is a fundamental concept within cognitive science and plays a crucial role in the process of human classiﬁcation [ 14]. Concepts are represented based on similarity for children, which is also a basic choice for adults [ 15]. Based on this principle, we prove the equivalence between classiﬁcation and equivalence relation (ER) via binary relation in set theory. Considering that ER is a special similarity, we actually demonstrate that all classiﬁcation problems can be solved based on similarity (see Section 2.1). Concept is fuzzy. An important conclusion in cognitive science is that fuzziness is the intrinsic property of concept [ 13]. The fuzziness and the randomness are the two most important sources of the uncertainty. In machine learning, probability theory is often used for dealing with the uncertainty, such as Bayesian methods [ 16,17], label distribution learning [ 18], etc. It is sufﬁcient for probability theory to deal with the randomness rather than the fuzziness [ 19]. However, the intrinsic property of concept is just the fuzziness rather than the randomness. An intuitive example is shown in Figure 1. Figure 1: The visual concepts of ‘cat’ and ‘dog’ are of fuzziness. In fuzzy set theory, the membership degree of the 3rd image to ‘cat’ (‘dog’) can be 0.6(0.5). In probability theory, the probability of the 3rd image being ‘cat’ (‘dog’) can be 0.6(1−0.6 = 0.4). Obviously, the latter is unreasonable, because the 3rd image can not be ‘cat’ in some case and be ‘dog’ in the other case. Based on this principle, fuzzy equivalence relation (FER) in fuzzy set theory [ 20] rather than ER is chosen as the core component of the new learning machine (see Section 2.2). Exemplar theory based concept representation In cognitive science, there are several theories of concept representation [ 13]. Exemplar theory is chosen to solve concept representation problem, in this paper. Because it not only has a good in cognitive science, but also it can be naturally integrated with the data-driven machine learning paradigm (see Section 2.3). To summarize, the main contributions of this paper are four-folds as below: (1) This paper revisits classiﬁcation problem from the perspective of concept cognition, and proves the equivalence between classiﬁcation problem and ER. Furthermore, considering that concept is intrinsically fuzzy, the classiﬁcation problem is modeled as an FER problem. (2) A new and general learning machine FLM is proposed based on concept cognition for solving FER problem. (3) A new fuzziness permissible loss is designed. Given the new loss, this paper prove that FER can be approximated effectively by fuzzy similarity relation (FSR). And then a neural network based FLM (NN-FLM) is designed. (4) Extensive experimental results demonstrate the superiority of NN-FLM on different evaluation dimensions including , robustness, and generalization performance. 2 Fuzzy Learning Machine Basic notations the input and output spaces, respectively. Let ϕ:X →Y be the unknown target function and ∀x∈X ,ϕ(x)∈Y is the true class label of the samplex. The goal of classiﬁcation is to approximate ϕas closely as possible. Let Dtrain= {(= , j=n+ 1,n+ 2,···,n+m}be the training and test data sets, respectively. 2 2.1 Classiﬁcation Problem Can Be Solved Based on Similarity Researches [ 14,15] on cognitive science prove that similarity is the basis of concept representation. However, the relationship between similarity and classiﬁcation (e.g. whether all classiﬁcation problems can be solved by using similarity) still lacks a mathematical justiﬁcation. This section will accomplish it. The mathematical tool used in this section is binary relation in set theory. See Appendix A.1.2 for a brief introduction. First, the formal deﬁnition of classiﬁcation problem is given. Deﬁnition 1 For an input space X, an output }, and an unknown target functionϕ:X→Y . The ( problem is deﬁned as: ∀x∈X, ﬁndingϕ(x). It should be pointed out that the Deﬁnition 1 is so general that almost all classiﬁcation problems studied in machine learning can be represented by it. Deﬁnition 2 Given a ( problem, (1) let X×X be the new input space, (2) let{0,1}be the new output space, (3) let ϕ†:X×X→{ 0,1}be the new target function, where ∀()) =I(ϕ(xi) =ϕ(xj)), then ﬁnding the ϕ†is deﬁned as its adjoint( X×X,{0,1},ϕ†) -classiﬁcation problem. Obviously, the ϕ†is an equivalence relation (ER) on X(seeDeﬁnition 7 inAppendix A.1.2) and the adjoint( X×X,{0,1},ϕ†) -classiﬁcation problem is a problem. Based on the above deﬁnitions, we have the following proposition. Proposition 1 problem is equivalent to its adjoint( X×X,{0,1},ϕ†) - classiﬁcation problem, i.e. if one problem is solved, then the other problem will also be solved. (See Appendix A.3.1 for proof) The Proposition 1 establishes the relationship between ER and classiﬁcation problem. For problem, we can always solve it equivalently by solving its adjoint( X×X,{0,1},ϕ†) -classiﬁcation problem. Because ER ϕ†:X×X→{ 0,1}is a special similarity onX. Therefore, we can solve any classiﬁcation problem by ﬁnding a appropriate similarity. 2.2 Capture Fuzziness of Concept The mathematical tools used in this section are fuzzy set theory and binary fuzzy relation. See Appendix A.1.3 for a brief introduction. A large number of theories and experiments in cognitive science have shown that fuzziness is an intrinsic property of concepts [ 13]. A short mathematical argument is as follows. For any (X,Y,ϕ)- classiﬁcation problem, we have |X|>|Y|. , such thatϕ(xi) =ϕ(xj). The membership degrees of xiandxjto the concept ϕ(xi)are equal even if xiandxjmay be very different. When fuzziness of concept is introduced, the dilemma can be solved directly. Because xiandxjare different, their membership degrees to the concept ϕ(xi)are different. As shown in Figure 1, the membership degree of the 1st image to ‘cat’ is greater than the 3rd image, and the membership degree of 3rd image to ‘dog’ is less than the 6th image. TheSection 2.1 concludes that any classiﬁcation problem can be solved based on the special similarity, i.e.∀xi,xj∈X, either they belong to the same concept then ϕ†((xi,xj)) = 1 , or they don’t belong to the same concept then ϕ†((xi,xj)) = 0 . Obviously, this assumption is hard to satisfy in real world because it does not capture the intrinsic fuzziness of concept. Based on the above analysis, a new mathematical tool is needed to effectively deal with the fuzziness of concept. Fortunately, an effective solution is found in the fuzzy set theory that is introduced by Lotﬁ A. Zadeh [ 20] in 1965. In it, the crisp deﬁnition of the set is extended to a fuzzy one. Speciﬁcally, in set theory, the membership degree of an object to a set is crisp, i.e. either it belongs to the set then membership degree is 1, or it does not belong to the set then the membership degree is 0. Instead, in fuzzy set theory, the membership degree of an object to a set is fuzzy, which is represented by a real number in the interval [ 11 in Appendix A.1.3 and Figure 1). To capture the fuzziness of concept, ER in Deﬁnition 2 is replaced by FER (see Deﬁnition 16 and Theorem 3 inAppendix A.1.3), which leads to the following deﬁnition. 3 Deﬁnition 3 Given a ( problem, (1) let X×X be the new input space, (2) let [0,1]be the new output space, (3) let ϕ‡:X×X→ [0,1]be the new target function, where ϕ‡is an FER onX, ,ϕ‡((xi,xj))be the degree that xiandxjbelong to the same concept. Then ﬁnding the ϕ‡is deﬁned as its adjoint( X×X,[0,1],ϕ‡) -FER problem. To solve the ( problem, it is more reasonable to solve the( X×X,[0,1],ϕ‡) - FER problem than the( X×X,{0,1},ϕ†) -classiﬁcation problem. Because it is fuzzy whether two samples belong to the same concept in the former. This assumption ﬁts well with the principle that concept is fuzzy. For a learning task, solving the( X×X,[0,1],ϕ‡) -FER problem is ﬁnding an FER onXthat can approximate ϕby usingDtrain. The process can be formally described as follows f∗= arg min fL(Dtrain,f) + is an FER onX}, (1) whereLis the loss function that measures how well the fﬁts the training data set Dtrain,Ris the regularization term, and γ >0is the tradeoff parameter. 2.3 Exemplar Theory Based Concept Representation The main knowledge used in this section is concept representation theory. See Appendix A.1.1 for a brief introduction. Suppose we have obtained an FER f∗(the optimal solution of formula (1)) by usingDtrain. From the perspective of cognitive science, we have constructed the basis used for representing all concepts in Y. Next, we need to complete concept representation based on the basis. In cognitive science, exemplar theory is about concept representation [ 13]. Its core idea is that human represents a concept by remembering some objects that belong to it. Therefore, we need to ﬁnd some exemplars that can represent every concept in Y. In cognitive science, which object is suitable as an exemplar of a concept is still an open question, but it is an unequivocal fact that the similarity is the key of exemplar theory [ 13]. In a data-driven learning task, the exemplar set of every concept can be determined by the following deﬁnition. Deﬁnition 4 Given a ( problem and a training data set Dtrain. Let f∗be the FER found by using Dtrain.∀c∈Y, letXc }. And∀x∈Xc train, letµ(x,c) =1 |Xc train|∑ xi∈Xc trainf∗((x,xi)). Then,∀c∈Y, the exemplar set is deﬁned as Ec={x|x∈Xc train,µ(x,c) is the top−nc exelargest value train}}, wherenc exeis a manually speciﬁed parameter. The above deﬁnition is intuitive. Take Figure 1 as an example. Among the 3 images of ‘cat’, the 1st image is more suitable as an exemplar of concept ‘cat’ than the other 2 images, because the 1st image is more similar to the 3 images of ‘cat’ than the other 2 images. After obtaining the exemplar set of every concept, the predicting process can be described as ˆy= arg max c∈Y1 |Ec|∑ . (2) 2.4 The Framework of FLM Combining the Sections 2.1-2.3, the training and test processes of FLM are given as follows. Algorithm 1: The training process of FLM Input : The training data set Dtrain, the number of exemplars of every class nc exe,∀c∈Y. Find f∗by solving optimization problem (1). forc∈Y do Compute the exemplar set Ecof classcby Deﬁnition 4. end Output : 2: The test process of FLM Input : The test data set Dtest, the learned f∗, and the exemplar set Ec,∀c∈Y. forxj∈Dtestdo Compute the predicting label ˆyjof sample xjby formula (2). end Output :(. 4 2.5 Relationship to the Existing Methods The existing classiﬁers [ 1] aim to learn a function f :X →Y , while FLM aims to learn an FER fFER:X×X→ [0,1]. Therefore, the existing classiﬁers and FLM solve classiﬁcation problem from different perspectives. FLM is designed based on concept cognition, while most existing classiﬁers seldom fully consider concept cognition. There is also a large corpus fuzzy classiﬁers [ 21] that use fuzzy set theory for classiﬁcation task. Compared with most existing fuzzy classiﬁers, FLM is more suitable for data-driven learning tasks. The distance metric learning [ 22,23,24], the siamese network [ 25,26,27], and the relation network based methods [ 28,29,30] both aim to learn a similarity s :X×X→ R. And FLM aims to learn an FER fFER:X×X→ [0,1]. On the one hand, the fFER is a special similarity. So these three methods and FLM are similar in this respect. On the other hand, the fFER needs to satisfy four properties (i.e. normalization, reﬂexivity, symmetry, and transitivity), while the susually cannot satisfy these properties at the same time. Therefore these methods can not fully capture the nature of classiﬁcation. The detailed analysis about the above conclusions can be found in Appendix A.2. 3 NN-FLM: A Neural Network Based FLM on Rd In this section, the most popular input space, X=Rd, is taken into account. And the goal is to instantiate optimization problem (1)and solve it. The mathematical tools used in this section are binary fuzzy relation and the . See Appendix A.1.3 for a brief introduction. And the code implementation is provided in supplementary material. 3.1 The Separability of Adjoint( Rd×Rd,{0,1},ϕ†) -Classiﬁcation Problem To solve the adjoint( Rd×Rd,{0,1},ϕ†) -classiﬁcation problem described in the Deﬁnition 2, classiﬁers need to deal with pairs of samples on Rd. One of the simplest methods is to concatenate the feature vectors of a pair of samples, i.e. Dconcat train ={([}, (3) the concatenation of them. Then, a pair of samples can be converted into a composite sample, which can be processed by classiﬁers designed for R2d. However, the problem obtained by the above method is not linearly separable. See the following theorem. Theorem 1 For a( Rd,Y,ϕ) -classiﬁcation problem and a training data set Dtrain, if the sample is deﬁned as formula (3), then the corresponding( R2d,{0,1},ϕ†) -classiﬁcation problem is not linearly separable. (see Appendix A.3.2 for proof) Therefore, FLM on Rdmust be able to handle the linear inseparability of the training set. 3.2 The Design of NN-FLM To solve the original( Rd,Y,ϕ) -classiﬁcation problem, an FER on Rd(see Deﬁnition 3) should be found. It is very difﬁcult to directly obtain a binary fuzzy relation that satisﬁes transitivity. So an indirect method is adopted. First, construct a binary fuzzy relation that satisﬁes reﬂexivity and symmetry, i.e. FSR (see Deﬁnition 15 in Appendix A.1.3). Then convert the FSR into FER. To fully implement FLM, the following requirements must be considered. (a) The linear inseparability of the problem (see Theorem 1). (b) The learned function must be an FER, i.e. the learned function is normalized, reﬂexive, symmetric, and transitive. (c) The loss function must consider the fuzziness of concept. (d) An effective optimizer must be designed, and it should be efﬁcient especially for large scale training data sets. Based on the above analysis, NN-FLM includes three components: (1) Fuzzy similarity relation network, which responds to (a) and the ﬁrst 3 conditions in (b), (2) Fuzziness permissible loss, which responds to (c) and indirectly satisﬁes the transitivity in (b), and (3) Stochastic gradient descent based optimizer, which responds to (d). An overall design of NN-FLM is shown in Figure 2. 5 Binary Fuzzy Relation NetworkFeature Extraction Network Forward propagation Backward Permissible Loss Fuzzy Similarity Relation on Input Space Proximate Fuzzy Equivalence Relation on Training Data Set =?Figure 2: The overall design of NN-FLM. (1) Fuzzy similarity relation network contains two parts: feature extraction network and binary fuzzy relation network. First, the feature extraction network can be formally described as follows ∀x∈Rd,h (x; Θ)∈Rdh +, (4) where R+is the nonnegative real numbers, dhis the dimension of the latent space, and Θis the set of learnable parameters. A multi-layer neural network with nonlinear activation function is recommended, to deal with the linear inseparability of the problem. Second, the cosine similarity is used as the skeleton of the binary fuzzy relation network, i.e. ∀hi,hj∈Rdh +,g (hi,hj) =hT ihj ∥hi∥2∥hj∥2.Letf ((xi,xj) ; Θ) = g (h ( xi; Θ),h (xj; Θ)),∀xi,xj∈Rd, be the composite of handg. Obviously, fis an FSR on Rd, i.e. fis normalized, reﬂexive, and symmetric. It is very difﬁcult to make fsatisfy transitivity on Rd. However, it is easy to make fsatisfy transitivity on the training sample set Xtrain ={}. Let S∈[0,1]n×n,sij= f ((xi,xj)),∀i,j= 1,2,···,n,be the FSR matrix on Xtrain (seeDeﬁnition 14 in Appendix A.1.3). And let T= tFSR2FER (S) (5) be the of S(see Deﬁnition 20,Theorem 4 and 5 in Appendix A.1.3). Then, T∈[0,1]n×nis an FER matrix on Xtrain. (2) Fuzziness permissible loss For a pair of training samples (, lettijbe the predicting FER value of (xi,xj)by formula (5). The ( loss is used to measure the difference between the predicting value and the true value and can be written as Lα,β(tij,yi,yj) ={ max{tij−α,0}, yi̸=yj max{β−tij,0}, yi=yj, (6) where 0≤α<0.5<β≤1are two . The gap β−α∈(0,1]is used for controlling the fuzziness degree of concept. And the greater the value is, the less the fuzziness will be. Therefore, the loss on the entire training data set is LFER=n∑ i=1n∑ ). (7) However, the time complexity of converting the FSR matrix Sinto the FER matrix TisO( n3log2n) (see Remark inAppendix A.1.3). In practice, it is not feasible to directly optimize the above loss, especially when the number of training samples nis large. Fortunately, due to the intrinsic property of the , we can indirectly control the loss (7) by optimizing the following loss LFSR=n∑ i=1n∑ ). (8) See the following theorem. Theorem 2 Given a training set Dtrain={ (= 1,2,···,n} and an }. Let S∈[0,1]n×n,sij= f (( xi,xj)),∀i,j= 1,2,···,n, be the FSR matrix on Xtrain, and T= tFSR2FER (S)be the of S.∀0≤α < 0.5< β≤1, if∑n i=1∑n ) = 0 , then∑n i=1∑n ) = 0 . (see Appendix A.3.3 for proof) 6 According to the Theorem 2, when the loss (8)reaches its minimum 0, the loss (7)also reaches its minimum 0. The loss (8)can be optimized without calculating the FER matrix on all training samples. Therefore, the loss (8) is used to complete the learning process, in this paper. (3) Stochastic gradient descent based optimizer To sum up, the learning process of NN-FLM can be written as min Θ1 n2n∑ i=1n∑ j=1Lα,β(f ((xi,xj) ; Θ),yi,yj) +γR(Θ). (9) The time complexity of calculating the entire FSR matrix Sis at leastO( n2) , which still has a large computational burden. Fortunately, to optimize the above loss, it is not necessary to calculate the entire matrix Sat the same time. So the above optimization problem can be solved efﬁciently using the stochastic gradient descent with mini-batch, even when nis large. 4 Experiments 4.1 An Example on MNIST Data Set Experimental settings In this section, the MNIST data set [ 31] is chosen to demonstrate the working mechanism of NN-FLM. In NN-FLM, a 5-layer convolutional neural network is used as the feature extraction network, and the fuzzy parameters are ﬁxed as α= 0.2,β= 0.8. See Appendix A.4.1 for more experimental details. analysis First, Figure 3a shows the FSR matrix predicted by NN-FLM on 10,000 test samples. The following observations can be seen from it. (1) The FSR value between samples from the same classes is signiﬁcantly larger than that of samples from different classes. (2) The FSR matrix exhibits an obvious block-diagonal structure, where each block corresponds to a class. Combining (1) and (2), we can conclude that NN-FLM can learn a high-quality FSR from the training data. Based on the learned FSR, the concepts of 10 visual digits can be accurately represented. Second, Figure 3b shows the exemplars selected from the training set according to Deﬁnition 4. The following observations can be seen from it. (1) The 5 exemplars of every concept are regular, with no interrupted strokes, no distortions, parts of the digits, and the digits are located in the center of the image. It shows that the exemplars of the every concept selected by NN-FLM can accurately capture the of every concept. (2) There are certain changes among the 5 exemplars of every concept, which ensures that the exemplars can cover the intra-concept variations. In summary, we can conclude that NN-FLM can learn high-quality FSR from training data. And then based on learned FSR, the high-quality exemplars can be selected to accurately represent the corresponding concept. (a) (b) (c) Figure 3: (a) The predicting FSR matrix on the test set (the samples are sorted by class labels). (b) The exemplars of 10 classes selected from the training set. (c) The 31 errors of NN-FLM, with annotated label (up right) and top-2 or top-3 predicting labels (down right). Robustness analysis A total of 31test samples are misclassiﬁed and they are shown in Figure 3c. The following conclusions can be drawn from it. (1) All the misclassiﬁed samples are irregular digital images. (2) Among them, 26images ( 3images) are hit by the 2nd (3rd) class label predicted by NN-FLM. (3) For the 2 images (1st and 3rd in row 3 of Figure 3c), none of the top-3 class labels predicted by NN-FLM hits the annotated label. These 31images are so irregular that it is difﬁcult 7 to give crisp and labels even for humans. In this case, the class labels predicted by NN-FLM are in line with human cognition to a certain degree. In summary, we can conclude that NN-FLM can adequately capture fuzziness of concept. Therefore, the samples with a high degree of fuzziness can be identiﬁed and assigned reasonable candidate concepts by NN-FLM. This veriﬁes that NN-FLM is robust to the controversial samples. 4.2 Comparison With 179 Classiﬁers on 121 Data Sets Experimental settings In the literature [ 1], 179 classiﬁers from 17 families (see Table 3 in Appendix A.4.2) are systematically compared on 121 benchmark data sets (see Table 2 in Appendix A.4.2). In order to verify the generalization performance of FLM, NN-FLM is chosen and compared with the above 179 classiﬁers. In all experiments, NN-FLM adopts a 3-layer fully connected network as the feature extraction network. The fuzzy parameters are ﬁxed as α= 0.2,β= 0.8. More details about the experimental settings can been found in Appendix A.4.2. Table 1: The rank of NN-FLM among 180 classiﬁers on 121 data sets ID Rank ID Rank ID Rank ID Rank ID Rank ID Rank 1 1 21 1 41 1 61 2 81 10 101 21 2∗1 22 1 42 1 62∗2 82 10.5 102 23 3 1 23 1 43 1 63 2.5 83 11 103∗23.5 4 1 24 1 44 1.5 64∗2.5 84 12 104 24.5 5 1 25 1 45 1.5 65 3 85 13 105 26 6 1 26 1 46 1.5 66 3 86 13.5 106 26 7 1 27 1 47 1.5 67 3 87 14 107 29 8 1 28 1 48∗1.5 68 3 88∗14.5 108 31.5 9 1 29 1 49∗1.5 69 3.5 89 15 109 32 10 1 30 1 50 1.5 70∗4 90 15 110 35 11 1 31 1 51 1.5 71 4.5 91∗15 111 36 12 1 32 1 52 1.5 72 5 92 15.5 112 38 13 1 33 1 53 1.5 73 5.5 93 15.5 113 39 14 1 34 1 54∗2 74 6 94 16 114 39 15 1 35 1 55 2 75 6.5 95 16 115 39 16 1 36∗1 56 2 76 7.5 96 16.5 116 42 17 1 37 1 57 2 77 8 97 17.5 117 47 18 1 38 1 58 2 78 9 98 18.5 118 60 19 1 39 1 59∗2 79 9 99 19 119 62 20 1 40 1 60 2 80 10 100 20 120 63.5 121 65 (a) ID: The unique ID of every data set (see Table 2 in Appendix A.4.2). (b)∗: The 12 small sample data sets (see Table 5 in Appendix A.4.2). (c) The rank considers that two or more classiﬁers have the same test accuracy. For example, on data set 121 there are 129 classiﬁers with 100% test accuracy, so the ranks of these 129 classiﬁers are(∑129 r=1r) /129 = 65 . The top-10 classiﬁers The 10 classiﬁers with the smallest mean rank among 180 classiﬁers on 121 data sets are selected for comparison analysis. They are NN-FLM (1st), 3 classiﬁers from the random forest family (2nd parRF-t, 3rd rf-t, 5th rforest-R), 3 classiﬁers from the support vector machine family (4th svm-C, 6th svmPoly-t, 8th svmRadCost-t), 1 classiﬁer from the decision tree family (7th C5.0-t), and 2 classiﬁers from the neural network family (9th elm-kernel-m, 10th avNNet-t). Accuracy analysis The test accuracy of the 179 comparison methods are from the literature [ 1]. We run NN-FLM on 121 data sets and record the test accuracy. Then the accuracy of 180 classiﬁers are used for evaluating their performance. The following conclusions can be drawn from it. (1) Due to space constraint, the accuracy of NN-FLM and other top-9 comparison methods are shown in Table 4 inAppendix A.4.2. And it can be seen that the mean accuracy on 121 data sets of NN-FLM is 5.49% higher than the 2nd parRF-t and 6.00% higher than the 3rd rf-t. Therefore, NN-FLM achieves a signiﬁcant improvement compared with the 179 classiﬁers. (2) The rank of NN-FLM among 180 8 methods on 121 data sets is recorded in Table 1. And it can be seen that NN-FLM ranks ﬁrst on 43 data sets (data sets 1-43) and ties for ﬁrst place on 10 data sets (data sets 44-53), and ranks in the top-3 on more than half of the data sets (data sets 1-68). Therefore, NN-FLM achieves outstanding rank among the the 180 classiﬁers. Comparative advantage analysis Figure 4a shows the proportion of the top-10 classiﬁers ranked in top-K(K= 1,3,5,10,15,20) among the 121 data sets. And Figure 4b shows the mean and standard deviation of the rank of the top-10 classiﬁers on 121 data sets. The following conclusions can be drawn from it. (1) NN-FLM ranks the top-1 on 43/121≈35.5%of the data sets and the top-3 the data sets. And NN-FLM ranks in top-20 on more that 80% of the data sets. (2) None of the other 9 comparison classiﬁers ranks in top-20 on more than 50% of the data sets. (3) NN-FLM has the smallest mean rank 10.51. And the mean rank is signiﬁcantly superior to the second-ranked classiﬁer, parFR-t, 33.50. (4) NN-FLM has the smallest standard deviation 14.86. And the standard deviation is signiﬁcantly lower than to the other comparison classiﬁers. In summary, the performance of NN-FLM is signiﬁcantly superior to the comparison classiﬁers. At the same time, it can perform well on various tasks. In contrast, the comparison classiﬁers can only perform well on limited tasks. Therefore, the performance of NN-FLM is more stable than the others. The possible reason is that NN-FLM can adequately capture the nature of classiﬁcation. (%) top-3 top-5 top-10 top-15 top-20 (a) - (b) NN-FLMrf-t rforest-R ldaBag-Rrank - (c) Figure 4: The statistics on top-10 methods among 180 classiﬁers. (a) The proportion of top- Krank on 121 data sets. (b) The mean and standard deviation of the rank on 121 data sets. (c) The mean and standard deviation of the rank on 12 small data sets. Experiments on small sample data sets To verify the performance of NN-FLM on small sample classiﬁcation problem, we selected 12 data sets with small number of samples (see Table 5 in Appendix A.4.2) from the 121 data sets. In these data sets, the mean number of training samples for each class is less than 15. We selected the top-10 methods with the smallest average rank on the 12 small sample data sets for comparison, see Figure 4c. The following conclusions can be drawn from it. (1) Compared Figures 4b with 4c, it can be seen that the top-10 classiﬁers on the 12 small sample data sets differ from that on the all 121 data sets. Some classiﬁers (svmPoly-t, C5.0-t, svmRadCost-t, and avNNet-t) rank in the top-10 on the all data sets but not on the small sample data sets. (2) The mean rank of NN-FLM on the 12 small sample data sets is still the best. (3) The rank of NN-FLM is signiﬁcantly superior to the second-ranked method, fr-t ( ). In summary, it can be concluded that when there are fewer training samples, the superiority of NN-FLM is more signiﬁcant. The reason is that NN-FLM can capture the characteristic and intra-concept variations of concept through a small number of exemplars. Therefore, the exemplar theory based concept representation is reasonable. 5 Conclusion A new learning machine based on concept cognition, fuzzy learning machine, is proposed. The new learning machine has the advantages of strong and solid mathematical foundation. And it is of certain degree of robustness because it can effectively deal with the intrinsic fuzziness in classiﬁcation. At the same time, a large number of comparison experiments show that it can reach an advanced level even with a simple implementation. 9  and Disclosure of Funding This work is supported by the National Key Research and Development Program of China (under grant 2020AAA0106100).  
large scale retrieval for reinforcement learning 	Large-Scale Retrieval for Reinforcement Learning Peter C. Humphreys∗, Arthur Guez∗, Olivier Tieleman, Laurent Sifre, Théophane Weber, Timothy Lillicrap Deepmind, London {peterhumphreys, aguez, ...}@google.com Abstract Effective decision making involves ﬂexibly relating past experiences and relevant contextual information to a novel situation. In deep reinforcement learning (RL), the dominant paradigm is for an agent to amortise information that helps decision- making into its network weights via gradient descent on training losses. Here, we pursue an alternative approach in which agents can utilise large-scale context- sensitive database lookups to support their parametric computations. This allows agents to directly learn in an end-to-end manner to utilise relevant information to inform their outputs. In addition, new information can be attended to by the agent, without retraining, by simply augmenting the retrieval dataset. We study this approach for ofﬂine RL in 9x9 Go, a challenging game for which the vast combinatorial state space privileges generalisation over direct matching to past experiences. We leverage fast, approximate nearest neighbor techniques in order to retrieve relevant data from a set of tens of millions of expert demonstration states. Attending to this information provides a signiﬁcant boost to prediction accuracy and game-play performance over simply using these demonstrations as training trajectories, providing a compelling demonstration of the value of large-scale retrieval in ofﬂine RL agents. 1 Introduction How can reinforcement learning (RL) agents leverage relevant information to inform their decisions? Deep RL agents have typically been represented as a monolithic parametric function, trained to gradually amortise useful information from experience by gradient descent. This has been effec- tive [ 25,38], but is a slow means of integrating experience, with no way for an agent to incorporate new information without many additional gradient updates. In addition, this requires increasingly massive models (see e.g. [ 31]) as environments become more complex – scaling is driven by the dual role of the parametric function, which must support both computation and memorisation. Finally, this approach has a further drawback of particular relevance in RL – the only way in which previously encountered information (that is not contained in working memory) can aid decision making in a novel situation is indirectly through weight changes mediated by network losses. There is no end-to-end means for an agent to attend to information outside of working memory to directly inform its actions. While there has been a signiﬁcant amount of work focused on increasing the infor- mation available from previous experiences within an episode (e.g., recurrent networks, slot-based memory [ 19,28]), more extensive direct use of more general forms of experience or data has been limited, although some recent works have begun to explore utilising inter-episodic information from the same agent [ 6,10,29,34,41]. We seek to drastically expand the scale of information that is accessible to an agent, allowing it to attend to tens of millions of pieces of information, while learning in an end-to-end manner how to use this information for decision making. We view this as a ﬁrst step towards a vision in which an agent can ﬂexibly draw on diverse and large-scale information sources, ∗These authors contributed equally to this work. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). including its own (inter-episodic) experiences along with experiences from humans and other agents. In addition, given that retrieved information need not match the agent’s observation format, retrieval could enable agents to integrate information sources that have not typically been utilised such as videos [2], text, and third-person demonstrations. We investigate a agent architecture for large-scale retrieval, in which fast and efﬁcient approximate nearest neighbor matching is used to dynamically retrieve relevant information from a dataset of experience. We evaluate this approach in an ofﬂine RL setting for an environment with a combinatorial state space – the game of 9x9 Go with ≈1038possible games, where generalisation from past data to novel situations is challenging. We equip our agent with a large-scale dataset of ∼50M Go board-state observations, ﬁnding that a approach utilising this data is able to consistently and signiﬁcantly outperform a strong non-retrieval baseline. Several key advantages of this retrieval approach are worth highlighting: Instead of having to amortise all relevant information into its network weights, a network can utilise more of its capacity for computation. In addition, this form allows us to update the information available to the agent at evaluation time without having to retrain it. Strikingly, we ﬁnd that this enables improvements to agent performance without further training when games played against the evaluation opponent are added to its knowledge base. 2 Methods / / retrieval /– large-scale dataset /– retrieved neighbors and associated metadata / / Learn end-to-end how to use retrieved information Figure 1: Retrieving information to support deci- sion making. The agent observation otis used to generate a query to retrieve relevant information from a large-scale dataset Dr. This information is used to inform network outputs ˆyt. The agent is trained end-to-end to use this information to inform its introducing the details of our method, let us ﬁrst present its high-level ingredients. We train a model-based agent [ 36] that pre- dicts future policies and values conditioned on future actions in a given state. This semi- parametric model incorporates a retrieval mecha- nism, which allows it to utilise information from a large-scale dataset to inform its predictions. We train the agent in a supervised ofﬂine RL setting, which allows us to directly evaluate the degree to which retrieval improves the quality of the model predictions. We subsequently eval- uate the resulting model, augmented by Monte- Carlo tree search, against a reference opponent. This allows us to determine how these improve- ments translate to an effective acting policy for situations. To effectively improve model predictions using auxiliary information, we need 1) a scalable way to select and retrieve relevant data and 2) a ro- bust way of leveraging that data in our model. As with many successful attention mechanisms, we establish relevance for 1) through an inner product in an appropriate key-query space and select the top Nrelevant results. Choosing the relevant key-query space is critical, since perform- ing on the raw observations will only deliver desirable results for the simplest of domains. For example, in the game of Go, a single stone difference on the board can dramatically change the outcome and reading of the board, while seemingly different board positions might still share high-level (e.g. inﬂuence, partial local sequences, and life-and-death of groups). At the scale we want to operate, learning the key & query embeddings end-to-end in order to optimize the ﬁnal model predictions is challenging. Instead, as in the language modelling work by Borgeaud et al. [7], we learn an embedding function through a surrogate procedure and use the resulting frozen function to describe similarity. Moreover, scale also constrains the lookup to be approximate, since getting the true is too time-consuming at inference time and good approximations can be obtained. 2 To address 2) and make the best use of the relevant data, we provide the data associated with the as additional input features to the parametric part of our model, so that the network can learn to interpret it. We provide some light inductive biases in the architecture to ensure permutation invariance in the neighbors and robustness to outliers and distribution shifts (see Secs. 2.1.3 & 2.3). Letting the network interpret the data is essential in large and complex environments, such as Go, because the typically imperfect match between the current situation and retrieved neighbors will require different aspects of this retrieved data to be ignored or emphasized in a highly context speciﬁc manner. This is in contrast to episodic RL approaches like [ 6,29] which prescribe how to use the retrieved data. Subsequent sections describe the model, retrieval process, and the full algorithm in more detail. 2.1 agents We consider a setting in which we wish to train a neural network model mθon a set of environment (for example, the dataset Dπof trajectories produced by a policy π). For each timesteptofτ, the model takes an observation ot2and must produce predictions ˆyof targetsyt. In our retrieval paradigm, in addition to having access to ot, the model also has access to an auxiliary dataset of knowledge or experience Dr. The auxiliary set Drcould be the same as D, but it can also contain more information, or in the most general case, information from a completely different distribution and in a different format. If there is overlap between DrandD, it is important for robustness to ensure that the model cannot retrieve the same trajectory as it is being trained on (otherwise the network will tend to overly trust information retrieved from Dr). Common information sources in RL are trajectories from other agents or experts (ofﬂine RL) or the agent’s own previous experiences (episodic memory, replay buffer). Note that, in contrast to ofﬂine RL, we do not assume that we should directly use this auxiliary data as trajectories to train on.3 We wish to useDrto inform the model predictions ˆyt, such that ˆyt=mθ(ot,Dr). This is challenging, asDris typically far too large to directly be consumed as a model input. One solution, shown in Fig. 1, is to adopt a model, wherein the parametric and differentiable portion of the modelmθis provided with an informative subset of data {x1 t,x2 t,...,xN t}retrieved fromDr, conditioned on ot: ˆyt=mθ(ot,x1 t,x2 t,...,xN t). (1) This approach requires a number of design choices relating to the retrieval mechanism, which we explore further below. 2.1.1 Scalable retrieval using SCaNN Inspired by previous work in language modelling, we chose to leverage the SCaNN architecture [ 11] for fast approximate retrieval. This requires each entry oiin the datasetDrto be associated with a key vector ki∈Rd, and a given observation otto be mapped to a query vector qt∈Rd. During retrieval, the squared Euclidean distances between qtand the dataset keys kiare used to determine which neighbors to retrieve – reminiscent of neural attention mechanisms. This retrieval process is very efﬁcient and can be scaled to datasets with billions of items. We will typically want to retrieve further associated meta-data, or context, for each neighbor. For example, if a neighbor is part of a trajectory, we would like to retrieve information about action choices and their consequences. The neighbor observation oi, together with its meta-data, forms the auxiliary input xi tto the model. The retrieval process is , which means that the query and key mappings cannot be trained end-to-end directly. Instead, in this ﬁrst study, we pre-train a non-retrieval prediction network me φon our experience dataset D(details in Sec. A.4). We then use this network to generate an embedding corresponding to a given observation otby retrieving the network activations from a speciﬁed layer of me φ. We use principal component analysis to compress these activations to a ddimensional vector representing this observation state. The embedding and projection step together form our key (& query4) ). 2For simplicity of notation, we omit the dependence on past observations for domains. 3For example, Drneeds not contain actions, rewards, or the full context used to select actions [9]. 4In this study, retrieval dataset entries and othave the same format, but this need not be the case in general. 3  – pretrained embedding network & PCA projection White wins White wins White wins Black wins / – query SCaNN – fast approximate retrieval – recurrent forward model / / / – 52M Go game states with keys – observation & neighbor encoding / / Figure 2: Details of the architecture used for a Go playing agent. A pre-trained network is used to generate a query to the current Go game state ot. This query is used for fast approximate retrieval using SCaNN. Retrieved neighbors xn tare processed using an invariant architecture, and used to inform an recurrent forward model that outputs game outcome predictions ˆvkand distributions over next actions ˆπk. We preprocess all of the observations in Drusinggφto produce corresponding keys. The resulting dataset of keys and observations is then used by ScaNN to retrieve neighbors given a query vector. For ofﬂine RL training with a ﬁxed training dataset, we can also preprocess the nearest neighbor lookups for all training dataset observations, i.e., qt=gφ(o)∀ot∈D. However, to actduring evaluation, we must do this lookup online for each new observation. In future experiments, we intend to incorporate end-to-end learning of the query vector [ 12] to improve agent performance. A future online RL pipeline will also require us to dynamically retrieve neighbors during training. 2.1.2 Policy optimization with retrieval The objective in our ofﬂine RL setting is to leverage the available data in order to optimize a policy πfor acting. Our proposed model (Eqn. 1) is compatible with several methods for policy optimization in this ofﬂine setting. Indeed, it can represent the actor the an ofﬂine actor-critic method [ 21], or the Q-value in a value iteration approach [ 32]. Here we focus on a model-based approach, inspired by MuZero [ 36], where the learned model is employed in a search procedure based on Monte-Carlo Tree Search (MCTS) [ 8]. This is motivated by the efﬁcacy of MuZero in the ofﬂine RL setting [37, 24]. Model-based search with retrieval Following MuZero, our prediction model mθis conditioned on the current observation otbut also on a sequence of future actions ⃗ . The modelmθis therefore redeﬁned as ˆyt=mθ(ot,⃗ at,x1 t,x2 t,...,xN t). The architecture for this retrieval prediction model is illustrated in Fig. 2. The model mθcan be decomposed into an encoding t,x2 t,...,xn t), which incorporates the observation and neighbor information, followed by iterative inference sk+1 t=hθ(sk t,at+k)to produce embeddings 4 sk tcorresponding to subsequent time steps (where s0 t=st). The model output ˆytis composed ofK+ 1value and policy distributions for the current and next t,ˆπk t}k=0...K.5 These outputs are obtained from their respective embeddings sk t. The target for value predictions is the game outcome and the targets for the policy predictions are the actions taken by the expert . The sample obtained by summing the K+ 1individual loss terms for value and policy predictions (see detail in App. A.2). The training procedure for this retrieval prediction model is summarized in Alg 1. Algorithm 1 Training model Input: Training datasetDand retrieval dataset Dr 1:Initialize parameter vectors θ,φ. 2:Train key/query network gφusingD. 3:Pre-compute ki=gφ(oi)for alloi∈Dr. 4:foreach gradient step do 5: Sample mini-batches from Dof(ot,yt). 6: For each element otof the batch, compute qt=gφ(ot). 7: FetchNneighbor keys with (approx.) smallest distance ||qt−ki||2 2for allki∈Dr. 8: Gather meta-data associated with these keys as x1 t,...xn t. 9: Compute model output ˆyt=mθ(ot,⃗ a,x1 t,...,xN t). 10: Compute and sum losses L(θ,ˆyt,yt). 11: Updateθbased on∇θL. 12:end for 13:outputθ,φ In order to act using the trained model, online search is used to generate an improved policy πs=Search (mθ,Dr). We use MCTS with a pUCT rule for internal action selection [ 35,40], carrying out per time step. Model-based search is implemented as follows: after retrieving the observation’s neighbors, the encoded state stis computed. Search is carried out from stby varying the input action sequence ⃗ afor each simulation and collecting the model outputs to update search statistics. Details of this process can be found in [ 36]. At the end of the search, the resulting policy πs(a|ot)can be sampled to select the next action. Due to the nature of the model supporting the search, introducing changes to Drwill have an immediate effect on the acting policy πs, even if the model parameters θremain ﬁxed. As we explore in Sec. 3.4, this provide a mechanism for fast adaption to new information or experiences. 2.1.3 Using neighbor information Several choices are possible for the network fθthat processes the observation and neighbors. Since this is not the main focus of this work, we chose a approach. We ﬁrst compute an embedding oe tof the observation ot. We then compute an embedding for each neighbor et i= pθ(oe t,xt i), using the same network pθfor all neighbors. These streams are combined in a permutation invariant way through a sum, and then concatenated with oe tto produce the ﬁnal embedding st, : ) = [oe t,∑N i=1et i√ N]. (2) 2.2 Evaluating retrieval in the domain of Go In order to test the utility of retrieval in RL, we wish to evaluate whether agents can effectively generalise between related but distinct experiences, as opposed to simply retrieving the outcome of a previous instance of an identical situation. This motivates our choice of Go as a domain. We focus on 9x9 Go, instead of full-scale 19x19 Go, as the less demanding computational costs of 9x9 experiments enable a more thorough analysis. Even for 9x9 Go, the state space of 1038possible games is vastly larger than the number of positions that we could hope to query. We collected a dataset of ∼3.5M expert 9x9 Go self-play games from an agent [ 40]. We randomly subsampled 15% of the positions from these games, leaving us with ∼50M board 5In cases where non-terminal rewards exist, we also output reward estimates for each model transition. 5 state observations. These, along with metadata on the game outcome, ﬁnal game board, and future actions (all encoded as input planes), form our retrieval dataset Dr. We chose to subsample as we hypothesised that this would reduce the chance of retrieving multiple neighbors from the same trajectory, and therefore boost the retrieved neighbor diversity. However, we did not perform ablations on this choice. A future solution would be to use a ﬁltering mechanism to reject neighbors from the same trajectory. In this initial work, training and retrieval datasets are the same – at least during training. During training, we split Drin two halves such that each game’s observations are only in one of the datasets. We retrieve neighbors for an observation otfrom the half it is not contained in. This is simply to avoid retrieving the same position as the query. Other ways to obtain the same effect could be devised. 2.3 Regularisation We wish to make our network robust to poor quality neighbors, in order to ensure that the network can perform well in settings for which there is lower overlap with the retrieval dataset than encountered in training. We therefore explore several techniques to improve network robustness to irrelevant neighbors. We randomly zero-out a subset of retrieved neighbors during training ("neighbor dropout"), and/or more adversarially, randomly replace a subset of retrieved neighbors with the neighbors of a different observation ("neighbor randomisation"). Inspired by [ 10], we also explore using a loss to regularise the embedding produced by the neighbor retrieval towards the embedding produced with the observation alone ("neighbor regularisation"). Further details are given in Sec. A.5. We carried out ablations of these techniques (Appendix Fig. 7), which show that neighbor randomi- sation is important in some contexts for maintaining performance, but that the others do not seem to have a signiﬁcant effect in our ﬁnal conﬁguration. The results reported in this study utilise all of these augmentations, as during the development process they had been found to slightly beneﬁt per- formance. Interestingly, as we explore in later sections, MCTS with enough simulations compensates for the harmful effect of low-quality neighbors, and can perform effectively without these training augmentations. 3 Results 3.1 Qualitative examination of retrieved neighbors QueryExample 1 Example 2 Example 3Neighbors Figure 3: Visualisation of N=4approximate retrieved using a learned Go-speciﬁc distance function for 3 query positions (one per row). Red stone indicates the next action(s) – light red stones indicate white moves, dark red stones indicate black moves. The color of the board border indicates whether the current player to play (for each board) won the game. 6 Changing a single stone in a Go board can dramatically affect the game, and the number of Go positions is enormous. We ﬁrst wanted to assess whether, despite the combinatorial aspect of the domain, meaningful could be retrieved from the dataset using our learned keys/queries. Examples of retrieved positions in Go are shown in Fig. 3, where we observed relevant matches in terms of both local and global structure. In some cases, especially in the early game, the retrieved position is an exact match even though the rest of the game (and therefore the associated meta-data) differs. Row 1 in Fig. 3 is an example of this – in this case, the retrieved data effectively provides sample rollouts from the query position akin to those derived from MCTS. / / / / Figure 4: a) Test-set top-1 policy prior ˆπ0accuracy (top) and value error (MSE) (bottom) over the course of training, for the (non-retrieval) baseline and retrieval models with N= 2,10. Results are averaged over 3 seeds (error typically too small to see). Note that the sharp transitions are due to the learning rate schedule. b) Final performance after training plotted as a function of number of neighbors Nretrieved — a different network is trained for each number of neighbors. c) Final performance as a function of model size relative to the size used elsewhere in this study, for networks with N= 10 retrieved neighbors and the non-retrieval baseline. d) Final performance as a function of the evaluation retrieval dataset size, for networks with N= 10 retrieved neighbors. The equivalent baseline performance is shown as a dashed line for reference. Note that the networks are trained with dataset fraction = 0.5and simply evaluated at other dataset fractions. 3.2 Impact of retrieval on supervised training We next evaluated the extent to which the model can learn to exploit retrieved information for better model predictions and better . First, we evaluated the impact of retrieval on supervised learning losses. As a baseline comparison, we trained the same network architectures but settingxi= 0 for alli— this has the effect of maintaining the number of parameters, ﬂops, and useful capacity in the network, while removing access to the retrieval data. We evaluated losses for retrieval networks trained with different N(Fig. 4b) and model sizes (Fig. 4c, see Sec. A.1 for details). Across conditions, we consistently observed a signiﬁcant boost in test-set accuracy for all metrics and over the course of training. This improvement to predictions is further observed across game trajectories, and is not limited, for example, to opening play positions. One advantage of the approach we outlined is that we can modify the retrieval dataset and potentially see immediate effects on the prediction, without changing the parameters θ. We ﬁrst veriﬁed this by evaluating our model when allowed access to varying fractions of the full datasetDr(Fig. 4d). We observed clear gains in prediction accuracy from increasing the size of the retrieval set (only half is used at train time). A further important observation is that large-scale datasets are clearly important - the evaluation metrics drop below the baseline level for a dataset Dr that is 1% the size of the full dataset. 7 3.3 Evaluation against a reference opponent While the results observed on the ofﬂine dataset suggest a strong positive effect from learning to leverage the retrieval data, this is in the context of a ﬁxed test data distribution that matches the retrieval data distribution. When deployed, games played against different opponents will likely diverge from that distribution. We evaluated the performance of our search policy πs(nsims= 200 ) by playing against a ﬁxed reference opponent — the Pachi program [ 5], which can perform beyond strong amateur level of play in 9x9 Go given sufﬁcient simulation budget (we evaluate against 400k simulations). We observed a signiﬁcant boost to performance for networks over the equivalent baseline network of the same capacity (Fig. 5). Interestingly, as shown in Appendix Fig. 8, playing using only the base policy prior ˆπ0shows a much smaller boost over the baseline network. As we explore further below, the quality of retrieved neighbors available during play is much lower than for training - we hypothesise that the search policy is more robust to this distributional shift than ˆπ0. / / Figure 5: Win rate against a ﬁxed reference opponent (Pachi) when playing using the MCTS policy πs for (a) retrieval networks using varying numbers of retrieved neighbors and for a baseline non-retrieval network. (b) Win rate as a function of model size relative to the size used elsewhere in this study. Retrieval leads to a clear performance boost compared to non-retrieval baselines of the same capacity. 3.4 Augmenting the retrieval dataset / / Figure 6: (a) Distribution of similarity distances from an encoded observation to its retrieved nearest neighbors. Retrieving from the original retrieval dataset using positions from the test dataset (orange) gives neighbors with a markedly different distance distribution to positions queried during play against a Pachi opponent (blue). Augmenting the retrieval dataset with ∼600k recorded agent-Pachi game states improves the similarity distribution (green). (b) For play with the MCTS policy πs, this augmentation also leads to a consistent win-rate boost (green) over using the train retrieval dataset alone (blue). We observed a signiﬁcant distribution shift between game positions observed in play against the Pachi opponent versus positions in the retrieval dataset (Fig. 6a), as measured by the empirical distance distribution of game observations to their approximate nearest neighbors. Changing or augmenting the retrieval dataset Drmodiﬁes this distributional shift. A particularly interesting modiﬁcation in this setting is to augment the dataset with play recorded between our agents and the Pachi opponent. As can be seen in Fig. 6a, augmenting the dataset with a set of ∼600k agent-Pachi game states increases 8 the similarity between positions observed in play and their retrieved neighbors. Strikingly, we found that integrating these games into our retrieval dataset leads to improvements in performance without further training for subsequent play against the Pachi opponent (play with the MCTS policy πsis shown in Fig. 6b, play with the policy prior ˆπ0is shown in Appendix Fig. 9). This highlights the potential of a model to rapidly integrate recent experience without further training. As a related intervention, we instead inputted randomly retrieved neighbors into the model at evaluation time (see Appendix Fig. 6). This signiﬁcantly impaired performance for play using the policy prior ˆπ0, but somewhat unexpectedly, for play with the MCTS policy πs, performance did not fully regress to the level of the baseline network. This suggests that our retrieval network is able to amortise some information from neighbors during training, leading to better performance even when no relevant neighbors are available. 4 Related work The idea of supporting decision making by directly attending to a cache of related events has been visited many times in different contexts, under the names of case-based reasoning [ 30], non- parametric RL [ 16,4,1,27], or episodic memory [ 20,23]. The aim of some methods is to better support the working memory of an agent during a given episode [ 26,42] or a series of successive related episodes [ 33]. Other methods, including ours, aim to leverage a broader class of relevant experience or persistent knowledge (including across episodes, and from other agents) to better support reasoning and planning. One factor in our work to these past approaches is that we do not prescribe how to process the information from the available data (e.g. through specifying the agent’s action-value directly in terms of previously generated value estimates [ 6,13,15,29], or a model from observed transitions [ 39]) but rather learn end-to-end how the data can support better predictions within the parametric model. A recent approach by Goyal et al. [10] has considered an attention mechanism to select where and what to use from available trajectories, but over a small retrieval batch of data rather than the full available experience data. Another class of method to leverage a transition dataset is to replay the data at training time in order to perform more gradient steps per experience, this is a widespread technique in modern RL algorithms [ 21,22,25,36] but it does not beneﬁt the agent at test time, requires additional learning steps to adapt to new data, and does not allow end-to-end learning of how to relate past experience to new situations. 5 Discussion Our approach and empirical results highlight how reinforcement learning agents can beneﬁt from direct access to a large collection of raw interaction data at inference time, through a retrieval mechanism, in addition to their already effective parametric representation. We showed this was the case even 1) when the domain is large enough to require generalisation in how to interpret past data, 2) when there is signiﬁcant distribution shift when acting, and 3) at a scale where only approximate can be retrieved. We believe this already demonstrates the potential of this approach for many possible scenarios and applications. We show that retrieval can be effectively combined with model-based search. We ﬁnd that the beneﬁts from retrieval and search are synergistic, with increasing numbers of retrieved neighbors and increasing simulations both leading to performance increases in almost all contexts we investigated. Furthermore, empirical evidence suggests that search signiﬁcantly improves agent robustness to distributional shift as compared to playing with the policy prior. In Appendix Fig. 10, we compare the boost in performance as a function of compute cost for increasing MCTS simu- lations versus increasing the number of retrieved neighbors processed by the model. This provides a tentative indication that retrieval is also a compute efﬁcient means of improving performance. A key future direction is to investigate the online learning scenario in which recent experience of the agent is rapidly made available for retrieval, hence progressively growing the retrieval dataset over the course of training. While there are additional challenges associated with the online paradigm (e.g., it may be desirable to update the queries and/or keys during training [ 12]), the fast adaptation effect we highlighted in this work may have even more impact there. There are many potentially relevant sources of information beyond an agent’s own experience, or that of other humans or agents. For example, it has been shown that YouTube videos are useful 9 for learning to play the Atari game Montezuma’s revenge [ 2]. Training an embedding network on sufﬁciently diverse data may enable retrieval of information from a wide range of contexts [ 43], including third-person demonstrations, videos and perhaps even books. Disclosure of Funding The authors received no speciﬁc funding for this work.  
nonlinear mcmc for bayesian machine learning 	Nonlinear MCMC for Bayesian Machine Learning James Vuckovic Abstract We explore the application of a nonlinear MCMC technique ﬁrst introduced in [ 1] to problems in Bayesian machine learning. We provide a convergence guarantee in total variation that uses novel results for long-time convergence and large-particle (“propagation of chaos”) convergence. We apply this nonlinear MCMC technique to sampling problems including a Bayesian neural network on CIFAR10. 1 Introduction Characterizing uncertainty is a fundamental problem in machine learning. It is often desirable for a machine learning model to provide a prediction anda measure of how “certain” the model is about that prediction. Having access to a robust measure of uncertainty becomes particularly important in real-world, high risk scenarios such as self-driving cars [ 2–4], medical diagnosis [ 5,6], and classifying harmful text [ 7]. However, despite the need for uncertainty in machine learning predictions, it is well known that traditional ML training, i.e. based on optimizing an objective function, frequently does not provide robust uncertainty measures [ 8], yielding overconﬁdent predictions for popular neural networks such as ResNets [ 9].1An appealing alternative to the traditional optimization paradigm for ML is the Bayesian probabilistic framework, due to its relatively simple formulation and extensive theoretical grounding; see for example [ 10]. From the probabilistic perspective of machine learning [ 10], one combines a prior P(✓)over the parameter space ✓2⇥and a likelihood of the data given model parameters P(D|✓)using Bayes’ rule to obtain a posterior over the parameters P(✓|D)/P(D|✓)P(✓). The “traditional” approach in ma- chine learning is to optimize the posterior (or the likelihood) to obtain ✓⇤2arg max P(✓|D)and gen- erate predictions via P(y|x,D)=P(y|x, ✓⇤). However, if we adopt the Bayesian approach, the poste- rior characterizes the uncertainty about the parameters of the model (i.e. epistemic uncertainty), which can propagate to uncertainty about a prediction by integration :P(y|x,D)=´ P(y|x, ✓)P(✓|D)d✓. This paper studies the problem of how to approximate this integration with samples from P(✓|D). 1.1 Contributions •Our main contribution is the novel analysis of a modiﬁcation of the general nonlinear Markov Chain Monte Carlo (MCMC) sampling method from [ 1] to obtain quantitative convergence guarantees in both the number of iterations and the number of samples. •We apply the general results from above to determine the convergence of two speciﬁc nonlinear MCMC samplers. •In experiments, we compare these nonlinear MCMC samplers to their linear counterparts, and ﬁnd that nonlinear MCMC provides additional ﬂexibility in designing sampling algo- rithms with as good, or better, performance as the linear variety. 1In Appendix C.2.4 , we provide an experiment that demonstrates this effect. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). 1.2 Background Bayesian ML & MCMC. In Bayesian machine learning, the “ difﬁcult” step is integration since the integral´ P(y|x, ✓)P(d✓|D)is not analytically solvable except for rare cases. In practice, one typically uses a Monte Carlo approximation such as ˆ P(y|x, ✓)P(✓|D)d✓⇡1 NNX i=1P(y|x, ✓i),where ✓iiid⇠P(✓|D) where the expected error in this approximation is well known to converge to zero like O(1/p N) by the Central Limit Theorem (CLT). There are various approaches to sampling from P(✓|D), but Markov chain Monte Carlo (MCMC) is perhaps the most widely used. The basic idea of MCMC is to use a Markov transition kernel Twith stationary measure P(✓|D)to simulate a Markov chain ✓ converges rapidly to P(✓|D). In this case, we can estimate ˆ P(y|x, ✓)P(✓|D)d✓⇡1 NNX i=1P(y|x, ✓i 1)⇡1 NNX i=1P(y|x, ✓i nsim) where nsimis some large number of simulation steps and {✓i n}1 n=0are independent Markov chains governed by T. The basic problem is then to design an efﬁcient transition kernel T. There is a vast body of literature studying various choices of T; some well known choices are the algorithm [11,12], the Gibbs sampler [ 13], the Langevin algorithm [ 14–17], Langevin [18,19], and Hamiltonian Monte Carlo [ 20–24]. However, there are various challenges in Bayesian ML that make applying these samplers difﬁcult in practice. One challenge is that the posterior P(✓|D)can be highly multimodal [ 25,26], which makes it difﬁcult to ensure that the Markov chain ✓nexplores all modes of the target distribution. One can combat this issue by employing auxiliary samplers that explore a more “tractable” variation ofP(✓|D)[27,28]. Other methods that empirically improve posterior sampling quality include tempering [ 29–32], RMSProp-style [ 33], or adaptive MCMC algorithms [ 34,35] such as the popular No U-Turn Sampler [ 36]. Nonlinear MCMC. Another class of powerful MCMC algorithms, which is less-studied in the context of Bayesian ML, arises from allowing the transition kernel Tto depend on the distribution of the Markov chain as in ✓n+1⇠T Distribution( ✓n)(✓n,•). This approach gives rise to so-called nonlinear MCMC since {✓n}is no longer a true Markov chain. Nonlinear Markov theory is a rich area of research [ 37–42] and has strong connections to nonlinear ﬁltering problems [ 43,44], sequential Monte Carlo [ 45,46], and nonlinear Feynman-Kac models [ 47]. One can replace Distribution( ✓n), which is often intractable, with an empirical estimate Distribution( ✓n)⇡1 NPN i=1 ✓into obtain interacting particle MCMC (iMCMC, or iPMCMC) methods; see for example [ 48,49,1,50,51]. Our view is that nonlinear MCMC offers some appealing features that traditional linear MCMC lacks. One such feature is the ability to leverage global information about the state space ⇥contained in Distribution( ✓i n)to improve exploration, a central issue in Bayesian ML. Another feature is the increased ﬂexibility of nonlinear MCMC algorithms, which can be leveraged to correct biases that are introduced by other design decisions in MCMC for Bayesian ML such as tempering. These features will be explored empirically in Section 4. However, the theoretical analysis of nonlinear Markov MCMC presents an added difﬁculty in that the particles {✓1 n,...,✓N n}of an interacting particle system are now statistically dependent . This means that, in addition to studying the long-time behaviour which is classical in MCMC [ 52,53], one must study the large-particle behaviour separately to obtain Monte Carlo estimates since the CLT does not apply. One such large-particle behaviour is the propagation of chaos [ 41], which is the tendency for groups of interacting particles to become independent as the number of particles, N, increases; see [41]. We will need both of these elements — long-time convergence and propagation of chaos — to properly characterize the convergence of nonlinear MCMC. Other Sampling Methods. Finally, let us mention that MCMC is certainly not the only way to obtain Monte Carlo sample estimates in Bayesian ML; some popular examples include MC dropout [8], black-box variational inference [ 54], and normalizing ﬂows [ 55,56]. 2 1.3 Common Notation LetP(Rd)be the set of probability measures on the measurable space (Rd,B(Rd)). For µ2 P(Rd)andf2B b(Rd): = {f:Rd!R|fis bounded }, we will denote µ(f): =´ fdµ. If a Markov kernel2then we will denote Kf(x): =´ f(y)K(x,dy)and µK(dy): =´ µ(dx)K(x,dy). Finally, for , we will denote the empirical measure of yasm(y): =1 NPN i=1 yi2P(Rd). 2 Nonlinear MCMC In this section, we outline the family of MCMC algorithms that will be studied in rest of the work. We will use general notation for simplicity but this difference is merely cosmetic; the “target distribution” ⇡in this section corresponds directly to P(✓|D)from the previous section. 2.1 Nonlinear Jump Interaction Markov Kernels To specify a MCMC algorithm, we must specify the Markov transition kernel. The family of nonlinear Markov kernels that we will be studying was introduced in [ 1] and is a mixture of a linear kernel, denoted K, and a nonlinear kernel indexed by a probability measure ⌘, denoted J⌘, to obtain K⌘(x,dy): =( 1  ")) (1) where "2]0,1[is the mixture hyperparameter. The Markov kernel K⌘will be the main object of interest throughout this paper. We will give speciﬁc examples of J⌘in Section 2.2, which were also introduced in [ 1]. Despite building on the constructions of [ 1], this work proceeds in some different substantially directions; see Appendix A.1for more details. Mean Field System. Now we show how the kernel K⌘can be used to construct a Markov chain. Following [ 1], we use an auxiliary Markov chain {Yn}with transition kernel Qon the same state space as K⌘(i.e.Rd) to obtain the nonlinear Markov chain {(Yn,Xn)}1 n=0deﬁned by 8 >>< >>:Yn+1⇠Q(Yn,•) ⌘n+1:= Distribution( Yn+1) (2) where µ0,⌘02P(Rd)are the initial distributions and ⇠denotes “sample from”. One should interpret this as a sequence of steps where ﬁrst we sample the auxiliary state Yn+1from Q, then we obtain the distribution of Yn+1denoted ⌘n+1, and we use this distribution to index the primary kernel K⌘n+1 and obtain a sample Xn+1. We sample Xn+1with probability (1 ")from the linear kernel K, and with probability "it will “jump” according to J⌘n+1(Xn,•). Because the Markov dynamics depend onDistribution( Yn), we call this a “mean ﬁeld system”. Interacting Particle System. One issue with the mean ﬁeld system (2)is the fact that computing Distribution( Yn+1)is generally impossible except in special cases. Hence, to get a viable simulation algorithm, we must approximate Distribution( Yn), and we do this by replacing Distribution( Yn) with its empirical measure estimated from a set of Nparticles Yn:={Y1 n,...,YN n}as follows: 8 >>< >>:Yi n+1⇠Q(Yi n,•) ⌘N n+1:=m(Yn+1) Xi n+1⇠K⌘N n+1(Xi n,•)Yi 0iid⇠⌘0,Xi . (3) 2.2 Application to MCMC Now we detail how to apply K⌘and the Markov chains (2)and(3)to MCMC. In particular, we must understand how to choose Q, K, J ⌘such that K⌘will be invariant w.r.t. a target distribution ⇡. 2i.e.K(x,•)is a probability measure measurable 8A2B(Rd) 3 As is usually the case in probabilistic inference problems, we will assume that the target distribution ⇡is known only up to a normalizing constant and that it has a density, also denoted ⇡. We also make the simplifying assumption that Qhas an invariant measure ⌘?(also with density denoted ⌘?) i.e. ⌘?Q=⌘?. This is not burdensome; in practice we can, and will, obtain Qfrom a linear MCMC algorithm for some choice of ⌘?. In fact, being able to choose ⌘?is a powerful design parameter of our methods as we will see in Section 4. We will also assume that the linear kernel Kis⇡-invariant, i.e.⇡K=⇡. To see how we can ensure that ⇡isK⌘-invariant, consider the fact that we will design Qs.t. ⌘n:= Distribution( Yn)converges to ⌘?. This means we will eventually be sampling from the kernel K⌘?and we already have ⇡-invariance of K. Therefore, if we arrange for J⌘?to be ⇡-invariant, ⇡ will be invariant for K⌘?since ⇡K⌘?=( 1  ")⇡K+"⇡J ⌘?=( 1  ")⇡+"⇡=⇡. Intuitively, if the auxiliary chain converges to a steady state and the jumps in that steady state preserve ⇡(and Kpreserves ⇡), then so will K⌘?. Now the remaining task is to design nonlinear interaction kernels J⌘that will yield good performance; we detail two choices below. Interaction. The ﬁrst choice of J⌘we will investigate, from [ 1], relies on the transformation [ 47], which we now explain. Let G:Rd!]0,1[be a potential function; then the (BG) transformation is a nonlinear mapping G:P(Rd)! P(Rd)deﬁned by G(µ)(dx): =G(x) µ(G)µ(dx)or equivalentlyˆ f(x) G(µ)(dx): =ˆ f(x)G(x) µ(G)µ(dx) for any f2Bb(Rd)and whenever µ(G)6=0. This transformation has many interesting properties and been extensively studied in [ 47] and related works. To use the BG transformation in MCMC, we will assume that the densities ⇡and⌘?are positive3and make the choice that G(x)will be the function G(x): =⇡(x) ⌘?(x). (4) With this choice, we get an interaction kernel JBG ⌘(x,dy): = G(⌘)(dy). We can easily see that ⌘?(G)=ˆ⇡ ⌘?d⌘?=ˆ d⇡=1 and G(⌘?)(dx)=G(x) ⌘?(G)⌘?(dx)=⇡(x) ⌘?(x)⌘?(dx)=⇡(dx), i.e. Gis the multiplicative “change of measure” from ⌘?to⇡. Hence the ﬁrst nonlinear Markov kernel we will investigate is KBG ⌘(x,dy): =( 1  ")K(x,dy)+" G(⌘)(dy). (5) From the remarks above, is clear that ⇡. Accept-Reject Interaction. The second choice of jump interaction we will study, also introduced in [1], is a type of accept-reject interaction related to the algorithm. For the same choice of potential function Gin (4), we can deﬁne the acceptance ratio ↵(x, y): =1 ^G(y) ) ⌘?(y)⇡(x)and the quantity A⌘(x): =ˆ ↵(x, y)⌘(dy) for⌘2P(Rd). Hence we can deﬁne the accept-reject interaction kernel as4 JAR ⌘(x,dy): = ↵(x, y)⌘(dy)+( 1  A⌘(x)) x(dy). We can interpret this jump interaction as: starting in state x, we jump to a new state distributed according to ⌘(dy)with probability ↵(x, y)(i.e. accept the proposed jump) and remain the in current state with probability 1 A⌘(x)(i.e. reject the proposed jump). This is a form of “adaptive ” in which the proposal distribution evolves over time as the distribution of the auxiliary Markov chain. Hence we obtain the accept-reject nonlinear jump interaction kernel KAR ⌘(x,dy): =( 1  ")K(x,dy)+"[↵(x, y)⌘(dy)+( 1  A⌘(x)) x(dy)]. (6) We note that ⇡is also JAR ⌘?-invariant; see Proposition 3in Appendix Gfor a simple calculation. 3this can be relaxed to ⇡⌧µandµ⌧⇡ 4Given f2B b(Rd), we can also write this as JAR ⌘f(x)=´ [f(y) f(x)]↵(x, y)⌘(dy)+f(x) 4 Simulation. Let us note brieﬂy that using both KBG ⌘andKAR ⌘in(3)produce interacting particle systems that can, and will, be simulated. The simulation is relatively , see Appendix A for pseudocode implementing the nonlinear MCMC algorithms we have now constructed. 3 Convergence Analysis We will now study whether the nonlinear MCMC algorithms based on K⌘from Section 2— i.e., the interacting particle system (3)with the restrictions on K,Q,J ⌘from Section 2.2— will actually converge to the target distribution ⇡. In other words, we would like to estimate kµN n ⇡kfor some suitable notion of distance on P(Rd), where µN n:= Distribution( X1 n)is the distribution of a single particle (it doesn’t matter which particle as the Xi ). The nonlinear nature of K⌘makes this analysis more difﬁcult than of a linear MCMC method. We break the problem into two parts: one studying the convergence of the mean-ﬁeld system (2)as the number of steps n!1, and one studying the convergence of the interacting particle system (3)to the mean ﬁeld system as the number of particles N!1. This will allow us to apply the triangle inequality as follows: kµN n ⇡k k µN n µnk|{z} large-particle convergence+ kµn ⇡k|{z} long-time convergence where µn:= Distribution( Xn)is the distribution of the mean-ﬁeld system. Crucially, our analysis of the large-particle limit is uniform in the number of steps n, which will allow us to establish bounds above that hold as n!1. The actual result is contained in Theorem 1. While our analysis does not rely on heavy mathematical machinery, to state the full set of conditions and results for long-time and large-particle convergence — each of which is a substantial result in its own right — would occupy too much space in the main text. Instead, we will state the main result in Theorem 1, which is essentially a corollary of the long-time and large-particle analyses Theorems 2and3in Appendices Eand Frespectively, and below we will sketch the general arguments used in those appendices. The proofs of the main results are in Appendix F.2. Note that our analysis, and the results we obtain, are novel and not found in [ 1]; see Appendix Dfor an elaboration. The following result is stated in terms of the total variation metric, deﬁned here for µ, ⌫2P(Rd)as kµ ⌫ktv:= k•k1is the sup-norm on Bb(Rd). Theorem 1. [Convergence of Nonlinear MCMC] Under suitable conditions on K⌘andQ, there exist ﬁxed constants C1,C2,C3>0, a function R:[ 0,1[![1,1[, and ⇢>0s.t. kµN n ⇡ktvC11 . ⌥ Let us make a couple of remarks: •This result shows that, to control the approximation error kµN n ⇡ktv, it does not necessarily sufﬁce to run the MCMC algorithm for a large number of steps n, since if n!1but N< 1then our bound on kµN n ⇡k 6!0. However, this approximation cannot lead to arbitrarily bad results: Theorem 1provides a quantitative upper bound on how much the MCMC algorithm can be biased. This behaviour is supported empirically; in Figure 5of Appendix C.1.4 we provide a clear illustration of how changing Nsigniﬁcantly affects the bias of our nonlinear MCMC methods while having no effect on the bias of linear MCMC, as expected. •This result uses total variation, which is a strong metric that represents a worst-case over allbounded functions f(up to rescaling by kfk1). It is entirely possible that, for many choices of practical f, the approximation will be better as we will see empirically. •The constant ⇢is, roughly speaking, the slower of the rate of convergence for Qand for K. Hence if K,Q are chosen to be efﬁcient samplers with fast convergence, this will result in ⇢⌧1and hence µN nwill also converge quickly. 5 •In our speciﬁc samplers KBG ⌘andKAR, we will see in Appendix GthatRis a monotoni- cally increasing function that is lower-bounded by 1. Hence, as N!1,1 NR(1 N)!0as expected. A corollary of Theorem 1is that that we regain a Monte Carlo estimate for the interacting particle system. This result is essentially due to [ 41] Theorem 2.2. Corollary 1. [Adapted from [ 41], Theorem 2.2] Suppose that Theorem 1applies to K⌘. Let Xn:={X1 n,...,XN n}be the interacting particle system from (3). Then for every n2Nand f2Bb(Rd)we have lim N!1E"     1 NNX i=1f(Xi n) µn(f)     # =0. ⌥ This corollary directly relates to the application of Bayesian ML we are interested in, where we would have f(✓)=P(y|x, ✓). 3.1 Long-Time Bounds There are two main ingredients in the general result on long-time convergence: ergodicity of Kand Q, and Lipschitz regularity of the interaction kernel ⌘7!J⌘. These, along with other technical conditions, produce an estimate of the form k•kis a weighted total variation norm. The full statement is in Theorem 2of Appendix E. Ergodicity of KandQ.A fundamental requirement of our results is that the linear building blocks ofK⌘must converge to their respective stationary measures in an appropriate metric. This type of result is now standard in the Markov chain literature, and we use a result from [ 57] for Kand a result from [ 1] for Q. The former is actually able to ensure that Kis a contraction on P(Rd)w.r.t. a suitable weighted total variation; we use this feature repeatedly in our analysis. Lipschitz Regularity of J⌘.We also need that ⌘7!J⌘is w.r.t. a weighted total variation norm on Markov kernels (the Lipschitz constant does not have to be <1). This regularity is used to translate the convergence of ⌘n!⌘?(as guaranteed by the ergodicity of Q) into convergence of J⌘!J⌘?with the Lipschitz estimate . In Appendix G, we verify this analytically for JBG ⌘andJAR ⌘, see Lemma 5and [ 1] Proposition 5.3. 3.2 Large-Particle Bounds To study the large-particle behaviour, we would like to measure how close subset of q2{1,...,N } interacting particles {X1 n,...,Xq n}⇢{ X1 n,...,XN n}=:Xnfrom (3)is to being i.i.d. according to the mean-ﬁeld measure µn. This analysis was pioneered in [ 41] under the name “propagation of chaos” and formalizes the intuition that, as N!1, the inﬂuence of any individual particle !0. To state this more precisely, ﬁrst note that if we had random variables the joint distribution of be ⌘⌦N. Hence, as N!1for the interacting particles Xnat time n, we expect the distribution of {X1 n,...,Xq n}, denoted µq,N n, to get closer to the distribution of i.i.d. mean ﬁeld particles from (2), denoted µ⌦q n. In other words, we expect kµq,N n µ⌦q nktv!0asN!1. The full statement of this result is Theorem 3of Appendix F. The main condition in our propagation of chaos result is another type of regularity for ⌘7!J⌘which basically requires that, if one approximates a distribution ⌘2P(Rd)by its empirical measure m(Y) where ⇠⌘, then Jm(Y)!J⌘asN!1. More precisely, there should be a function R:[ 0,1[![1,1[, which is ideally nondecreasing, s.t. |E[J⌦q m(Y)f(x)] J⌦q ⌘f(x)|.q2 “oscillations” osc(f)1. The expectation is taken over ⌘⌦N, and “oscillations” are deﬁned precisely in Appendix D. This inequality is essentially a total variation regularity since we can alternately write kµ ⌫ktv= ]. 6 3.3 Analysis of Speciﬁc Interaction Kernels The main result Theorem 1is in terms of conditions on a general K⌘(i.e. general choices of K,Q,J ⌘). To apply this result to the samplers in Section 2, we must establish if these conditions hold for KBG ⌘ andKAR ⌘. Fortunately this can be done analytically; in Appendix G, we present conditions under which the Lipschitz regularity (see Lemma 5and [ 1] Proposition 5.3) and large-particle regularity (see Corollaries 3and5) hold. These results, particularly for KBG ⌘, are interesting and rely on novel techniques for controlling the various quantities, sometimes improving over previous methods. Due to space constraints, the results for KBG ⌘andKAR ⌘are in Corollaries 2and4from Appendix G. 4 Experiments In this section, we detail two experiments designed to explore how one might apply the nonlinear MCMC methods developed in the previous sections to Bayesian machine learning.5Let us state explicitly that our aim is notto achieve with these experiments, nor do we claim that this method will necessarily lead to results on a particular task. Rather, the aims of these experiments are: to demonstrate that nonlinear MCMC can be applied successfully to large-scale problems; to compare linear vs nonlinear methods to understand what beneﬁts and drawbacks nonlinear MCMC offers compared to linear MCMC in practice; and to develop some recipes for choosing the various and samplers that determine a nonlinear MCMC method. 4.1 Toy Experiments First, we use a toy setting of distributions to compare the relative beneﬁts of linear and nonlinear MCMC. A beneﬁt of this simple setting is that the multimodal toy distributions can be exactly sampled. This gives us the opportunity to quantify the quality of our samples via an unbiased estimator of the Maximum Mean Discrepancy (MMD) metric on P(Rd)[58]. This approach stands in contrast to many previous works, which use simplistic distributions (e.g. Gaussians) with analytically tractable statistics to measure quality. See Appendix C.1for an overview of our methodology. Setup. Our setup will compare the Metropolis Adjusted Langevin Algorithm (MALA) [ 18] (see Appendix Bfor an overview of MALA) with the nonlinear BG and AR samplers using MALA for the kernels K,Q in our nonlinear setup from Section 2. This will allow us to examine the effects of the nonlinearity in KAR ⌘andKBG ⌘while controlling for the type of sampler used and its . The main difference between the linear and nonlinear algorithms, aside from the interaction itself, is the extra “design knob” to control in the form of the choice of the auxiliary density ⌘?. Below, we show how one can use ⌘?to incorporate additional insight to guide the sampling, such as regions of the state space to explore. In our experiments, we choose ⌘?to be a centered, 2-dimensional Gaussian with a large variance ( ⌃=4 I2for the circular MoG and two-rings densities, and ⌃= 20 I2for the grid MoG). This conveys “coarse-grained” information of roughly where the support of the target density is located – in this case, a neighbourhood of (0,0). See Table 2in Appendix C.1for a full account of our experimental settings. Results. From Figure 1, we see that having a simple auxiliary density with good coverage of the support of the target distribution is quite helpful. In all three examples, one or both of the nonlinear samplers outperformed the equivalent linear sampler in the empirical MMD metric. For the two most challenging densities, the “two rings” and “grid MoG” distributions, the improved exploration is particularly evident. We also include an analysis of the runtime of the algorithms in Appendix C.1.3 . Comparison With [ 1].We also compared the performance of our methods with those of [ 1] in this toy setting. See Appendix C.1.4 for an overview of the results; they support all of the theoretical considerations and design principles we have introduced in this paper. 5The code used in our experiments can be found at / . See also Appendix Afor a discussion of the implementation details. 7  1: Visualizations of the 2d experiment. The top row shows the empirical over number of sampled steps, where the shaded region is±1standard deviation runs. The bottom three rows show histograms for theN= 2000samples of the of Gaussians (MoG) density [59], the Two Rings density [59], and the Grid Mixture ofGaussians density [60] CIFAR104.2.1 SetupTo examine the properties of the nonlinear sampler outside of a toy setting, we have also implementeda Bayesian neural network on the CIFAR10 dataset. We use a , ✓)parameterized bya ResNet-18 convolutional neural network [61] and a Gaussian priorP(✓)on the parameters of thisneural network which are combined to form a goal is to this posterior. See Table4in AppendixC.2for a fullaccount of the experimental settings.To deal with the fact that this sampling problem is very |= 60,000)we use a variety of sample size 256 to obtain the surrogate target use an RMSProp-like “preconditioned” Langevin algorithm, called RMS-Langevinor RMS-ULA, as in [33] for the auxiliary samplerQ; see AppendixBfor details on thissampler. As shown in [33], this sampler is biased.3.We use tempering, wherein we aim to sample asmall number. This substantially improves mixing for hard-to-sample distributions such the cost of bias since we are no longer sampling from the true posterior [32].Using our nonlinear algorithm presents a novel opportunity to correct the bias introduced by tempering.For our experiments, we ). This means that the a tempered version of the target, whereas the target chainXn(in theory) exploresthe true target distribution. This is a novel strategy that is made possible by being able to select⌘?almost independently of the target⇡. We study the case when⇡is tempered as well.8 For our experiments, we use the RMS-Langevin sampler as the baseline, and we also use it for the auxiliary sampler Q. For the target sampler, it is not possible to use the RMS-Langevin algorithm because the smoothed estimate is incompatible with the (i.e. jumps) introduced by the nonlinear interaction. Instead, for the linear sampler Kwe use the unadjusted Langevin algorithm, ULA, [ 16] (see Appendix B). We investigate both test accuracy and calibration error [ 9] to assess performance. Table 1: Results for CIFAR10 experiments. ±represents 1 standard deviation on 5random seeds. The tempered results are using ⌧= 10 4. See Appendix C.2for an overview of expected calibration error. We also compute the maximum calibration error in Appendix C.2. All Expected Calibration Error numbers are multiplied by 102in this table. Test Accuracy (") Expected Calibration Error (#) Algorithm Non-Tempered Tempered Non-Tempered Tempered Linear 85.01±0.10 85.01±0.19 0.24±0.02 0.26±0.014 Nonlinear (BG) 84.28±0.28 84.74±0.08 0.14±0.03 0.16±0.03 Nonlinear (AR) Diverged 84.67±0.23 Diverged 0.15±0.05 Figure 2: Evaluation of test accuracy during sampling for CIFAR10. The shaded areas represent ±1standard deviation for 5random seeds. For readability, we omit the AR interaction curve on the non-tempered result since it diverged and it distorts the scale of the plot. For completeness, all the curves for the non-tempered case are plotted in Appendix C.2.2 , Figure 6. 4.2.2 Results Linear vs Nonlinear. From Table 1, we see that the linear (RMS-Langevin) sampler has slightly higher, but comparable test accuracy to the nonlinear samplers. This is likely because RMS-Langevin algorithm has better stability around the regions of high probability due to its adaptive stepsize scaling. However, from Figure 2, we see that for both the tempered and non-tempered cases, the nonlinear interaction appears to beneﬁt during early exploration. This is an expected and desired property of these nonlinear samplers, which incorporate global information about the sampler state (in this case, the relative potential Gof each auxiliary chain’s state) and are able to emphasize those states with higher probability. However, the linear method is able to eventually explore the relevant regions of the state space, and the difference disappears. See Appendix C.2.5 for a comparison linear vs nonlinear performance scaled by the number of gradient evaluations. Tempering Vs Non-Tempering. The linear MCMC sampler is always tempered in our experiments so there should not be any statistically signiﬁcant difference in the linear case. For the nonlinear sampler, the tempered version has slightly higher accuracy; this trend is also observed in [ 32]. On the other hand, the calibration errors are the same for both tempered and non-tempered variants. This is somewhat surprising, given the aggressive tempering used, and one would expect that this reduces the variance of the posterior estimate.6This observation can perhaps be explained by the fact that we are using N= 10 samples which may not be enough to accurately change the tempered auxiliary distribution ⌘?into the non-tempered primary distribution ⇡in the jump interaction. 6As⌧!0, this ✓⇠ to the maximum a posteriori estimate with zero variance. 9 Calibration. Considering the expected calibration error (ECE) [ 9], in Table 1we see that the nonlinear method has statistically signiﬁcantly lower ECE ( to the linear method. We hypothesize that this is due to a tension between the RMS scaling of the gradient which improves the efﬁciency of each MCMC step but at the cost of bias, which may be measurable in the form of calibration error. The Langevin algorithm is also biased, but is generally known to have good convergence properties [ 16] and much less is known about the RMS-Langevin variant. By using our nonlinear setup, we are able to aggressively explore the auxiliary distribution without sacriﬁcing calibration on the target distribution. 5 Conclusion In this paper, we have studied the theoretical and empirical properties of nonlinear MCMC methods. We have obtained powerful theoretical results to characterize the convergence of our MCMC methods, and we have applied these methods to Bayesian neural networks. The results on BNNs are comparable to, but not better than, the linear methods we studied. We hypothesize that this is because more investigation into choosing the best auxiliary density ⌘?is required; our choice is simplistic and may not be optimal. This hypothesis is supported by our toy experiments, which show signiﬁcant improvement when ⌘?is able to incorporate some additional insight into the problem. How to do this in high dimensions is an exciting direction for future research. Broader Impact. There are beneﬁts and drawbacks to the nonlinear MCMC methods we describe. The beneﬁts are mainly that properly accounting for uncertainty in machine learning will lead to better real-world outcomes for high-value scenarios such as self-driving cars or medical imaging. The drawbacks are that MCMC methods require O(N)storage and computations relative to the O(1) for deterministic methods; in fact, our nonlinear method would require 2Nresources compared with Nfor a linear method (see also Appendices C.1.3 andC.2.5 ). If our algorithms were applied to a large swath of ML “as is”, this would mean a substantial increase in the energy consumption required for and deployment, worsening an already substantial issue in the ﬁeld.  
optimizing data collection for machine learning 	Optimizing Data Collection for Machine Learning Rafid Mahmood1James Lucas1Jose M. Alvarez1Sanja T. Law1 of Toronto3Vector Institute {rmahmood, jalucas, josea, sfidler, Abstract Modern deep learning systems require huge data sets to achieve impressive per- formance, but there is little guidance on how much or what kind of data to collect. data incurs unnecessary present costs, while may incur future costs and delay workflows. We propose a new paradigm for modeling the data collection workflow as a formal optimal data collection problem that al- lows designers to specify performance targets, collection costs, a time horizon, and penalties for failing to meet the targets. Additionally, this formulation generalizes to tasks requiring multiple data sources, such as labeled and unlabeled data used in learning. To solve our problem, we develop Learn-Optimize- Collect (LOC), which minimizes expected future collection costs. Finally, we numerically compare our framework to the conventional baseline of estimating data requirements by extrapolating from neural scaling laws. We significantly reduce the risks of failing to meet desired performance targets on several classification, segmentation, and detection tasks, while maintaining low total collection costs. 1 Introduction When deploying a deep learning model in an industrial application, designers often mandate that the model must meet a pre-determined baseline performance, such as a target metric over a validation data set. For example, an object detector may require a certain minimum mean average precision before being deployed in a setting. One of the most effective ways of meeting target performances is by collecting more training data for a given model. Determining how much data is needed to meet performance targets can impact costs and development delays. Overestimating the data requirement incurs excess costs from collection, cleaning, and annotation. For instance, annotating segmentation masks for a driving data set takes between 15 to40seconds per object. For 100,000images the annotation could require between 170and460 of time [ 1,2]. On the other hand, collecting too little data may incur future costs and workflow delays from having to collect more later. For example, in medical imaging applications, this means further clinical data acquisition rounds that require expensive clinician time. In the worst case, designers may even realize that a project is infeasible only after collecting insufficient data. The growing literature on sample complexity in machine learning has identified neural scaling laws that scale model performance with data set sizes according to power laws [ 3–10]. For instance, Rosen- feld et al. [6]fit power law functions on the performance statistics of small data sets to extrapolate the learning curve with more data. In contrast, Mahmood et al. [2]consider estimating data requirements and show that even small errors in a power law model of the learning curve can translate to massively over- or how much data is needed. Beyond this, different data sources have different costs and scale differently with performance [ 11–14]. For example, although unlabeled data may be easier to collect than labeled data, some learning tasks may need an order of magnitude more unlabeled data to match the performance of a small labeled set. Thus, collecting more data based only on estimation will fail to capture uncertainty and collection costs. 36th Conference on Neural Information Processing Systems (NeurIPS 2022).  Sample bootstrap performance with subsets of data to learn data requirement distribution Use estimated distribution to optimize collection cost plus risk of failing to meet V*Pay c(qt - qt-1) to collect additional data Have we achieved score V*?Have we hit time limit T? Terminate Pay P and terminate YesNo Yes Initialize with q0 data points & target V* NoLearn Optimize Collect Figure 1: In the optimal data collection problem, we iteratively determine the amount of data that we should have, pay to collect the additional data, and then re-evaluate our model. Our approach, , optimizes for the minimum amount of data q∗ tto collect. In this paper, we propose a new paradigm for modeling the data collection workflow as an optimal data collection problem . Here, a designer must minimize the cost of collecting enough data to obtain a model capable of a desired performance score. They have multiple collection rounds, where after each round, they re-evaluate the model and decide how much more data to order. The data has per-sample costs and moreover, the designer pays a penalty if they fail to meet the target score within a finite horizon. Using this formal framework, we develop an optimization approach for minimizing the expected future collection costs and show that this problem can be optimized in each collection round via gradient descent. Furthermore, our optimization problem immediately generalizes to decisions over multiple data sources (e.g., unlabeled, long-tail, cross-domain, synthetic) that have different costs and impacts on performance. Finally, we demonstrate the value of optimization over naïvely estimating data set requirements (e.g., [2]) for several machine learning tasks and data sets. Our contributions are as follows. (1) We propose the optimal data collection problem in machine learning, which formalizes data collection workflows. (2) We introduce (LOC), a framework that minimizes future collection costs, can be solved via gradient descent, and has analytic solutions in some settings. (3) We generalize the data collection problem and LOC to a multi-variate setting where different types of data have different costs. To the best of our knowledge, this is the first exploration of data collection with general multiple data sets in machine learning, covering for example, and long-tail learning. (4) We perform experiments over classification, segmentation, and detection tasks to show, on average, approximately a2×reduction in the chances of failing to meet performance targets, versus estimation baselines. 2 Related work Neural Scaling Laws. According to the neural scaling law literature, the performance of a model on a validation set scales with the size of the training data set qvia a power law V∝θ0qθ1[5, 6, 8– 10,15–19]. Hestness et al. [5]observe this property over vision, language, and audio tasks, Bahri et al. [9]develop a theoretical relationship under assumptions on and the Lipschitz continuity of the loss, model, and data, and Rosenfeld et al. [6]estimate power laws using smaller data sets and models to extrapolate future performance. Multi-variate scaling laws have also been considered for some specific tasks, for example in transfer learning from synthetic to real data sets [ 11]. Finally, Mahmood et al. [2]explore data collection by estimating the minimum amount of data needed to meet a given target performance over multiple rounds. Our paper extends these prior studies by developing an optimization problem to minimize the expected total cost of data collected. Specifically, we incorporate the uncertainty in any regression estimate of data requirements and further generalize to multiple data sources with different costs. Active Learning. In active learning, a model sequentially collects data by selecting new subsets of an unlabeled data pool to label under a pre-determined labeling budget that replenishes after each round [ 20–24]. In contrast, our work focuses on systematically determining an optimal collection budget. After determining how much data to collect, we can use active learning techniques to collect the desired amount of data. Statistical Learning Theory. Theoretical analysis of the sample complexity of machine learning models is typically only tight asymptotically, but some recent work have empirically analyzed these 2 relationships [ 25,26]. Particularly, Bisla et al. [10] study generalization bounds for deep neural networks, provide empirical validation, and suggest using them to estimate data requirements. In contrast, our paper formally explores the consequences of collection costs on data requirements. Optimal Experiment Design. The topic of how to collect data, select samples, and design scientific experiments or controlled trials is well-studied in econometrics [ 27–29]. For example, Bertsimas et al. [30] optimize the assignment of samples into control and trial groups to minimize inter-group variances. Most recently, Carneiro et al. [31] optimize how many samples and covariates to collect in a statistical experiment by minimizing a treatment effect estimation error or maximizing t-test power. However, our focus on industrial machine learning applications differs from experiment design by having target performance metrics and continual rounds of collection and modeling. 3 Main Problem In this section, we give a motivating example before introducing the formal data collection problem. We include a table of notation in Appendix A. Motivating Example. A startup is developing an object detector for use in autonomous vehicles within the next T= 5years. Their model must achieve a mean Average Precision greater than V∗= 95% on a pre-determined validation set or else they will lose an expected profit of P= $1,000,000. Collecting training data requires employing drivers to record videos and annotators to label the data, where the marginal cost of obtaining each image is approximately c= $1 . In order to manage annual finances, the startup must plan how much data to collect at the beginning of each year. Letz∼p(z)be data drawn from a distribution p. For instance, z:= (x, y)may correspond to images xand labels y. Consider a prediction problem for which we train a model with a data set D of points sampled from p(z). LetV(D)be a score function evaluating the model trained on D. Optimal Data Collection. We possess an initial data set Dq0:={zi}q0 i=1ofq0points; we omit the subscript on Dreferring to its size when it is obvious. Our problem is defined by a target score V∗> V(Dq0), a cof collection, a horizon of Trounds, and a penalty Pfor failure. At the end of each round t∈ {1, . . . , T }, letqtbe the current amount of data collected. Our goal is to minimize the total collection cost while building a model that can achieve the target score: min ) +P1{V(DqT)< V∗} s.t. q0≤q1≤ ··· ≤ qT = min q1,...,qTcTX t=1(qt−qt−1) +P1{V(DqT)< V∗} s.t. q0≤q1≤ ··· ≤ qT (1) The collection cost is measured by the difference in data set size between the final and the 0-th round c(qT−q0) =cPT t=1(qt−qt−1), Because we collect data iteratively over multiple rounds (see Figure 1), we break (1) into the sum of differences per round. Specifically in each round, we 1.Decide to grow the data set to qt≥qt−1points by sampling i=1 ∼p(z). Pay a cost c(qt−qt−1)and update D ← D ∪ ˆD. 2. Train the model and evaluate the score. If V(D)≥V∗, then terminate. 3. Ift=T, then pay the penalty Pand terminate. Otherwise, repeat for the next round. The model score typically increases monotonically with data set size [ 5,6]. This means that the minimum cost strategy for (1)is to collect just enough data such that V(DqT) =V∗. We can estimate this minimum data requirement by modeling the score function as a stochastic process. Let Vq:=V(Dq)and let {Vq}q∈Z+be a stochastic process whose indices represent training set sizes in different rounds. Then, collecting data in each round yields a sequence of subsampled data sets Dqt−1⊂ D qtand their performances V(Dqt). The minimum data requirement is the stopping time D∗:= arg min q{q|Vq≥V∗}. (2) which is a random variable giving the first time that we pass the target. Note that q∗ 1=···=q∗ T=D∗ is a minimum cost solution to the optimal data collection problem, incurring a total cost c(D∗−q0)1. 1We assume that c(D∗−q0)< P , since otherwise the optimal strategy would be to collect no data. 3 Estimating D∗using past observations of the learning curve is difficult since we have only Trounds. Further, Mahmood et al. [2]empirically show that small errors in fitting the learning curve can cause massive over- or . Thus, robust policies must capture the uncertainty of estimation. 4 (LOC) Our solution approach, which we refer to as (LOC), minimizes the total collection cost while incorporating the uncertainty of estimating D∗. Although D∗is a discrete random variable, it is realized typically on the order of thousands or greater. To simplify our problem and ensure , we assume that D∗is continuous and has a well-defined density. Assumption 1. The random variable D∗is absolutely continuous and has a cumulative density function (CDF) F(q)and probability density function (PDF) f(q) :=dF(q)/dq. In Section 4.1, we first develop an optimization model when given access to the CDF f(q)and PDF F(q). In Section 4.2, we estimate these distributions and combine them with the optimization model. Finally in Section 4.3, we delineate our optimization approach from prior regression methods. 4.1 Optimization Model We propose an optimization problem that for any t, can simultaneously solve for the optimal amounts of data to collect qt, . . . , q Tin all future rounds. Consider t= 1and to develop intuition, suppose we know a priori the exact stopping time D∗. Then, problem (1) can be re-written as min q1,···qTL(q1, . . . , q T;D∗) s .t. q0≤q1≤ ··· ≤ qT (3) where the objective function is defined recursively as follows L(q1, . . . , q T;D∗) :=c(q1−q0) +1{q1< D∗} c(q2−q1) +1{q2< D∗} c(q3−q2). . . ···+1{qT−1< D∗} c(qT−qT−1) +P1{qT< D∗} ··· =cTX s=11{qs< D∗}+PTY t=11{qs< D∗} =cTX < D∗}+P1{qT< D∗}. The objective differs slightly from (1)due to the indicator terms, which ensure that once we collect enough data, we terminate the problem. The second line follows from gathering the terms. The third line follows from observing that q1≤q2≤ ··· ≤ qTare constrained. In practice, we do not know D∗a priori since it is an unobserved random variable. Instead, suppose we have access to the CDF F(q). Then, we take the expectation over the objective E[L(q1, . . . , q T;D∗)] to formulate a stochastic optimization problem for determining how much data to collect: min q1,···qTcTX t=1(qt−qt−1) (1−F(qt−1)) +P(1−F(qT)) s .t. q0≤q1≤ ··· ≤ qT.(4) Note that the collection variables should be discrete q1, . . . , q T∈Z+, but similar to the modeling ofD∗, we relax the integrality requirement, optimize over continuous variables, and round the final solutions. Furthermore, although problem (4)is constrained, we can re-formulate it with variables dt:=qt−qt−1; this consequently replaces the current constraints with only non-negativity constraints dt≥0. Finally due to Assumption 1, problem (4) can be optimized via gradient descent. 4.2 Learning and Optimizing the Data Requirement Solving problem (4)requires access to the true distribution F(q), which we do not have in reality. In each round, given a current training data set Dqtofqtpoints, we must estimate these distribution functions F(q)andf(q)and then incorporate them into our optimization problem. 4 Given a current data set Dqt, we may sample an increasing sequence of Rsubsets Dqt/R⊂ D 2qt/R⊂ ··· ⊂ D qt, fit our model to each subset, and compute the scores to obtain a data set of the learning , V (Drqt/R))}R r=1. In order to model the distribution of D∗, we can take B bootstrap resamples of Rto fit a series of regression functions and obtain corresponding estimates {ˆDb}B b=1. Given a set of estimates of the data requirement, we estimate the PDF via Kernel Density Estimation (KDE). Finally to fit the CDF, we numerically integrate the PDF. In our complete framework, LOC, we first estimate F(q)andf(q). We then use these models to solve problem (4). Note that in the t-th round of collection, we fix the prior decision variables q1, . . . q t−1 constant. Finally, we collect data as determined by the optimal solution q∗ tto problem (4). Full details of the learning and optimization steps, including the complete Algorithm, are in Appendix B. 4.3 Comparison to Mahmood et al. [2] Our prediction model extends the previous approach of Mahmood et al. [2], who consider only point estimation of D∗. They (i) build the set R, (ii) fit a parametric function ˆv(q;θ)toRvia least-squares minimization, and (iii) solve for ˆD= arg ∗}. They use several parametric functions from the neural scaling law literature, including the power law function (i.e., ˆv(q;θ) :=θ0qθ1+θ2[2,8] where θ:={θ0, θ1, θ2}), and use an ad hoc correction factor obtained by trial and error on past tasks to help decrease the failure rate. Instead, we take bootstrap samples of Rto fit multiple regression functions, estimate a distribution for ˆD, and incorporate them into our novel optimization model. Finally, we show in the next two sections that our optimization problem has analytic solutions and extends to multiple sources. 5 Analytic Solutions for the T= 1Setting In this section, we explore analytic solutions for problem (4). The unobservable D∗and sequential nature suggest this problem can be formulated as a Partially Observable Markov Decision Process (POMDP) with an infinite state and action space (see Appendix C.1), but such problems rarely permit exact solution methods [ 32]. Nonetheless, we can derive exact solutions for the simple case of a single T= 1round, re-stated below min q1c(q1−q0) +P(1−F(q1)) s .t. q0≤q1 (5) Theorem 1. Assume F(q)is strictly increasing and continuous. If there exists c P≤F(q1)−F(q0) q1−q0, ˆϵ≤1−F(q0), P =c/f(F−1(1−ˆϵ)) (6) then there exists an ϵ≤1−F(q0)that satisfies an optimal solution to the corresponding problem (5)isq∗ 1:=F−1(1−ϵ). Otherwise, the optimal solution is q∗ 1:=q0. When the penalty Pis specified via a failure risk ϵ, the optimal solution to problem (5) is equal to a quantile of the distribution of D∗. We defer the proof and some auxiliary results to Appendix C.2. Theorem 1 further provides guidelines on choosing values for the cost and penalty parameters. While cis the dollar-value cost per-sample, which includes acquisition, cleaning, and annotation, Pcan reflect their inherent regret or opportunity cost of failing to meet their target score. A designer can accept a risk ϵof failing to collect enough data Pr{q∗< D∗}=ϵ. From Theorem 1, their optimal strategy should be to collect F−1(1−ϵ)points, which is also the optimal solution to problem (5). 6 The Multi-variate LOC: Collecting Data from Multiple Sources So far, we have assumed that a designer only chooses how much data to collect and must pay a fixed per-sample collection cost. We now explore the multi-variate extension of the data collection problem where there are different types of data with different costs. For example, consider long-tail learning where samples for some rare classes are harder to obtain and thus, more expensive [ 33], learning where labeling data may cost more than collecting unlabeled data [ 34], or domain adaptation where a source data set is easier to obtain than a target set [ 35]. In this section, we highlight our main formulation and defer the complete multi-variate LOC to Appendix D. 5 Consider K∈Ndata sources (e.g., K= 2with labeled and unlabeled) and for each k∈ {1, . . . , K }, letzk∼pk(zk)be data drawn from their distribution. We train a model with a data set D:=∪K k=1Dk where each Dkcontains points of the k-th source. The performance or score function of our model is V(D1, . . . ,DK). For each k, we initialize with qk 0points. Let q0= (q1 0, . . . , qK 0)Tdenote the vector of data set sizes and let c= (c1, . . . , cK)Tdenote costs (i.e., ckis the cost of collecting data from pk(zk)). Given a target V∗, penalty P, andTrounds, we want to minimize the total cost of collection min q1,...,qTcTTX t=1(qt−qt−1) +P1{V(Dq1 T, . . . ,DqK T)< V∗} s.t.q0≤q1≤q2≤ ··· ≤ qT We follow the same steps shown in Section 4 for this problem. First, the learning curve is now a stochastic process {Vq}q∈ZK +indexed in Kdimensions. Next, the multi-variate analogue of the minimum data requirement in (2) is the minimum cost amount of data needed to meet the target: D∗:= arg min q cTq|Vq≥V∗ We randomly pick a unique solution to break ties. From Assumption 1, D∗is a random vector with a PDFf(q)and a CDF F(q) :=Rq 0f(ˆq)dˆq. Finally, the multi-variate analogue of problem (4) is min q1,···,qTcTTX t=1(qt−qt−1) (1−F(qt−1)) +P(1−F(qT)) s.t.q0≤q1≤ ··· ≤ qT (7) The Multi-variate LOC requires multi-variate PDFs, which we can fit in the same way as discussed in Section 4.2. However, we now need multi-variate regression functions that can accommodate different types of data. In Appendix D, we propose an additive family of power law regression functions that can handle an arbitrary number of Ksources. In our experiments, we also generalize the estimation approach of Mahmood et al. [2] to the multi-source setting for comparison. 7 Empirical Results We explore the data collection problem over two sets of experiments covering single-variate K= 1 (Section 4) and multi-variate K= 2 (Section 6) problems. We consider image classification, segmentation, and object detection tasks. For every data set and task, LOC significantly reduces the number of instances where we fail to meet a data requirement V∗, while incurring a competitive cost with respect to the conventional baseline of naïvely estimating the data requirement [2]. In this section, we summarize the main results. We detail our data collection and experiment setup in Appendix E. We expand our full results and experiments with additional baselines in Appendix F . 7.1 Data and Methods When K= 1, the designer decides how much data to sample without controlling the type of data. We explore classification on CIFAR-10 [ 36], CIFAR-100 [ 36], and ImageNet [ 37], where we train ResNets [ 38] to meet a target validation accuracy. We explore semantic segmentation using Deeplabv3 [ 39] on BDD100K [ 40], which is a large-scale driving data set, as well as (BEV) segmentation on nuScenes [ 41] using the ‘Lift Splat’ architecture [ 42]; for both tasks, we desire a target mean (IoU). We explore 2-D object detection on PASCAL VOC [43, 44] using SSD300 [45], where we evaluate mean average precision (mAP). When K= 2, the designer collects two types of data with different costs. We first divide CIFAR-100 into two subsets containing data from the first and last 50 classes, respectively. Here, we assume that the first 50 classes are more expensive to collect than the last; this mimics a real-world scenario where collecting data for some classes (e.g., long-tail) is more expensive than others. We then explore learning on BDD100K where the labeled subset of this data is more expensive than the unlabeled data; the cost difference between these two types is equal to the cost of data annotation. We use a simulation model of the deep learning workflow following the procedure of Mahmood et al. [2], to approximate the true problem while simplifying the experiments (see Appendix E for full details). To avoid repeatedly sampling data, re-training a model, and evaluating the score, each 6 80 85 90 of points q*/D*CIFAR-10 (T=1) Power Law LOC 40 50 60 70 V*1234Ratio of points q*/D*CIFAR-100 (T=1) Power Law LOC 45 50 55 60 65 of points q*/D*Imagenet (T=1) Power Law LOC 66 68 70 72 74 V*012345Ratio of points q*/D*VOC (T=1) Power Law LOC 20 22 24 26 V*02468Ratio of points q*/D*BDD100K (T=1) Power Law LOC 22 24 26 28 30 of points q*/D*nuScenes (T=1) Power Law LOC 80 85 90 of points q*/D*CIFAR-10 (T=3) Power Law LOC 40 50 60 70 of points q*/D*CIFAR-100 (T=3) Power Law LOC 45 50 55 60 65 of points q*/D*Imagenet (T=3) Power Law LOC 66 68 70 72 74 V*0510Ratio of points q*/D*VOC (T=3) Power Law LOC 20 22 24 26 V*01234Ratio of points q*/D*BDD100K (T=3) Power Law LOC 22 24 26 28 30 of points q*/D*nuScenes (T=3) Power Law LOC 80 85 90 of points q*/D*CIFAR-10 (T=5) Power Law LOC 40 50 60 70 of points q*/D*CIFAR-100 (T=5) Power Law LOC 45 50 55 60 65 of points q*/D*Imagenet (T=5) Power Law LOC 66 68 70 72 74 V*012345Ratio of points q*/D*VOC (T=5) Power Law LOC 20 22 24 26 V*123Ratio of points q*/D*BDD100K (T=5) Power Law LOC 22 24 26 28 30 of points q*/D*nuScenes (T=5) Power Law LOCFigure 2: Mean ±standard deviation of 5 seeds of the ratio of data collected q∗ T/D∗for different V∗. The rows correspond to T= 1,3,5and the columns to different data sets. The black line corresponds to collecting exactly the minimum data requirement. LOC almost always remains slightly above the black line, meaning we rarely fail to meet the target. simulation uses a approximation of a ‘ground truth’ learning curve that returns model performance as a function of data set size. In our problems, we initialize with q0= 10% of the full data set (we use 20% for VOC). Then in each round, we solve for the amount of data to collect and then call the learning curve to obtain the current score. We compare LOC against the conventional estimation approach of Mahmood et al. [2]who fit a regression model to the learning curve statistics, extrapolate the learning curve for larger data sets, and then solve for the minimum data requirement under this extrapolation. There are many different regression models that can be used to fit learning curves [ 15,17,5,8]. Since power laws are the most commonly studied approach in the neural scaling law literature, we focus on these. In Appendix F.4, we show that our optimization approach can be incorporated with other regression models. 7.2 Main Results We consider T= 1,3,5rounds and V∗∈[V(Dq0) + 1, V(D)]targets, where Dis the entire data set. We evaluate all methods on (i) the failure rate, which is how often the method fails to achieve the given V∗within Trounds, and (ii) the cost ratio, which is the suboptimality of an algorithm for solving problem (4), i.e.,cT(q∗ . Note that the suboptimality does not count the penalty for failure since this would distort the average metrics. For K= 1, we also measure the ratio of points collected q∗ T/D∗. Although there is a natural trade-off between low cost ratio () and failure rate (), we emphasize that our goal is to have low cost but with zero chance of failure. The Value of Optimization over Estimation when K= 1.Figure 2 compares LOC versus the corresponding power law regression baseline when c= 1 andP= 107(P= 106for VOC and P= 108for ImageNet). If a curve is below the black line, then it failed to collect enough data to meet the target. LOC consistently remains above this black line for most settings. In contrast, even with up to T= 5rounds, collecting data based only on regression estimates leads to failure. Table 1 aggregates the failure rates and cost ratios for each setting. To summarize, LOC fails at less than10% of instances for 12/18settings, whereas regression fails over 30% . In particular, regression nearly always under-collects data when given a single T= 1round. Here, LOC reduces the risk of by 40% to90% over the baseline. While this leads to a marginal increase in costs, our cost ratios are consistently less than , meaning that we spend at most 50% more than the true minimum cost. We remark that previously, Mahmood et al. [2]observed that incorrect regression estimates necessi- tated real machine learning workflows to collect data over multiple rounds. Instead, with LOC, we can make significantly improved data collection decisions even with a single round. Robustness to Cost and Penalty Parameters (see Appendix F.2 for details). Figure 3 evaluates the ratio of points collected for T= 5when the cost and the penalty of the optimization problem are 7 Data set T Power Law Regression LOC Failure rate Cost ratio Failure rate Cost 100% − 60% 0 .19 3 95% 0 .00 32% 0 .05 5 86% 0 .00 29% 0 .03 CIFAR-1001 56% 0 .12 4% 0 .99 3 48% 0 .10 3% 0 .31 5 48% 0 .10 2% 0 .19 Imagenet1 99% 0 .00 37% 0 .49 3 75% 0 .01 5% 0 .16 5 56% 0 .01 2% 0 .10Seg.BDD100K1 77% 0 .03 12% 2 .03 3 31% 0 .00 0% 0 .72 5 23% 0 .01 0% 0 .35 nuScenes1 95% 0 .00 52% 0 .16 3 71% 0 .01 0% 0 .09 5 62% 0 .00 0% 0 .04Det.VOC1 36% 1 .24 25% 0 .56 3 8% 0 .88 0% 1 .10 5 6% 0 .86 0% 0 .84Table 1: Average cost ratio cT(q∗ and failure rate measured over a range of V∗for each Tand data set. We fix c= 1andP= 107(P= 106for VOC and P= 108for ImageNet). The best performing failure rate for each setting is bolded. The cost ratio is measured only for in- stances that achieve V∗. LOC consistently reduces the aver- age failure rate, often down to 0%, while keeping the average cost ratio almost always below 1(i.e., spending at most 2×the optimal amount). 80 85 90 of points q*/D*CIFAR-10 (T=5) Power Law c=1e-03 c=1e-02 c=1e-01 c=1e+00 40 50 60 70 of points q*/D*CIFAR-100 (T=5) Power Law c=1e-03 c=1e-02 c=1e-01 c=1e+00 45 50 55 60 65 of points q*/D*Imagenet (T=5) Power Law c=1e-03 c=1e-02 c=1e-01 c=1e+00 80 85 90 V*0123Ratio of points q*/D*CIFAR-10 (T=5) Power Law P=1e+06 P=1e+07 P=1e+08 P=1e+09 40 50 60 70 of points q*/D*CIFAR-100 (T=5) Power Law P=1e+06 P=1e+07 P=1e+08 P=1e+09 45 50 55 60 65 of points q*/D*Imagenet (T=5) Power Law P=1e+07 P=1e+08 P=1e+09 P=1e+10 66 68 70 72 74 V*02468Ratio of points q*/D*VOC (T=5) Power Law c=1e-03 c=1e-02 c=1e-01 c=1e+00 20 22 24 26 V*123Ratio of points q*/D*BDD100K (T=5) Power Law c=1e-03 c=1e-02 c=1e-01 c=1e+00 22 24 26 28 30 of points q*/D*nuScenes (T=5) Power Law c=1e-03 c=1e-02 c=1e-01 c=1e+00 66 68 70 72 74 V*1000 of points q*/D*VOC (T=5) Power Law P=1e+06 P=1e+07 P=1e+08 P=1e+09 20 22 24 26 V*123Ratio of points q*/D*BDD100K (T=5) Power Law P=1e+06 P=1e+07 P=1e+08 P=1e+09 22 24 26 28 30 of points q*/D*nuScenes (T=5) Power Law P=1e+06 P=1e+07 P=1e+08 P=1e+09 Figure 3: Mean ±standard deviation of 5 seeds of the ratio of data collected q∗ T/D∗for different V∗and fixed T= 5.Rows 1 & 3: We sweep the cost parameter from 0.001to1and fix P= 107. Rows 2 & 4: We sweep the penalty parameter from 106to109and fix c= 1. The dashed black line corresponds to collecting exactly the minimum data requirement. See Appendix F for all T. 8 40 50 60 70 V*102 101 100101102Cost RatioCIFAR-100 (2 Types) (T=5) Power Law c=[0.01 0.005] 40 50 60 70 Rate 40 50 60 70 V*102 101 100101102Cost RatioCIFAR-100 (2 Types) (T=5) Power Law c=[0.01 0.002] 40 50 60 70 Rate 40 50 60 70 V*102 101 100101102Cost RatioCIFAR-100 (2 Types) (T=5) Power Law c=[0.01 0.001] 40 50 60 70 Rate 40 50 60 70 V*102 101 RatioCIFAR-100 (2 Types) (T=5) Power Law c=[0.01 0.0005] 40 50 60 70 Rate 36 38 40 42 V*102 101 100101Cost RatioBDD100K () (T=5) Power Law c=[1. 0.1] 36 38 40 42 Rate 36 38 40 42 V*102 101 100101Cost RatioBDD100K () (T=5) Power Law c=[1. 0.05] 36 38 40 42 Rate 36 38 40 42 V*102 101 100101Cost RatioBDD100K () (T=5) Power Law c=[1. 0.01] 36 38 40 42 Rate 36 38 40 42 V*102 101 100101Cost RatioBDD100K () (T=5) Power Law c=[1. 0.005] 36 38 40 42 RateFigure 4: Mean ±standard deviation over 5 seeds of the cost ratio cT(q∗ and failure rate for different V∗, after removing 99-th percentile outliers. The columns correspond to scenarios where the first set c1costs increasingly more than the second c2. See Appendix F for all T. varied. Our algorithm is robust to variations in these parameters, as LOC retains the same shape and scale for almost every parameter setting and data set. Further, LOC consistently remains above the horizontal 1line, showing that even after varying candP, we do not fail as frequently as the baseline. Finally, validating Theorem 1, the penalty parameter Pprovides natural control over the amount of data collected. As we increase P, the ratio of data collected increases consistently. The Value of Optimization over Estimation when K= 2(Appendix F.3). Figure 4 compares LOC versus regression at T= 5with different costs, showing that we maintain a similar cost ratio to the regression alternative, but with lower failure rates. Table 2 aggregates failure rates and cost ratios for all settings, showing LOC consistently achieves lower failure rates for nearly all settings of T. When T= 5, LOC also achieves lower cost ratios versus regression on CIFAR-100, meaning that with multiple rounds of collection, we can ensure meeting performance requirements while paying nearly the optimal amount of data. However, solving the optimization problem is generally more difficult as Kincreases, and we sometimes over-collect data by large margins. In practice, these outliers can be identified from common sense (e.g., if a policy suggests collecting more data than we can reasonably afford, then we would not use the policy suggestion). Consequently, we report these results after removing the 99-th percentile outliers with respect to total cost for both methods. Nonetheless, this challenge remains when T= 1, particularly for CIFAR-100. 8 Discussion We develop a rigorous framework for optimizing data collection workflows in machine learning applications, by introducing an optimal data collection problem that captures the uncertainty in estimating data requirements. We generalize this problem to more realistic settings where multiple data sources incur different collection costs. We validate our solution algorithm, LOC, on six data sets covering classification, segmentation, and detection tasks to show that we consistently meet pre-determined performance metrics regardless of costs and time horizons. Our approach relies on estimating the CDF and PDF of the minimum data requirement, which is a challenging problem, especially with multiple data sources. Nonetheless, LOC can be deployed on top of future advances in estimating neural scaling laws. Further, we allow practitioners to input costs and penalties, but these quantities may not always be readily available. We provide some theoretical insight into parameter selection and show that LOC is robust to these parameters. Finally, our empirical analysis focuses on computer vision, but we expect our approach to be viable in other domains governed by scaling laws. 9 Data set T Cost Power Law Regression LOC Failure rate Cost ratio Failure rate Cost ratioCIFAR-100 (2 ) 62% 0.89 40% 41.80 (0.01,0.001) 58% 1.19 46% 9.85 (0.01,0.002) 56% 1.55 54% 6.98 (0.01,0.005) 54% 1.65 33% 4.43 3(0.01,0.0005) 43% 3.47 30% 4.88 (0.01,0.001) 45% 1.22 43% 1.31 (0.01,0.002) 45% 1.47 44% 1.21 (0.01,0.005) 38% 1.31 36% 1.17 5(0.01,0.0005) 38% 3.31 24% 5.19 (0.01,0.001) 35% 1.22 24% 0.79 (0.01,0.002) 37% 1.33 38% 0.90 (0.01,0.005) 36% 1.30 24% 0.82BDD100K () 86% 0.11 44% 7.02 (1,0.01) 79% 0.15 30% 13.47 (1,0.05) 72% 0.19 49% 1.02 (1,0.1) 70% 0.19 65% 0.40 3(1,0.005) 23% 0.18 7% 1.20 (1,0.01) 21% 0.15 7% 2.57 (1,0.05) 26% 0.18 23% 0.50 (1,0.1) 26% 0.21 30% 0.15 5(1,0.005) 16% 0.22 2% 1.91 (1,0.01) 21% 0.15 2% 0.86 (1,0.05) 16% 0.17 9% 0.27 (1,0.1) 16% 0.20 7% 0.32Table 2: Average cost ratio cT(q∗ and failure rate over different V∗for each Tandc, after re- moving 99-th percentile out- liers. We fix P= 1013for CIFAR-100 and P= 108for BDD100K. The best perform- ing failure rate for each set- ting is bolded. The cost ra- tio is measured over instances that achieve V∗. LOC con- sistently reduces the average failure rate, and for T > 1, preserves the cost ratio. Fur- ther, LOC is more robust to uneven costs than regression. Improving data collection practices yields potentially positive and negative societal impacts. LOC reduces the collection of extraneous data, which can, in turn, reduce the environmental costs of training models. On the other hand, equitable data collection should also be considered in real-world data collection practices that involve humans. We envision a potential future work to incorporate privacy and fairness constraints to prevent over- or under-sampling of protected groups. Finally, our method is guided by a score function on a held-out validation set. Biases in this set may be exacerbated when optimizing data collection to meet target performance. There is a folklore observation that over 80% of industry machine learning projects fail to reach production, often due to insufficient, noisy, or inappropriate data [ 46,47]. Our experiments verify this by showing that naïvely estimating data requirements will often yield failures to meet target performances. We believe that robust data collection policies obtained via LOC can reduce failures while further guiding practitioners on how to manage both costs and time. and Disclosure of Funding We thank Jonathan Lorraine, Daiqing Li, Jonah Philion, David Acuna, and Zhiding Yu, as well as the anonymous reviewers and meta-reviewers for valuable feedback on earlier versions of this paper.  
sequence to set generative models 	 Generative Models Longtao Tang1, Ying Zhou2and Yu Yang⇤1,3 1School of Data Science, City University of Hong Kong, Hong Kong, China 2Department of Economics and Finance, City University of Hong Kong, Hong Kong, China 3Hong Kong Institute for Data Science, City University of Hong Kong, Hong Kong, China , {ying.zhou, Abstract In this paper, we propose a method that can transform any sequence generative model based on maximum likelihood to a set generative model where we can evaluate the of any set. An efﬁcient importance sampling algorithm is devised to tackle the computational challenge of learning our sequence- to-set model. We present GRU2Set, which is an instance of our method and employs the famous GRU model as the sequence generative model. To further obtain permutation invariant representation of sets, we devise the SetNN model which is also an instance of the model. A direct application of our models is to learn an order/set distribution from a collection of e-commerce orders, which is an essential step in many important operational decisions such as inventory arrangement for fast delivery. Based on the intuition that small-sized sets are usually easier to learn than large sets, we propose a size-bias trick that can help learn better set distributions with respect to the `1-distance evaluation metric. Two e-commerce order datasets, TMALL and HKTVMALL, are used to conduct extensive experiments to show the effectiveness of our models. The experimental results demonstrate that our models can learn better set/order distributions from order data than the baselines. Moreover, no matter what model we use, applying the size-bias trick can always improve the quality of the set distribution learned from data. 1 Introduction Learning a generative model [Goodfellow et al., 2014, Kingma and Welling, 2014] from data to generate random samples is a fundamental unsupervised learning task. In many real applications, the data, such as customer orders and drug packages, can be represented by sets. Learning a set generative model can boost many important applications such as simulating users’ demand of e-commerce orders for inventory arrangement [Wu et al., 2021]. However, in literature, little attention has been paid to generative models for sets. In this paper, we study the problem of learning a set generative model from data. Speciﬁcally, we study sets containing some categorical items from a ﬁnite ground set. Some previous studies [Benson et al., 2018, Wu et al., 2021] proposed traditional statistical models to learn the set generative model, while how to employ deep learning to design set generative models remains largely untouched. Meanwhile, deep generative models for sequence data [Hochreiter and Schmidhuber, 1997, Schäfer and Zimmermann, 2006, Cho et al., 2014, Devlin et al., 2018] have been extensively studied. Since the only difference between a sequence and a set is that the order of items does not matter in a set, we propose to leverage a sequence generative model to build a set generative model. ⇤Yu Yang is the corresponding author. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). A natural idea of converting a random sequence to a random set is to just ignore the order of items. However, for a set, the number of possible sequences that can be regarded as equivalent to the set is exponential to the number of items. This brings us a serious computational challenge where we need to enumerate all the equivalent sequences for a set when learning the parameters of the model. Traditional statistical models [Benson et al., 2018, Wu et al., 2021] along this line either make unrealistic independent assumptions on items or run a learning algorithm of complexity exponential to set sizes to only deal with small sets. These constrained treatments undermine the model’s ability to capture the complex correlations among items in sets. To tackle the computational challenge while still well capturing the correlations among items, in this paper, we make the following technical contributions. In Section 3, we ﬁrst show that we can convert any sequence generative model based on maximum likelihood, such as RNN [Schäfer and Zimmermann, 2006] and DeepDiffuse [Islam et al., 2018], to a set generative model. We call our method . To address the computational challenge, we propose an efﬁcient importance sampling algorithm to estimate the gradient of the log-likelihood function of our model. In Section 4, to capture the complex item correlations, we propose two deep models GRU2Set and SetNN, which are instances of our model. GRU2Set directly uses the GRU as an essential building block, while SetNN is designed for obtaining permutation invariant set embeddings when learning a set generative model. We discuss that GRU2Set and SetNN can express any set distributions as long as we increase the model capacity. To learn a better set distribution from data, in Section 5, we reveal that we often need to modify the empirical size distribution of the training data and propose a heuristic to do so. This size-bias trick can be applied to all set generative models aiming at learning a set distribution. We conduct extensive experiments using two e-commerce datasets TMALL and HKTVMALL, and report the experimental results in Section 6. The experimental results clearly show the superiority of our models to the baselines and the effectiveness of our size-bias trick. 2 Related Work Learning a set generative model is often regarded as learning a choice model for subset selection. Along this line, Benson et al. [2018] proposed a discrete choice model (DCM) to model the util- that users select a speciﬁc set. Wu et al. [2021] utilized representation learning to design a parameterized Markov Random Walk to model how users select random sets from an item graph. Both [Benson et al., 2018] and [Wu et al., 2021] are set generative models, though they are not deep models. To achieve practical computational efﬁciency, both [Benson et al., 2018] and [Wu et al., 2021] only handle small sets and compromise on modeling high-order item correlations. To model the permutation invariant nature of sets, Ruiz et al. [2017] used the sum or max of item embedding as the set embedding. Zaheer et al. [2017] systematically investigated the theory of permutation invariant functions. Stelzner et al. [2020] proposed a generative model for point clouds based on GAN [Goodfellow et al., 2014] and Set Transformer [Lee et al., 2019]. Although a point cloud is a set of 2D-coordinates, we cannot use the point cloud generative model [Stelzner et al., 2020] to solve our problem as the model outputs continuous random variables instead of categorical data. Kosiorek et al. [2020] presented another generative model for sets of real-valued vectors based on Set Transformer [Lee et al., 2019] and their generating process is based on V AE [Kingma and Welling, 2014]. A disadvantage of using GAN or V AE to train a generative model is that it would be difﬁcult to evaluate the probability density of a random sample. Normalizing Flows are powerful generative models based on solid statistical theories. However, the study of Discrete Flows is still in its infancy [Hoogeboom et al., 2019, Tran et al., 2019] and no existing works have investigated using Discrete Flows to learn set distributions. Multi-label classiﬁcation also needs to model the permutation invariance of sets of labels. Yang et al. [2019] applied reinforcement learning with a reward function to remove the impact of label orders. Yazici et al. [2020] proposed to align the labels of a sample with its predicted labels before calculating the loss of the sample. However, our task in this paper is an unsupervised learning task. We cannot apply the loss functions in [Yang et al., 2019, Yazici et al., 2020] since we do not have “labels” (or feature vectors if one regards a set as a set of labels) for training samples. 2 To sum up, existing works on set generative models either deal with sets of real-valued vectors which are different from categorical sets, or adopt non-deep models which need to compromise on capturing item correlations to make the computation efﬁcient. It is still urgent to design deep generative models for categorical sets and propose efﬁcient algorithms to deal with computational challenges. 3 Generative Models In this section, we introduce how to convert a sequence generative model to a set generative model and how to optimize the model parameters. 3.1 Converting Sequences to Sets We ﬁrst characterize the sequence generative model that we want to convert to a set generative model. Deﬁnition 1 (Sequence Generative Model) A sequence generative model is a model with a se- quential generating process, which can be represented by a sequence of states and actions [s0,a0,s1,a1,. . . ,s T 1,aT 1,sT], where siis the state at time i,aiis the action taken at time iand sTis the end state. We call [s0,a0,s1,a1,. . . ,s path . Like a ﬁnite-state machine, the sequence generative model uses each action update the current state sitosi+1. For each state s, the sequence generative model has a corresponding probability distribution the probability of choosing action awhen the current state is s, where ✓is the parameters of the sequence generative model. Our deﬁnition of the sequence generative model can cover a wide range of models that can generate items in a sequential manner. For example, the commonly used sequence model RNN is an instance, where the state is the history vector Hand an action is to choose the next word/item. Moreover, inﬂuence diffusion models such as the Independent Cascade (IC) model [Goldenberg et al., 2001] and the DeepDiffuse model [Islam et al., 2018] are also covered by Deﬁnition 1, since in these models the state at time tis the set of activated nodes until time tand the action atis to select the group of nodes to be activated at time t+1. To convert a sequence generative model in Deﬁnition 1 to a set generative model, we need to associate each state swith a set Ss. For example, in an RNN, we can deﬁne Sstas the set of items generated in the ﬁrst tsteps. In the IC model, we can deﬁne Sstas the set of nodes activated at or before time t. We say that a generating path l=[s0,a0,...,s a set SifS=SsT. LetL(S)= {l|lcan induce S}be the path set ofS. As the sequence generative model has the probability distribution the probability of taking aas the next action given the current state s, we can derive the probability of having a path l=[s0,a0,...,s T 1,aT 1,sT]as ;✓)⇥···⇥ p(aT|sT;✓). Therefore, given a sequence generative model with parameters ✓, the probability to induce a set Sis p(S;✓)=X l2L(S)p(l;✓). 3.2 Parameter Learning Suppose we have a collection (multiset) of observed sets S={S1,S2,. . . ,S N}. We want to use Sas the training data to learn a sequence model which can generate the observed sets. We adopt maximum likelihood estimation to learn the parameters ✓. Speciﬁcally, the loss function is L(✓)= NX i=1log0 @X l2L(Si)p(l;✓)1 A We apply stochastic gradient descent based algorithms to minimize the loss function. The stochastic gradient w.r.t. an observation Sisrlogp(S;✓)= rlog⇣P l2L(S)p(l;✓)⌘ . As in the log-sum term of rlogp(S;✓), the number of paths that can induce Sis often exponential to |S|(in some 3 sequence generative model the number of such paths is even inﬁnite), it is very challenging to evaluate . To tackle the computational challenge in computing rlogp(S;✓), we ﬁrst ﬁnd that rlogp(S;✓) can be regarded as an expectation as follows. rlogp(S;✓)=P l2L(S)rp(l;✓) p(S;✓)=P ;✓) ;✓)], where p(l|S;✓)=p(l;✓) p(S;✓)is a posterior distribution indicating the probability that a given set S is generated by the path l. Therefore, rlogp(S;✓)can be seen as the expected value of rp(l;✓) where the random variable lfollows the posterior distribution p(l|S;✓). We can apply Monte Carlo method to estimate ;✓)], where the key point is to sample generating paths from p(l|S;✓). A naive idea is to use rejection sampling, where we use the model with parameters ✓to generate random paths and reject those that cannot induce S. Obviously such a naive implementation would be extremely inefﬁcient as the rejection rate would be very high. To sample lfrom p(l|S;✓)more efﬁciently, we devise an important sampling method as follows. Let ⌧(a, s, S )be an indicator function, where ⌧(a, s, S )=1 thatlinduces  1},si=s^ai=a. In our sampling algorithm, for a state st, we only consider actions atsuch that ⌧(at,s ,S)=1 . We take such an action atwith probability a:⌧(a,s,S )=1p(a|s;✓). Therefore, the proposal distribution T 1,aT 1,sT]in our importance sampling is )⇥···⇥ ) The (unnormalized) importance weight of lis r(l;✓)=0 @X A⇥0 @X A⇥· · ·⇥0 @X A Clearly, we have ;✓). This shows the correctness of the sampling method. An EM perspective Using our importance sampling algorithm to estimate rlogp(S;✓)in min- imizing the loss function L(✓)can be interpreted from an algorithm’s perspective. Our method is actually a Monte Carlo EM algorithm if we treat a generating path l as a latent variable. In the E-step, we sample the latent variable lfrom the posterior distribution p(l|S;✓old). Our importance sampling can boost the efﬁciency of the E-step. In the M-step, we reconstruct an object function which isPM ), where Mis the number of samples and R=PM i=1r(li). To maximize this function, we update ✓by gradient based meth- ods, such as ✓new=✓old+⌘PM ), where ⌘is the learning rate andPM estimated by our importance sampling algorithm. MRW as an instance of Sequence2Set We show that the Markov Random Walk (MRW) model [Wu et al., 2021] is also an instance of our Sequence2Set model. The sequence model in MRW is a random walk on the item graph [Wu et al., 2021] where a node is an item and we have a special stop item . The state stat time tis associated with a set Sstwhich includes all the items visited until time t. An action attaken at time tgiven the state stis to jump to an item itfrom it 1, the item visited at time t 1. Once the stop item is visited, the random walk terminates. The probability of jumping from it 1toit=iis parameterized as p(it=i|st 1;✓), where ✓is the embeddings of items. Speciﬁcally, in MRW, ( e> ieit 1)P jexp( e> jeit 1), where eiis the embedding 4 vector of item i. When an action ais jumping to an item i6=, the indicator ⌧(a, s, S )=1 ifi2S. When an action ais jumping to the stop item ,⌧(a, s, S )=1 ifSs=S. Wu et al. [2021] propose an algorithm of complexity exponential to |S|to calculate the gradient rlogp(S;✓)for learning the item embeddings and thus, only small sets of size at most 4 are considered in [Wu et al., 2021]. If we adopt our important sampling algorithm, we can deal with large sets. 4 GRU2Set and SetNN In this section, we propose two deep models GRU2Set and Set Neural Networks (SetNN). We ﬁrst introduce some general settings that will be used in both GRU2Set and SetNN. Item embedding and stop item We also adopt item embeddings, which should be learned from the training dataset, to parameterize the probability p(a|s;✓)in the sequence generative model. We assign an embedding vector ei2Rdto each item i. Similar to MRW [Wu et al., 2021], we also add a stop item where once we add to the set associated to the current state, we stop the generating process. The stop item also has an embedding vector e2Rd. Figure 1: An example of sparse item graph. The item is the stop item.Sparse item graph In our model, the indicator function ⌧(a, s, S )is a key for us to efﬁciently sample generating paths from the posterior distribution p(l|S;✓). Intuitively, ⌧(a, s, S )indicates what actions are valid given the current state sand the induced set S. To further boost the efﬁciency of our importance sampling algorithm, we make extra constraints on the items that can be added in the next step when we are given the current state s. To do so, we build an item graph G=hV,Ei, where Vis the set of items plus the stop item  and(i, j)2Eif there is a set Sin the training dataset Ssuch that i2Sandj2S. The stop item is connected to all the other items. When using our importance sampling algorithm, given the current state sassociated with a set Ss✓S,⌧(a, s, S )=1 if action ais adding an item i2S\Neighbor (Ss), where Neighbor (Ss)is the set of neighbors to items in Ss. When starting from the ﬁrst state s0associated with a set Ss0=;, we set Neighbor (Ss0)=V∩{}. In real set datasets such as TMALL and HKTVMALL used in our experiments, such an item graph built from training datasets is often very sparse. Therefore, we call the item graph the sparse item graph . Clearly, the sparsity of the item graph can help boost the efﬁciency of generating random sets from a learned model. Choice vector In our model design, the conditional probability p(a|s;✓)is the key to control the generating process. Exploiting the idea of representation learning, we represent each state swith an embedding vector cs2Rd. We call csthechoice vector ofs. Given s, we control the probability of adding ias the next item as p(i|s;✓)=exp( c> sei)P j2Neighbor (Ss)exp( c>sej). The sparsity of the item graph can make computing the normalization term in . Note that in MRW [Wu et al., 2021], the choice vector csis the embedding of the last item visited so far. We will see that in GRU2Set and SetNN, we use deep neural nets to aggregate the embeddings of all the items visited so far to obtain the choice vector cs. The initial choice vector cs0is trainable. 4.1 GRU2Set We leverage GRU [Cho et al., 2014] to build our ﬁrst model, since GRU is one of the most popular sequential model based on RNN and its computational cost is low. We set the history vector Hof GRU as a d-dimensional vector and His regarded as the choice vector of the current state. The initial history vector H0is trainable and His updated according to the design of GRU. The set Sstfor the state stat time tis just the set of items added in the ﬁrst tsteps of the sequence generating process of GRU. When the stop item is added, we terminate the generating process. Due to limited space, the overview of GRU2Set is put in Appendix. 5 Figure 2: Overview of SetNN. We do not show the situation when we add the stop item. If the action is adding the stop item, we end the process immediately. 4.2 SetNN The choice vector cscan be regarded as the embedding of the set Ss. In GRU2Set, the choice vector, which is the history vector H, is sensitive to the order of items added to Ss. As sets are permutation invariant, we hope to have a model which can also produce permutation invariant of sets. Therefore, we develop the Set Neural Networks (SetNN) where the key idea is to design a permutation invariant aggregation of embeddings of items in a set. The following theorem by Zaheer et al. [2017] guides our model design. Theorem 1 ([Zaheer et al., 2017]) A function f(X)operating on a set Xhaving elements from a countable universe, is a set function, i.e., invariant to the permutation of instances in X, iff it can be decomposed in the form ⇢(P x2X (x))for suitable  and⇢.  (x)can be viewed as the embedding of xand the sum over  (x)can be replaced by other aggregate operators such as mean or pooling. In SetNN, we use an MLP to express the function ⇢in Theorem 1 to obtain the embedding (choice vector) csofSsfor any state s. Fig 2 shows the overview of SetNN. 4.3 Expressive Power of GRU2Set and SetNN Compared to SetNN, our GRU2Set model uses the history vector H. By the expressive power of RNN [Schäfer and Zimmermann, 2006], as we increase the embedding dimension d, hidden neural units and layers in GRU module, the history vector Hof GRU2Set can approximate any function on a ﬁnite history path arbitrarily close. Then, of course, Hcan express any permutation invariant function on sets which implies that the expressive power of GRU2Set is more than SetNN. To explore the expressive power of SetNN, we ﬁrst give a recursive deﬁnition of the probability that a setSis generated by SetNN, denoted by p(S), as follows. the stop item)  (S)=X })  (;)=1 ,(1) where p(x|S∩{x})is the conditional probability of adding item xto the set S∩{x}and (S)is the probability of reaching a state ssuch that Ss=Sin SetNN. The following theorem shows that Eq. (1) is general enough to cover all possible set distributions. Theorem 2 For any distribution q(S)on all subsets of a ground set of items, there exists a group of transition probabilities p(x|S)andp(|S), such that q(S)=p(S)holds for any S, where p(S)is deﬁned in Eq. (1). 6 The proof of Theorem 2 can be found in Appendix B. As we increase the capacity of the MLP in SetNN, SetNN can represent any conditional probability p(x|S)andp(|S), and as a result of Theorem 2, SetNN and GRU2Set can express any set distributions. 5 Size-Bias Trick for Improving Learned Set Distributions In this section, we propose a size-bias trick that can help reduce the distance between the ground truth set distribution p⇤(S)and the learned distribution p(S), where p(S)can be learned by any generative model not limited to our models. We ﬁrst decompose the ground truth distribution p⇤(S)when |S|=kas ⇤ k, where p⇤ k=P A:|A|=kp(A)is the probability of generating a set of size k, and p⇤(k)(S)=p(S) p⇤ kis the probability that a random size- kset is S. Suppose we know the real size distribution p⇤ k and we have q(k)(S)as an estimation of p⇤(k)(S)for each k. How can we construct a distribution q(S)close to p⇤(S)? An intuitive way is to combine q(k)(S)andp⇤ ksuch that q(S)=q(k)(S)p⇤ kif |S|=k[Stelzner et al., 2020, Benson et al., 2018]. However, we argue that this intuitive way is not always the optimal plan as follows. LetKbe the largest possible set size. To ﬁnd the best size distribution qkto collaborate with our estimation q(k)(S), we consider the following optimization aiming at minimizing the KL-divergence between the distribution the ground truth p⇤(S). min q1,...,q KKL(q||p⇤)=X Sq(S) logq(S) p⇤(S) s.t. q ( 0qk1,8k KX k=1qk=1(2) Since we do not know p⇤(S), we cannot solve Eq. (2)exactly. However, we still can tell if setting qk=p⇤ kis the optimal solution to some extend. Letfk(qk)=P S:|S|=kq(S) logq(S) p⇤(S). We have KL(q||p⇤)=P kfk(qk). Note that fk(qk)only depends on qk. We can interpret Eq. (2)as a portfolio optimization problem. There are Kprojects. qk is the investment for project kand fk(qk)is the revenue of project k. To decide the best portfolio qk, we check the derivative of fk(qk), which indicates the marginal revenue. We have f0 k(qk)=1+X S:|S|=kq(k)(S) logq(S) p⇤(S) By taking qk=p⇤ k, we have f0 k(p⇤ k)=1+X S:|S|=kq(k)(S) logq(k)(S) p⇤(k)(S)=1+ KL⇣ )⌘ As the estimation quality of q(k)(S)often varies for different k, the derivative f0 k(p⇤ k)probably also varies for different k, making qk=p⇤ knot a stationary point. Therefore, we probably need to set qk different from p⇤ kfor solving Eq. (2). We ﬁrst make the following reasonable assumption. Assumption 1 Small-sized sets are easier to learn than large sets. In other words, p⇤(k)(S)better as the set size kdecreases. Based on Assumption 1, compared to p⇤ k, we should “invest” more on small set sizes to construct the “portfolio” (size distribution) qk. Therefore, we set qkslightly bigger than p⇤ kfor small k, and set qk smaller than p⇤ kfor big k. To address the issue of not knowing p⇤ k, we can resort to the empirical size 7 Table 1: Basic statistics dataset #item #training sets average #testing sets average #edge TMALL 1,363 100,000 393,226 60,956 HKTVMALL 782 200,000 616,652 53,853 Table 2: Size distribution of TMALL and HKTVMALL 1234 5 6  7 TMALL 38% 18% 14% 10% 7% 5% 8% HKTVMALL 42% 17% 11% 8% 6% 5% 12% distribution of the training dataset, which is a good estimation of the real size distribution p⇤ k.W e construct a biased size distribution qkby adopting the following heuristic. Constructing Biased Size Distribution Suppose the empirical size distribution of the training data Sis[p1,p2,. . . ,p K]. We ﬁrst calculate the rest proportion by ), which means pi=( 1  . Then we calculate r0 k= max {rk+kp |V|/|S|,1}, where Vis the set of all items. We construct a biased size distribution [q1,q2,...,q K]by setting qi=( 1  r0 1)...(1 r0 i 1)r0 i. 6 Experiments Datasets We did our empirical study on two real-world datasets of customer orders from online e-commerce platforms. This ﬁrst dataset is TMALL ( ) which contains 1363 items and orders from August 2018 to March 2019. We treated the collection of orders in each month as an instance and in total we have 8 instances for the TMALL dataset. The second dataset is HKTVMALL ( ) which contains 728items form the supermarket sector and orders from February 2020 to September 2020. Similar to TMALL, we also treated HKTVMALL as 8 instances where each instance is the collection of orders in each month. For both datasets, We split all the orders in a month to a training dataset Strainand a testing dataset Stest, where the size of Strainis 100,000 for Tmall and 200,000 for HKTVmall. The datasets can be found in the source code. We report the sparsity of item graphs and statistics of the datasets in Table 1. The set size distribution of each dataset is shown in Table 2. Experiment Setting and Evaluation We treat the method of directly using the histogram of Strain as the benchmark method, as it is a natural and unbiased method to estimate the set distribution. Note that this benchmark method has a major disadvantage that its cannot model probabilities of any sets not showing in Strain. We also compare our models with two baselines which are DCM (Discrete Choice Model) [Benson et al., 2018] and MRW [Wu et al., 2021]. Following [Wu et al., 2021], for each method (except the benchmark), we generated a collection Spredof 10,000,000 random sets and used the empirical set distribution of Spredto approximate the set distribution learned by the model. We regarded the empirical distribution of the testing data Stest as the pseudo ground truth. Since the bigger Stestis, the more accurate the pseudo ground truth is, we set the size of Stestbigger than Strainas shown in Table 1. Denote by Ntest(S)the number of sets that are SinStest. Let Npred(S)be the number of sets that are SinSpred. We used the `1-distance between a model’s (approximate) distribution and the pseudo ground truth to evaluate the effectiveness of the model. Speciﬁcally, the `1-distance can be calculated as follows. ` S2S test[S pred|Ntest(S) |Stest| Npred(S) |Spred|| (3) We set the embedding dimension of all methods as 10. For each training and testing dataset, the biased size distribution was the same for all methods and was obtained by the heuristic introduced at the end of Section 5. For all experiments, the MLP of SetNN only has one hidden layer which contains 50 neural units. All the experiments were ran on a CPU with 10 cores. The optimizer used by us is RMSProp with default parameter in PyTorch. The source code and data used in our experiments can be found at . 8 Table 3: The performance of each groups on TMALL. G0 stands for the ﬁrst group of data. G0 G1 G2 G3 G4 G5 G6 G7 Average STDEV Benchmark 1.11 1.02 0.91 0.90 0.94 0.94 0.93 0.85 0.95 0.07 +Size Bias 1.06 0.97 0.87 0.86 0.90 0.90 0.89 0.81 0.91 0.07 DCM 1.17 1.08 0.95 0.94 1.00 0.99 0.99 0.89 1.00 0.08 +Size Bias 1.10 1.01 0.90 0.89 0.94 0.93 0.93 0.84 0.94 0.07 MRW 1.14 1.07 0.99 0.96 1.02 1.02 1.03 0.92 1.02 0.06 +Size Bias 1.06 0.97 0.88 0.87 0.90 0.92 0.95 0.84 0.92 0.06 SetNN 1.12 1.01 0.92 0.89 0.90 0.91 1.01 0.89 0.96 0.08 +Size Bias 1.04 0.93 0.84 0.82 0.85 0.87 0.89 0.79 0.88 0.07 GRU2Set 1.11 1.04 0.89 0.90 0.94 0.94 1.00 0.90 0.97 0.07 +Size Bias 1.02 0.93 0.83 0.82 0.85 0.86 0.89 0.80 0.87 0.06 Table 4: The performance of each groups on HKTVMALL. G0 stands for the ﬁrst group of data. G0 G1 G2 G3 G4 G5 G6 G7 Average STDEV Benchmark 0.73 0.87 0.91 0.84 0.81 0.87 0.81 0.80 0.83 0.05 +Size Bias 0.70 0.83 0.88 0.81 0.79 0.84 0.78 0.77 0.80 0.05 DCM 0.76 0.89 0.93 0.88 0.84 0.89 0.84 0.82 0.86 0.05 +Size Bias 0.72 0.84 0.89 0.83 0.80 0.85 0.79 0.78 0.81 0.05 MRW 0.80 0.91 0.94 0.88 0.88 0.90 0.85 0.85 0.88 0.04 +Size Bias 0.74 0.84 0.88 0.82 0.83 0.85 0.79 0.80 0.82 0.04 SetNN 0.82 0.86 1.00 0.84 0.89 0.92 0.84 0.84 0.88 0.05 +Size Bias 0.71 0.82 0.87 0.79 0.79 0.83 0.76 0.78 0.79 0.04 GRU2Set 0.83 0.87 0.94 0.84 0.93 0.94 0.86 0.82 0.88 0.05 +Size Bias 0.70 0.80 0.85 0.78 0.79 0.82 0.75 0.75 0.78 0.04 Experiment Results Due to limited space, we only report the main experimental results. More detailed experimental results can be found in Appendix D and E. Table 3 and Table 4 show the experimental results. We place the results of using the size-bias trick below the original results. We ﬁnd that both DCM and MRW never beat the benchmark method, no matter the size-bias trick is played or not. A possible reason is that in our experiments we did not make size constraints on sets, while Wu et al. [2021] constrained the set size to be at most 4 and Benson et al. [2018] set the maximum size of sets to be 5. Big sets in our data may have negative effects on learning both DCM and MRW. We can see that applying the size-bias trick can always reduce the `1-distance signiﬁcantly, which demonstrates the effectiveness of this trick. Before applying the size-bias trick, the benchmark method has the best performance which is slightly better than our models GRU2Set and SetNN. However, after applying the size-bias trick, our GRU2Set model becomes the best and it outperforms the benchmark method by 4% on TMALL and 2.5% on HKTVMALL. SetNN also outperforms the benchmark method after using the size-bias trick. This suggests that GRU2Set and SetNN may learn probabilities of small sets better than the benchmark method. The SetNN model performs slightly worse than GRU2Set but the gap between them is small. A possible reason is that GRU2Set has stronger expressive power than SetNN as illustrated in Section 4.3. GRU2Set records the order of items added to the set, while SetNN ignores such order to achieve permutation invariant set embeddings. This shows that GRU2Set includes more information in the generating process than SetNN. However, we want to emphasize that SetNN can produce permutation invariant set embeddings while GRU2Set cannot. If we have a downstream task that needs set embeddings, SetNN may have more advantages in this case. 9 7 Conclusion In this paper, we present a method to build generative models for set data and the SOTA method MRW is an instance of our method. To utilize deep learning in learning set distributions, we further design two models, GRU2Set and SetNN, which are two instances of our model. To learn better set distributions from data, we also propose a size-bias trick. Experimental results on two e-commerce order datasets clearly show that our models outperform the baselines including two SOTA methods in this line of research. For future work, we will explore using more sophisticated deep learning modules in our model, such as replacing the MLP in SetNN with Set Transformer and substituting the RNN in GRU with more advanced sequence models. For potential negative societal impacts, learning the embeddings of items and sets might cause user privacy leakage as many other embeddings might do. Research on how to hide personal information on set data could help protect users’ privacy. and Disclosure of Funding Tang and Yang’s research is supported in part by the Hong Kong Research Grants Council under ECS grant 21214720, City University of Hong Kong under Project 9610465, and Alibaba Group through Alibaba Innovative Research (AIR) Program. Zhou’s research is supported in part by City University of Hong Kong under Project 7200694. The authors thank the HKTVmall Open Databank for providing the HKTVMALL dataset. All opinions, ﬁndings, conclusions, and in this paper are those of the authors and do not necessarily reﬂect the views of the funding agencies. References Austin R. Benson, Ravi Kumar, and Andrew Tomkins. A discrete choice model for subset selection. InProceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018 . ACM, 2018. Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase using RNN for statistical machine translation. CoRR , abs/1406.1078, 2014. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint , 2018. Jacob Goldenberg, Barak Libai, and Erika Muller. Using complex systems analysis to advance marketing theory development. 2001. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. CoRR , 2014. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8): 1735–1780, 1997. Emiel Hoogeboom, Jorn W.T. Peters, Rianne van den Berg, and Max Welling. Integer Discrete Flows and Lossless Compression . Curran Associates Inc., Red Hook, NY, USA, 2019. Mohammad Raihanul Islam, Sathappan Muthiah, Bijaya Adhikari, B Aditya Prakash, and Naren Ra- makrishnan. Deepdiffuse: Predicting cascades. In 2018 IEEE International Conference on Data Mining (ICDM) , pages 1055–1060. IEEE, 2018. Sudeep Kamath, Alon Orlitsky, Dheeraj Pichapati, and Ananda Theertha Suresh. On learning distributions from their samples. In Conference on Learning Theory , pages 1066–1100. PMLR, 2015. Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International Conference on Learning , ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings , 2014. 10 Adam R. Kosiorek, Hyunjik Kim, and Danilo J. Rezende. Conditional set generation with transform- ers.CoRR , 2020. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for neural networks. In ICML 2019 . PMLR, 2019. Francisco J. R. Ruiz, Susan Athey, and David M. Blei. SHOPPER: A probabilistic model of consumer choice with substitutes and complements. CoRR , abs/1711.03560, 2017. Anton Maximilian Schäfer and Hans Georg Zimmermann. Recurrent neural networks are universal approximators. In Stefanos D. Kollias, Andreas Stafylopatis, Włodzisław Duch, and Erkki Oja, editors, Artiﬁcial Neural Networks – ICANN 2006 , pages 632–640, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. Karl Stelzner, Kristian Kersting, and Adam R Kosiorek. Generative adversarial set transformers. In Workshop on Learning at ICML , volume 2020, 2020. Dustin Tran, Keyon Vafa, Kumar Krishna Agrawal, Laurent Dinh, and Ben Poole. Discrete Flows: Invertible Generative Models of Discrete Data . Curran Associates Inc., Red Hook, NY, USA, 2019. Tongwen Wu, Yu Yang, Yanzhi Li, Huiqiang Mao, Liming Li, Xiaoqing Wang, and Yuming Deng. Representation Learning for Predicting Customer Orders . Association for Computing Machinery, New York, NY, USA, 2021. ISBN 9781450383325. Pengcheng Yang, Fuli Luo, Shuming Ma, Junyang Lin, and Xu Sun. A deep reinforced sequence- to-set model for multi-label classiﬁcation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5252–5258, 2019. Vacit Oguz Yazici, Abel , Arnau Ramisa, Bartlomiej Twardowski, and Joost van de Weijer. Orderless recurrent models for multi-label classiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13440–13449, 2020. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. Checklist 1.For all authors... (a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b)Did you describe the limitations of your work? [Yes] It was described in Conclusion. (c)Did you discuss any potential negative societal impacts of your work? [Yes] We discuss the privacy risk. (d)Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2.If you are including theoretical results... (a)Did you state the full set of assumptions of all theoretical results? [Yes] (b)Did you include complete proofs of all theoretical results? [Yes] All the proofs can be found in the supplemental material. 3.If you ran experiments... (a)Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] (b)Did you specify all the training details (e.g., data splits, , how they were chosen)? [Yes] 11 (c)Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] The standard deviation among 8 groups and detail results can be ﬁnd in Table 3 and Table 4 in the supplemental material. (d)Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] All experiments were ran on a CPU of 10 cores. 4.If you are using existing assets (e.g., code, data, models) or new assets... (a)If your work uses existing assets, did you cite the creators? [Yes] We use the MRW’s code to compare their results and we cite it. (b)Did you mention the license of the assets? [Yes] the code license can be ﬁnd in Github (c)Did you include any new assets either in the supplemental material or as a URL? [Yes] In the source code (d)Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [Yes] The platforms (Tmall and HKTVMall) where we collect our data have already obtained such consent. (e)Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [Yes] The datasets we use are already anonymized. 5.If you used crowdsourcing or conducted research with human subjects.... (a)Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b)Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c)Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 12 
understanding the failure of batch normalization for transformers in nlp 	Understanding the Failure of Batch Normalization for Transformers in NLP Jiaxi Wang1, Ji Wu1,2, Lei Huang3 1Department of Electronic Engineering, Tsinghua University 2Institute for Precision Medicine, Tsinghua University {wjx20@mails, 3SKLSDE, Institute of Artiﬁcial Intelligence, Beihang University Abstract Batch Normalization (BN) is a core and prevalent technique in accelerating the training of deep neural networks and improving the generalization on Computer Vision (CV) tasks. However, it fails to defend its position in Natural Language Processing (NLP), which is dominated by Layer Normalization (LN). In this paper, we are trying to answer why BN usually performs worse than LN in NLP tasks with Transformer models. We ﬁnd that the inconsistency between training and inference of BN is the leading cause that results in the failure of BN in NLP. We deﬁne Training Inference Discrepancy (TID) to quantitatively measure this inconsistency and reveal that TID can indicate BN’s performance, supported by extensive experiments, including image classiﬁcation, neural machine translation, language modeling, sequence labeling, and text classiﬁcation tasks. We ﬁnd that BN can obtain much better test performance than LN when TID keeps small through training. To suppress the explosion of TID, we propose Regularized BN (RBN) that adds a simple regularization term to narrow the gap between batch statistics and population statistics of BN. RBN improves the performance of BN consistently and outperforms or is on par with LN on 17 out of 20 settings, involving ten datasets and two common variants of Transformer1. 1 Introduction Deep learning [ 19] has revolutionized Computer Vision (CV) [ 18] and Natural Language Processing (NLP) [ 39]. Normalization layers are key components to stabilize and accelerate the training in Deep Neural Networks (DNNs). In CV , Batch Normalization (BN) [ 15] is the default normalization technique and reveals superior performance over other normalization techniques in image recognition tasks by enforcing the input of a neuron to have zero mean and unit variance within a mini-batch data. Furthermore, a growing number of theoretical works analyze the excellent properties of BN in beneﬁting optimization [ 15,34,4,12,7,8]. While BN almost dominates in CV with empirical success and theoretical properties, Layer Normalization (LN) is the leading normalization technique in NLP, especially for Transformer models that achieve the performance on extensive tasks, including machine translation [ 39], natural language understanding [ 9], text generation [ 32], few shot learning [ 5], to name a few. As a direct substitute of LN, BN performs poorly in Transformer for neural machine translation [ 36]. It remains elusive to explain the failure of BN in NLP community. In this work, we are trying to take a step forward. Our contributions are summarized as follows: 1Our code is available at 36th Conference on Neural Information Processing Systems (NeurIPS 2022). •We ﬁnd that the inconsistency between training and inference leads to the failure of BN in NLP, supported by our extensive experiments, including image classiﬁcation, neural machine translation, language modeling, sequence labeling, and text classiﬁcation tasks. •We deﬁne Training Inference Discrepancy (TID) to quantitatively measure this inconsistency and show that TID can serve as an indicator of BN’s performance. In particular, BN reaches much better test performance than LN when TID keeps small through training, e.g., in image recognition and language modeling tasks. •We propose Regularized BN (RBN) that adds a regularization term in BN to penalize and reduce the TID when the TID of BN is large. We reveal the optimization advantages of RBN over LN by exploring the layer-wise training dynamics of Transformer. •We empirically show that RBN can exceed or match the performance of LN, sometimes with a large margin, on 17 out of 20 settings, involving ten datasets and two common variants of Transformer. Besides, RBN introduces no extra computation at inference compared to LN. 2 Related Work Analyses of BN’s Success As BN becomes an indispensable component in deep neural networks deployed in CV tasks, a bunch of works explore the theoretical reasons behind its success. From the view of optimization, the original BN paper [ 15] argues that BN can reduce internal covariate shift and thus stabilize the training, while Santurkar et al. [34] debate that BN could smooth the loss landscape and thus enable training of neural network with larger learning rate [ 4]. Daneshmand et al. [7,8]prove that a stack of randomized linear layers and BN layers will endow the intermediate features of neural network with sufﬁcient numerical rank as depth increases, which is beneﬁcial for optimization and learning discriminative hierarchical features. Huang et al. [12] show that BN could improve the layer-wise conditioning of the neural network optimization by exploring the spectrum of Hessian matrix with block diagonal approximation [ 26]. From the view of generalization, Ioffe and Szegedy [15], Luo et al. [23], Li et al. [20], Wu and Johnson [41] argue that BN serves as regularizer which reduces over-ﬁtting when its stochasticity is small and may have detrimental effect when it is large [ 41]. Huang et al. [11] further propose Stochastic Normalization Disturbance (SND) to measure such stochasticity and shows that large SND will hinder the training of neural networks. Training Inference Inconsistency of BN Normalizing along the batch dimension usually intro- duces training inference inconsistency since mini-batch data is neither necessary nor desirable during inference. BN uses population statistics, estimated by running average over mini-batch statistics, for inference. The training inference inconsistency usually harms the performance of BN for small- batch-size training since the estimation of population statistics could be inaccurate [ 40]. One way to reduce the inconsistency between training and inference is to exploit the estimated population statistics for normalization during training [ 14,6,45,48,47]. These works may outperform BN when the batch size is small, where inaccurate estimation may be the main issue [ 15,16], but they usually work inferior to BN under moderate batch-size training [ 22]. Another way to reduce the inconsistency is estimating corrected normalization statistics during inference only, either for domain adaptation [ 21], corruption robustness [ 35,29,2], or training [ 37,38]. We note that a recent work [ 13] investigates the estimation shift problem of BN. Unlike this work that addresses the accumulated estimation shift due to the stack of BNs for CNNs in CV tasks, our work pays more attention to how the training inference inconsistency of BN correlates with its performances for Transformers in NLP tasks. Besides, the estimation shift of BN deﬁned in [ 13], which addresses the differences between the estimated population statistics and the expected statistics, differs from our TID of BN that addresses the differences between the mini-batch statistics and populations statistics. Exploring the Failure of BN in Transformer Similar to our work, Power Normalization (PN) [ 36] also investigates the reason behind the failure of BN in Transformers. Our work signiﬁcantly differs from PN [ 36] in the following facets. PN attributes the failure of BN to the unstable training of BN incurred by ﬂuctuated forward and backward batch statistics with outlier values, while we observe that the training of BN is as good as LN and the inconsistency between training and inference of BN matters more. Based on our observation, we propose a regularization term to reduce the TID of BN. Compared with PN, which incorporates a layer-scale layer (root mean square layer normalization [ 49] without afﬁne transformation [ 43]), our method introduces no extra computation at inference. Besides, we use a more reasonable index to measure inconsistency which is invariant to the scale of data. Furthermore, we show that our RBN can improve the layer-wise training dynamics of LN, which reveals the optimization advantages of RBN. 2  BN LN BN LN BN LNFigure 1: Train loss, validation loss/BLEU of Transformer trained on IWSLT14 with BN and LN. The training of Transformer BNis better than Transformer LNwhile the validation loss/BLEU of Transformer that of Transformer LNafter 8 epoch. At the end of the training, Transformer BNfalls behind Transformer LNwith large BLEU scores. Lower loss and higher BLEU scores indicate better performance. Based on the inconsistency of training and validation perfor- mance of BN, we hypothesize that the training inference discrepancy of BN causes its performance degradation. 3 Analyses of Training Inference Inconsistency in Transformer BN 3.1 Preliminary Batch Normalization (BN) [ 15] is typically used to stabilize and accelerate DNN’s training. Let x∈Rddenote input to a neural network layer. During training, BN standardizes each neuron/channel within mmini-batch data by2 ˆxj=BNtrain(xj) =xj−µB,j√ σ2 B,j, j= 1,2,...,d, (1) whereµB,j=1 m∑m i=1x(i) jandσ2 B,j=1 m∑m i=1(x(i) j−µB,j)2are the mini-batch mean and variance for each neuron, respectively. Note that an extra small number ϵis usually added to the variance in practice to prevent numerical instability. During inference, the population mean µand varianceσ2of the layer input are required for BN to make a deterministic prediction [15] as: ˆxj=BN inf(xj) =xj−µj√ σ2 j, j= 1,2,...,d. (2) These population statistics {µ,σ2}are usually calculated as the running average of mini-batch statistics over different training iteration twith an update factor αas follows: { µ(t)= () B, (σ2)(t)= ( B)(t).(3) The discrepancy of BN for normalization during training (using Eqn. 1) and inference (using Eqn. 2) can produce stochasticity, since the population statistics of BN are estimated from the mini-batch statistics that depend on the sampled mini-batch inputs. This discrepancy is believed to beneﬁt the generalization [ 15,11] if the stochasticity is well controlled. However, this discrepancy usually harms the performance of training [ 40] since the estimation of population statistics can be inaccurate. To address this problem, a bunch of batch-free normalizations are proposed that use consistent operations during training and inference, e.g., Layer Normalization (LN) [1]. Basic Observations To analyze the failure of BN in NLP tasks, we ﬁrst plot the training loss and validation loss/BLEU [ 31] of BN and LN on IWSLT14 (De-En) dataset with the original Transformer model (see Figure 1). We observe that the training of Transformer BNis faster than Transformer LN. The training nll_loss of BN is even smaller than that of LN, especially at the beginning. However, validation loss/BLEU of BN is worse than that of LN after around the seventh epoch. This phenomenon can not be attributed to over-ﬁtting since BN introduces more stochasticity than LN in the training phase. The inconsistency between training and inference of BN may play a role. 2BN usually uses extra learnable scale and shift parameters [ 15] to recover the potentially reduced represen- tation capacity, and we omit them since they are not relevant to our discussion. 3 0 10 20 30 40 50 (%) IWSLT14 CIFAR10 0 10 20 30 40 50 epoch456789102 B2 2(%) IWSLT14 CIFAR10 2 4 6 810 12 # B2 2(%) Variance Deviation over BN layers IWSLT14 # #layer 246810122 B2 2(%) 2: Top: The average deviation of batch mean µB(left ﬁgure) and batch variance σ2 B(right ﬁgure) to population mean µand population variance σ2of all BN layers through training in ResNet18 and Transformer BN. There are 21 BN layers in ResNet18 and 12 BN layers in the encoder ofTransformer BN. At the end of training, ResNet18 has mean/variance deviation of around 4%/4% and those in Transformer BNare around 11%/13%. Large deviation of statistics hurts the performance ofTransformer BN. Bottom: Variance deviation of BN layers with different depths (left) at the end of training and variance deviation over depth and training progress (right). Since BN in ResNet18 also involves training inference inconsistency, we guess the degree of such inconsistency has a difference between ResNet18 and Transformer BN. Therefore, we plot the deviation of batch statistics to population statistics of BN in ResNet18 and Transformer BNin Figure 2 (top) to make a comparison. ResNet18 is trained on CIFAR-10 [ 17] and accuracy will drop 2 percent if we replace BN with LN. We ﬁnd that at the end of the training, Transformer BNhas a much bigger mean and variance deviation than ResNet18. Besides, the last several BN layers that are close to the output in Transformer BNhave large variance deviation (Figure 2 (bottom)), which negatively impact the model output. Furthermore, the performance degradation of Transformer BNcoincides with the increase of variance deviation by comparing Figure 1 (right) and Figure 2 (bottom right). Based on these observations, we hypothesize that the inconsistency between training and inference of BN causes BN’s performance degradation in neural machine translation . We ﬁrst mathematically deﬁne the training inference discrepancy of BN in the next subsection. 3.2 Training Inference Discrepancy By observing Eqns. 1 and 2, the normalized output during training can be calculated as: xj−µB,j σB,j=(xj−µj σj+µj−µB,j σj)σj σB,j, j= 1,2,...,d, (4) the standard deviation for the j-th dimension. We can seeµj−µB,j σj andσj σB,jcan be viewed as random variables. Their magnitude can characterize the diversity of mini-batch examples during training and indicate how hard the estimation of population statistics is. We thus deﬁne the training inference discrepancy to quantitatively measure the inconsistency as follows. 4 Deﬁnition 1 (Training Inference Discrepancy (TID)) .LetpBbe the distribution of batch data. Given a mini-batch data Xsampled from pB, we deﬁne the TID of its mean and variance (with respect to model parameter θ) as: Mean TID =EX∼pB∥µB−µ∥2 ∥σ∥2 Variance TID =EX∼pB∥σB−σ∥2 ∥σ∥2(5) In terms of computing the TID in practice, we add a small positive constant in the denominator to avoid numerical instability. We save the checkpoint at the end of each epoch and before training. We ﬁrst estimate the population statistics by running forward propagation one epoch and then compute mean and variance TID by another epoch. We omitθwhen it can be inferred from context without confusion. We compute the average mean and variance TID of all BN layers in ResNet18 trained on CIFAR10 and that of Transformer BNtrained on IWSLT14 throughout training. At the end of the training, the average mean/variance TID of BN in ResNet18 is approximately 0.8%/0.9%, while that in Transformer is around 2.8%/4.1%. TID in Transformer is much larger than that in ResNet18. The trends are the same as basic observations in Section 3.1. We will use Equation (5) to compute TID in the subsequent analysis due to its better theoretical formulation (Equation (4)). 3.3 Comprehensive Validation To further verify our hypothesis that large inconsistency between training and inference of BN causes BN’s degraded performance, we conduct experiments on Neural Machine Translation (NMT), Language Modeling (LM), Named Entity Recognition (NER), and Text Classiﬁcation (TextCls) tasks. We test both Post-Norm [39] and Pre-Norm [42] Transformers. Experimental Setup We brieﬂy illustrate the experimental settings. More detailed description can be found in supplementary materials. For neural machine translation, we use IWSLT14 German-to- English (De-En) and WMT16 (En-De) datasets, following the settings in Shen et al. [36]. Our code is based on fairseq [30]3. For language modeling, we conduct experiments on PTB [ 28] and WikiText-103 (WT103) [ 27]. We follow the experimental settings in Shen et al. [36], Ma et al. [24]. For named entity recognition, we choose CoNLL2003 (English) [ 33] and Resume (Chinese) [ 50] datasets. We mainly follow the experimental settings in Yan et al. [44]. For text classiﬁcation, we select one small scale dataset (IMDB) [ 25] and three large scale datasets (Yelp, DBPedia, Sogou News). We use the code4and follow most conﬁgurations in Bhardwaj et al. [3]. Performance Result We ﬁrst verify the inefﬁciency of BN compared to LN on four natural language tasks. Results for Post-Norm and Pre-Norm Transformers are listed in Table 1. BN performs much worse than LN on NMT, slightly worse on NER and TextCls tasks, but performs much better on LM. Although BN performs worse in most cases, it has remarkable improvement over LN on LM, raising the question: what contributes to the failure or success of BN? Analyzing the Statistics of BN We compute the TID of the last BN layer in Table 1 and leave the average TID of all BN layers in supplementary materials. The last BN layer, which is close to the output, signiﬁcantly impacts the model prediction. We observe that TID is highly correlated with the performance gap between BN and LN. When TID is large, e.g., on WMT16, BN performs much worse than LN. However, when the TID of BN is negligible, e.g., on PTB and WT103, BN performs better than LN with a large margin. We select one dataset from each task with Pre-Norm Transformer and deﬁne the total TID as the sum of mean and variance TID. At the end of the training, the total TID of the last BN layer for is around 38%/16%/9%/5%, and the performance gap is -2.1 BLEU scores/-1.1 F1 score/-0.1% accuracy/6.8 perplexity (PPL). Larger TID tends to hurt BN’s performance. To explore the quantitative relation between TID and performance gap, we substitute L= 3∼6 LN layers with BN layers from the bottom in the Post-Norm Transformer encoder on IWSLT14. As Lincreases, the variance TID of the last BN layer grows, and the BLEU scores of Transformer BN . MIT license. . Apache-2.0 license. 5 Table 1: Results for performance and TID of last BN layer with Post-Norm (top) and Pre-Norm (bottom) Transformers on four tasks containing ten datasets. We use BLEU scores (%)/perplexity/F1 score (%)/accuracy (%) to measure the model performance on neural machine modeling/named entity classiﬁcation. "+" ("-") means the bigger (smaller) the better. Post-LN means the Post-Norm Transformer with LN. Performance gap is the difference between performance of BN and LN. Positive (Negative) Performance gap indicates BN performs better (worse) than LN. Task NMT (+) LM (-) NER (+) TextCls (+) Datasets IWSLT14 WMT16 PTB WT103 Resume CoNLL IMDB Sogou DBPedia Yelp Post-LN 35.5 27.3 53.2 20.9 94.8 91.3 84.1 94.6 97.5 93.3 Post-BN 34.0 25.0 45.9 17.2 94.5 90.9 84.0 94.3 97.5 93.3 Performance Gap -1.5 -2.3 7.3 3.7 -0.3 -0.4 -0.1 -0.3 0 0 Mean TID of BN last 1.5% 4.2% 0.9% 1.8% 1.7% 4.2% 1.8% 1.8% 2.2% 3.1% Var TID of BN last 10.6% 17.9% 1.1% 2.0% 3.7% 9.5% 3.9% 4.3% 3.5% 4.0% Pre-LN 35.5 27.3 54.5 24.6 94.0 91.0 84.1 94.5 97.5 93.3 Pre-BN 34.8 25.2 45.9 17.8 93.2 89.9 84.0 94.3 97.5 93.3 Performance Gap -0.7 -2.1 8.6 6.8 -0.8 -1.1 -0.1 -0.2 0 0 Mean TID of BN last 3.4% 7.9% 1.6% 2.4% 9.6% 10.0% 2.9% 7.5% 3.9% 12.1% Var TID of BN last 4.6% 30.1% 1.7% 2.5% 6.5% 6.4% 6.2% 7.1% 3.3% 8.6% 4 5 6 7 8 9 10 1.2 1.0 0.8 0.6 0.4 ) 0 5 10 15 20 25 30 0.4 0.2 0.00.2 Figure 3: Left: Variance TID and BLEU gap between Transformer LNwhen replacing different numbers of LN layers with BN. Right: Variance TID and valid loss gap of Post-Norm Transformer through training. drops off. We plot the variance TID and BLEU gap between Transformer LNin Figure 3 (left). We can see that the two quantities are highly correlated. In Figure 3 (right), we plot the variance TID of the last BN layer and the validation loss gap between Transformer LNon IWSLT14 through training. The validation loss gap is calculated by subtracting loss of Transformer LNfrom Transformer BN. At the beginning of training, BN performs better than LN. When the TID begins to explode, BN’s performance starts to degrade. Based on the results in Table 1 and observations in Figure 3, we argue that TID serves as an indicator of BN’s performance in Transformers. Large TID hurts BN’s performance, while BN with small TID performs better than LN due to its more efﬁcient optimization (see experimental validation in Section 4.3). 4 Suppressing High TID by RBN In this section, we are devoted to reducing the TID of BN when it is large. If TID is suppressed, the performance of BN will be improved and may exceed LN due to the training efﬁciency of BN. 4.1 Regularized Batch Normalization Assume there are Hlayers of BN in a neural network. We denote the batch statistics and running statistics of each layer by µi B,σi B, andµi,σi,i= 1...,H . Assume the Cross-Entropy (CE) loss with respect to the neural network parameters θis denoted byL(θ). To avoid undesirable training 6 Table 2: Results for the performance of Post-Norm (top) and Pre-Norm (bottom) Transformers with LN/BN/RBN. RBN consistently improves BN and could match or exceed LN on 17 out of 20 settings. Task NMT (+) LM (-) NER (+) TextCls (+) Datasets IWSLT14 WMT16 PTB WT103 Resume CoNLL IMDB Sogou DBPedia Yelp Post-LN 35.5 27.3 53.2 20.9 94.8 91.3 84.1 94.6 97.5 93.3 Post-BN 34.0 25.0 45.9 17.2 94.5 90.9 84.0 94.3 97.5 93.3 Post-RBN 35.5 26.5 44.6 17.1 94.8 91.4 84.5 94.7 97.6 93.6 Pre-LN 35.5 27.3 54.5 24.6 94.0 91.0 84.1 94.5 97.5 93.3 Pre-BN 34.8 25.2 45.9 17.8 93.2 89.9 84.0 94.3 97.5 93.3 Pre-RBN 35.6 26.2 43.2 17.1 94.0 90.6 84.4 94.7 97.5 93.5 inference discrepancy, we pose the optimization as a constrained problem: min θL(θ) s.t.EpBdµ(µi B,µi)≤ϵi, i= 1,...,H EpBdσ(σi B,σi)≤ηi, i= 1,...,H(6) the inconsistency of mean and variance. This is equivalent to min θL(θ) +H∑ i=1λiEdµ(µi B,µi) +νiEdσ(σi B,σi) (7) To simplify the problem, we set λi=λ,νi=ν, fori= 1,...,H . When handling batch data, we apply gradient-based optimization to the following loss ( LB(θ)is the batch CE loss): LB(θ) +H∑ i=1λdµ(µi B,µi) +νdσ(σi B,σi) In particular, we choose dµ(µB,µ) =∥µB−µ∥2 2anddσ(σB,σ) =∥σB−σ∥2 2. The sensitivity analysis of hyperparameter is given in Section 4.3. Since back propagating through the running trace back to the ﬁrst batch of data which is impractical, we simply stop the gradient ofµandσin back propagation. 4.2 Experimental Result for RBN We chooseλ,νboth validation loss. Results are shown in Table 2. The optimal are listed in supplementary materials. Neural Machine Translation On IWSLT14 datasets, we see that RBN signiﬁcantly improves BN and can exceed LN with 0.1 BLEU scores with Pre-Norm Transformer and match LN with Post-Norm Transformer. On WMT16 dataset, although RBN still falls behind LN, it could improve 1.5/1.0 BLEU scores over BN in setting. The reason is that even though RBN can suppress a large amount of TID, the remaining is still large since the original TID is huge. We speculate that the high data diversity in WMT16 contributes to the explosive TID of BN, which is hard to remove. We leave the veriﬁcation as future work. Language Modeling On Post-Norm Transformer, BN could boost the testing PPL of LN from 53.2 to 45.9 on PTB and from 20.9 to 17.2 on WikiText-103. Furthermore, substituting RBN for BN improves the testing PPL to 44.6 on PTB and 17.1 on WikiText-103. On Pre-Norm Transformer, BN elevates the testing PPL of LN from 54.5 to 45.9 on PTB and from 24.6 to 17.8 on WikiText-103. Moreover, replacing BN with RBN improves the testing PPL to 43.2 on PTB and 17.1 on WikiText- 103. Overall, RBN exceeds LN with 8.6/3.8 testing PPL with Post-Norm Transformer and 11.3/7.5 testing PPL with Pre-Norm Transformer on . Named Entity Recognition BN performs worse than LN on both Resume and CoNLL2003 datasets, especially for Pre-Norm Transformer. RBN improves BN in all settings, matches or exceeds LN in three out of four settings. By taking the better performance of Post-Norm and Pre-Norm, RBN matches the performance of LN on Resume and exceeds LN on CoNLL2003. 7 Table 3: Results for the performance of Post-Norm (top) and Pre-Norm (bottom) Transformers with . Task NMT (+) LM (-) NER (+) TextCls (+) Datasets IWSLT14 WMT16 PTB WT103 Resume CoNLL IMDB Sogou DBPedia Yelp Post-PN-only 0 0 254.6 inf 94.4 67.1 84.2 90.6 97.1 89.6 Post-PN+LS 35.6 0 49.8 21.0 94.3 90.9 84.0 94.6 97.4 93.2 Post-BRN 35.3 25.8 45.1 17.3 93.6 89.9 83.6 94.5 97.5 93.3 Post-MABN 0 0 47.4 33.6 94.4 90.8 84.1 94.5 97.5 93.5 Post-RBN 35.5 26.5 44.6 17.1 94.8 91.4 84.5 94.7 97.6 93.6 Pre-PN-only 34.5 26.0 48.6 inf 5.0 11.1 84.2 94.4 97.4 93.3 Pre-PN+LS 35.6 27.2 59.8 20.9 93.3 54.1 83.3 94.4 97.3 93.4 Pre-BRN 35.2 25.3 45.7 17.5 94.1 91.1 84.3 94.5 97.4 93.4 Pre-MABN 35.0 26.8 48.7 inf 94.8 90.9 84.4 94.6 97.5 93.3 Pre-RBN 35.6 26.2 43.2 17.1 94.0 90.6 84.4 94.7 97.5 93.5 0.0 0.2 0.4 0.6 0.8 1.0 ) BN RBN 0.0 0.2 0.4 0.6 0.8 1.0 ) BN RBN 0.0 0.2 0.4 0.6 0.8 1.0 ) BN RBN 0.0 0.2 0.4 0.6 0.8 1.0 ) BN RBN 0.0 0.2 0.4 0.6 0.8 1.0 ) BN RBN 0.0 0.2 0.4 0.6 0.8 1.0 ) BN RBN 0.0 0.2 0.4 0.6 0.8 1.0 ) BN RBN 0.0 0.2 0.4 0.6 0.8 1.0 ) BN RBN Figure 4: Average Mean and Variance TID on for Pre-Norm Trans- former with BN and RBN. RBN reduces the Mean and Variance TID of BN at the end of the training and leads to better performance. Text Classiﬁcation We ﬁnd that BN performs similar to/worse than LN on 4/4 settings. RBN improves the performance of BN consistently and can match/exceed LN on 1/7 settings. RBN improves BN with 0.3% accuracy on average, which shows the beneﬁt of our regularization. We do not intend to achieve the performance but to verify the efﬁcacy of RBN. Comparison to BN’s Variants We compare our RBN with Power Normalization (PN) [ 36], Batch (BRN) [ 14], and Moving Averaing Batch Normaliazation (MABN) [ 45] in Table 3. These methods incorporate population statistics of BN in training, which is beneﬁcial for alleviating training inference inconsistency of BN. PN and MABN are implemented by their ofﬁcial codes5. BRN is implemented according to their paper [ 14]. The conﬁgurations of PN, BRN, and MABN are given in supplementary materials. We highlight that PN incorporates layer scaling (LS) [49], which is important for stabilizing training, as shown in the supplementary materials and ofﬁcial code of PN. We thus report the results for PN only and PN with layer scaling (PN+LS). We can see that RBN performs the best in most settings. PN and MABN is not stable without layer scaling, especially for Post-Norm Transformers. 4.3 Analysis Training Inference Inconsistency We compute the TID of the last BN layer ( BNlast) in Ta- ble 4 and plot the average TID of BN and RBN on WMT16, WT103, CoNLL2003, and IMDB datasets for Pre-Norm Transformers through training in Figure 4. Figures of TID for other datasets and Post-Norm Transformer can be found in supplementary materials. We can see that RBN re- duces BN’s mean and variance TID at the end of training. On neural machine translation and named entity recognition tasks, the original TID is large. RBN signiﬁcantly decreases the TID of . GPL-3.0 license. . MIT license. 8 Table 4: TID of the last BN/RBN layer in Post-Norm and Pre-Norm Transformers on various NLP tasks. RBN reduces the TID of BN effectively. Task NMT LM NER TextCls Datasets IWSLT14 WMT16 PTB WT103 Resume CoNLL IMDB Sogou DBPedia Yelp Post-Norm Transformer Mean TID of BN last 1.5% 4.2% 0.9% 1.8% 1.7% 4.2% 1.8% 1.8% 2.2% 3.1% Mean TID of RBN last 0.8% 2.3% 0.9% 1.8% 1.4% 1.9% 0.2% 0.2% 0.3% 0.2% Var TID of BN last 10.6% 17.9% 1.1% 2.0% 3.7% 9.5% 3.9% 4.3% 3.5% 4.0% Var TID of RBN last 6.7% 7.7% 1.1% 1.7% 3.0% 5.0% 1.2% 0.2% 0.3% 0.1% Pre-Norm Transformer Mean TID of BN last 3.4% 7.9% 1.6% 2.4% 9.6% 10.0% 2.9% 7.5% 3.9% 12.1% Mean TID of RBN last 3.2% 1.3% 1.6% 2.4% 4.5% 4.0% 0.7% 1.0% 1.1% 1.0% Var TID of BN last 4.6% 30.1% 1.7% 2.5% 6.5% 6.4% 6.2% 7.1% 3.3% 8.6% Var TID of RBN last 1.5% 12.1% 1.7% 2.4% 6.3% 5.6% 4.7% 0.4% 0.5% 0.5% 0 20 40 60 epoch101102C50% layer 2 LN RBN 0 20 40 60 epoch102103C50% layer 4 LN RBN 0 20 40 60 % layer 6 LN RBN 0 20 40 60 epoch102103C80% layer 2 LN RBN 0 20 40 60 epoch103104C80% layer 4 LN RBN 0 20 40 60 epoch103104C80% layer 6 LN RBN Figure 6:C50%(top), andC80%(bottom) of input features of Transformer encoder layer 2/4/6. RBN improves the C50%andC80%of LN, especially for deep layers (2 orders of magnitude at layer 6). BN and improves BN’s performance by a clear margin. For language modeling and text clas- siﬁcation tasks, RBN also reduces the moderate TID of BN and gets better PPL or accuracy. 0 0.01 0.1 1 0 0.01 0.1 1 34.8 35.3 35.5 35.5 35.3 35.6 35.6 35.6 35.3 35.2 35.4 35.5 34.5 34.7 34.8 35.0BLEU scores Figure 5: The BLEU scores on IWSLT14 with different mean ( λ) and variance ( ν) discrepancy penalty of to We test different penalty coefﬁcients for RBN on neural machine transla- tion with Pre-Norm Transformer. The results are shown in Figure 5. Penalizing the mean and variance discrep- ancy can both improve the performance of BN. Combin- ing them with moderate coefﬁcients achieves the best performance. Training Dynamics To show the optimization advan- tages of RBN over LN, we explore the layer-wise train- ing dynamics of LN and RBN in Pre-Norm Transformer on IWSLT14. We refer the reader to Huang et al. [12] for detailed analysis about the correlation between optimiza- tion of neural network and layer-wise training dynamics. We empirically observe that replacing LN with RBN signiﬁcantly improves the layer-wise conditioning [ 12] of Transformer. We denote the intermediate embedding in Transformer by ˜X∈RB×T×d, each ˜Xi,j,:∈Rdis a word vector. We reshape ˜Xto a sequence of word vectors to X= [x1,x2,..., xBT]∈RBT×d. We assumeBT >d which is satisﬁed in our experiments. We deﬁne the general condition number with 9 0 20 40 60 layer 2 LN RBN 0 20 40 60 layer 4 LN RBN 0 20 40 60 layer 6 LN RBNFigure 7: C max of input features of Transformer encoder layer 2/4/6 through training. respect to the percentage as Cp(X) =σ1 σ⌈pd⌉,0< p≤1.⌈a⌉is the smallest integer that is larger than or equal to a. LowerCp(X)is usually associated with faster convergence of training. We plot C50%, andC80%of input features of transformer encoder layer 2/4/6 in Figure 6. We can see that RBN signiﬁcantly reduces the C50%andC80%of LN, usually with orders of magnitude. We also plot the layer-wise Cmax(X) =λmax((XTX)1 2)in Figure 7. Smaller Cmax usually permits higher learning rates which leads to faster training and better generalization [ 10]. RBN has much smaller Cmax than LN. 5 Conclusion and Limitation In this paper, we deﬁned Training Inference Discrepancy (TID) and showed that TID is a good indicator of BN’s performance for Transformers, supported by comprehensive experiments. We observed BN performs much better than LN when TID is negligible and proposed Regularized BN (RBN) to alleviate TID when TID is large. Our RBN has theoretical advantages in optimization and works empirically better by controlling the TID of BN when compared with LN. We hope our work will facilitate a better understanding and application of BN in NLP. Limitation. Our analyses on TID are almost empirical studies without theoretical guarantee. It is better to further model the geometric distribution of word embedding, evolving along with the training dynamics and information propagation, with theoretical derivation under mild assumptions. Besides, our proposed RBN cannot entirely suppress huge TID in training large-scale datasets with high diversity, leading to degraded performance. One possible direction is to combine RBN and LN for both better optimization properties and small TID, as explored in [13, 46] for CV tasks. Ji Wu was sponsored by National Key Research & Development Program of China (2021ZD0113402), Tsinghua University Spring Breeze Fund (2021Z99CFZ010), Na- tional Key Research & Development Program of China (Grant Number: 2021YFC2500803), and Joint Research Institute Program. Lei Huang was supported by National Natural Science Foundation of China (Grant No. 62106012) and the Fundamental Research Funds for the Central Universities.  
cross episodic curriculum for transformer agents 	Cross-Episodic Curriculum for Transformer Agents Lucy Xiaoyang Shi1∗, Yunfan Jiang1∗, Jake Grigsby2, Linxi “Jim” Fan3†, Yuke Zhu2 3† 1Stanford University2The University of Texas at Austin3NVIDIA Research ∗Equal advising Abstract We present a new algorithm, Cross-Episodic Curriculum ( CEC ), to boost the learn- ing efficiency and generalization of Transformer agents. Central to CEC is the placement of cross-episodic experiences into a Transformer’s context, which forms the basis of a curriculum. By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes. Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism. The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task rein- forcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings, and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators’ expertise. In all instances, policies resulting from CEC exhibit superior performance and strong generalization. Code is open- sourced on the project website to facilitate research on Transformer agent learning. 1 Introduction The paradigm shift driven by foundation models [ 8] is the communities who study sequential problems [ 80], with innovations focusing on control [ 2,45,38,9], planning [ 76,32,33,78,17], pre-trained visual representation [ 57,50,67,51,82], among others. Despite the progress, the data-hungry nature makes the application of Transformer [ 75] agents extremely challenging in data-scarce domains like robotics [ 52,53,19,38,9]. This leads us to the question: Can we maximize the utilization of limited data, regardless of their optimality and construction, to foster more efficient learning? To this end, this paper introduces a novel algorithm named Cross-Episodic Curriculum (CEC ), a method that explicitly harnesses the shifting distributions of multiple experiences when organized into a curriculum. The key insight is that sequential cross-episodic data manifest useful learning signals that do not easily appear in any separated training episodes.1As illustrated in Figure 1, CEC realizes this through two stages: 1) formulating curricular sequences to capture (a) the policy improvement on single environments, (b) the learning progress on a series of progressively harder environments, or (c) the increase of demonstrators’ proficiency; and 2) causally distilling policy improvements into the model weights of Transformer agents through cross-episodic attention . When a policy is trained to predict actions at current time steps, it can trace back beyond ongoing trials and internalize improved behaviors encoded in curricular data, thereby achieving efficient learning 1Following the canonical definition in Sutton and Barto [73], we refer to the sequences of interaction with clearly identified initial and terminal states as “episodes”. We use “episode”, “trial”, and “trajectory” in this work. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). C u r r i c u l a r D a t aP o l i c y i m p r o v e m e n t o n s i n g l e e n v i r o n m e n t s :L e a r n i n g p r o g r e s s o n a s e r i e s o f p r o g r e s s i v e l y h a r d e r e n v i r o n m e n t s :I n c r e a s e o f d e m o n s t r a t o r ’ s p r o f i c i e n c y : ττM o d e l T r a i n i n g w i th C r o s s - E p i s o d i c A t t e n t i o nC o n t e x t W i n d o wO n g o i n g E p i s o d eFigure 1: Cross-episodic curriculum for Transformer agents. CEC involves two major steps: 1) Preparation of curricular data. We order multiple experiences such that they explicitly capture curricular patterns. For instance, they can be policy improvement in single environments, learning progress in a series of progressively harder environments, or the increase of the demonstrator’s expertise. 2) Model training with cross-episodic attention. When training the model to predict actions, it can trace back beyond the current episode and internalize the policy refinement for more efficient learning. Here each τrepresents an episode (trajectory). ˆarefers to actions predicted by the model. Colored triangles denote causal Transformer models. and robust deployment when probed with visual or dynamics perturbations. Contrary to prior works like Algorithm Distillation (AD, Laskin et al. [42]) which, at test time, samples and retains a single task configuration across episodes for in-context refinement, our method, CEC , prioritizes zero-shot generalization across a distribution of test configurations. With CEC , agents are evaluated on a new task configuration in each episode, emphasizing adaptability to diverse tasks. We investigate the effectiveness of CEC in enhancing sample efficiency and generalization with two representative case studies. They are: 1) Reinforcement Learning (RL) on DeepMind Lab (DM- Lab) [ 5], a 3D simulation encompassing visually diverse worlds, complicated environment dynamics, ego-centric pixel inputs, and joystick control; and 2) Imitation Learning (IL) from mixed-quality human demonstrations on RoboMimic [ 53], a framework designed to study robotic manipulation with proprioceptive and external camera observations and continuous control. Despite RL episodes being characterized by tuples and IL trajectories by state-action pairs, our method exclusively employs state-action pairs in its approach. In challenging embodied navigation tasks, despite significant generalization gaps (Table 1), our method surpasses concurrent and competitive method Agentic Transformer (AT, Liu and Abbeel [47]). It also significantly outperforms popular offline RL methods such as Decision Transformer (DT, Chen et al. [13]) and baselines trained on expert data, with the same amount of parameters, architecture, and data size. It even exceeds RL oracles directly trained on test task distributions by50% in a zero-shot manner. CEC also yields robust embodied policies that are up to 1.6× better than RL oracles when zero-shot probed with unseen environment dynamics. When learning continuous robotic control, CEC successfully solves two simulated manipulation tasks, matching and outperforming previous baselines [ 53,25,41]. Further ablation reveals that CEC with cross-episodic attention is a generally effective recipe for learning Transformer agents, especially in applications where sequential data exhibit moderate and smooth progression. 2 2 Cross-Episodic Curriculum: Formalism and In this section, we establish the foundation for our cross-episodic curriculum method by first reviewing the preliminaries underlying our case studies, which encompass two representative scenarios in sequential . Subsequently, we formally introduce the assembly of curricular data and the specifics of model optimization utilizing cross-episodic attention. Lastly, we delve into the practical implementation of CEC in the context of these two scenarios. 2.1 Preliminaries Reinforcement learning. We consider the setting where source agents learn through trial and error in partially observable environments. Denoting states s∈ S and actions a∈ A , an agent interacts in a Partially Observable Markov Decision Process (POMDP) with the transition function p(st+1|st, at) :S × A → S . It observes o∈ O emitted from observation function Ω(ot|st, at−1) :S × A → O and receives scalar reward rfrom R(s, a) :S × A → R. Under the episodic task setting, RL seeks to learn a parameterized policy πθ(·|s)that maximizes the return over a fixed length Tof interaction steps: πθ= arg max θ∈ΘPT−1 t=0γtrt, where γ∈[0,1)is a discount factor. Here we follow the canonical definition of an episode τas a series of interactions with length T,τ:= (s0, a0, r0, . . . , s T−1, aT−1, rT−1, sT), where initial states s0are sampled from initial state distribution s0∼ρ0(s)and terminal states sTare reached once the elapsed timestep exceeds T. Additionally, we view all RL tasks considered in this work as goal-reaching prob- lems [ 39,26] and constrain all episodes to terminate upon task completion. It is worth noting that sim- ilar to previous work [ 42], training data are collected by source RL agents during their online learning. Nevertheless, once the dataset is obtained, our method is trained offline in a purely supervised manner. Imitation learning. We consider IL settings with existing trajectories composed only of state-action pairs. Furthermore, we relax the assumption on demonstration optimality and allow them to be crowdsourced [ 10,12,11]. Data collected by operators with varying expertise are therefore unavoidable. Formally, we assume the access to a dataset DN:={τ1, . . . , τ N}consisting of , with each demonstrated trajectory τi:= (s0, a0, . . . , s T−1, aT−1)naturally identified as an episode. The goal of IL, specifically of behavior cloning (BC), is to learn a policy πθthat accurately models the distribution of behaviors. When viewed as goal-reaching problems, BC policies can be evaluated by measuring the success ratio in completing tasks [26]. 2.2 Curricular Data Assembly and Model Optimization Meaningful learning signals emerge when multiple trajectories are organized and examined cross- episodically along a curriculum axis. This valuable information, which is not easily discernible in individual training episodes, may encompass aspects such as the improvement of an RL agent’s navigation policy or the generally effective manipulation skills exhibited by operators with diverse proficiency levels. With a powerful model architecture such as Transformer [ 75,16], such emergent and valuable learning signals can be baked into policy weights, thereby boosting performance in embodied tasks. For a given embodied task M, we define its curriculum CMas a collection of trajectories τconsisting of state-action pairs. A series of ordered levels [L1, . . . ,LL]partitions this collection such thatS ∀}=∅. More importantly, these ordered levels characterize a curriculum by encoding, for example, learning progress in single environments, learning progress in a series of progressively harder environments, or the increase of the demonstrator’s expertise. With a curriculum CM:={τi}N i=1and its [L1, . . . ,LL], we construct a curricular sequence Tthat spans multiple episodes and captures the essence of gradual improvement in the following way: T:=M l∈{1,...,L}h τ(1), . . . , τ(C)i ,where C∼ U(J|Ll|K)and τ(c)∼ Ll. (1) The symbol ⊕denotes the concatenation operation. U(JKK)denotes a uniform distribution over the discrete set {k∈N, k≤K}. In practice, we use values smaller than |Ll|considering the memory consumption. 3 (a) Goal Maze (b) Watermaze (c) Irreversible Path (d) Lift (e) Can Figure 2: We evaluate our method on five tasks that cover challenges such as exploration and planning over long horizons in RL settings, as well as object manipulation and continuous control in IL settings. Figures are from Beattie et al. [5] and Mandlekar et al. [53]. We subsequently learn a causal policy that only depends on cross-episodic historical observations πθ(·|o(≤n) ≤t). Note that this modeling strategy differs from previous work that views sequential as a big problem [ 13,37,42,38]. It instead resembles the causal policy in Baker et al. [4]. Nevertheless, we still follow the best practice [ 36,60,22] to provide previous action as an extra modality of observations in POMDP RL tasks. We leverage the powerful attention mechanism of Transformer [ 75] to enable cross-episodic atten- tion. Given observation series O(n) t:={o(1) 0, . . . , o(≤n) ≤t}(shorthanded as Ohereafter for brevity), Transformer projects it into query Q=fQ(O), key K=fK(O), and value , with each row being a D-dim vector. Attention operation is performed to aggregate information: Attention (Q, K, V ) =softmax (QK⊺ √ D)V. (2) Depending on whether the input arguments for fQandf{K,V}are the same, attention operation can be further divided into self-attention and . Since tasks considered in this work do not require additional conditioning for task specification, we follow previous work [ 4,83] to utilize self-attention to process observation series. Nevertheless, ours can be naturally extended to handle, for example, natural language or multi-modal task prompts, following the introduced in Jiang et al. [38]. Finally, this Transformer policy is trained by simply minimizing the negative log-likelihood objective JNLLof labeled actions, conditioned on cross-episodic context: ) =1 |T | × T|T |X n=1TX t=1−logπθ a(n) t|o(≤n) ≤t . (3) Regarding the specific memory architecture, we follow Baker et al. [4], Adaptive Agent Team et al. [1]to use Transformer-XL [ 16] as our model backbone. Thus, during deployment, we keep its hidden states propagating across test episodes to mimic the training settings. 2.3 Practical We now discuss concrete instantiations of CEC for 1) RL with DMLab and 2) IL with RoboMimic. Detailed introductions to the benchmark and task selection are deferred to Sec. 3. We investigate the following three curricula, where the initial two pertain to RL, while the final one applies to IL: curriculum. In the first instantiation, inspired by the literature on learning progress [ 54,27,65,40], we view the progression of learning agents as a curriculum. Concretely, we train multi-task PPO agents [ 70,63] on tasks drawn from test distributions. We record their online interactions during training, which faithfully reflect the learning progress. Finally, we form the curriculum by sequentially concatenating episodes collected at different learning stages. Note that this procedure is different from Laskin et al. [42], where for each environment, the learning dynamics of multiple single-task RL agents has to be logged. In contrast, we only track a single multi-task agent per environment. curriculum. In the second instantiation, instead of taking snapshots of RL agents directly trained on test configurations, we collect learning progress on a series of easier 4 Table 1: Generalization gaps between training and testing for DMLab levels. Note that agents resulting from curricula are not trained on test configurations. Therefore, their performance should be considered as zero-shot . Level NameDifficulty ParameterTest Difficulty Ours (Learning Progress)Ours (Task Difficulty)BC w/ Expert DataRL ( RL (Oracle) Goal Maze Room Numbers 20 20 5 →10→15 20 20 5 →10→15→20 Watermaze Spawn Radius 580 580 150 →300→450 580 580 150 →300→450→580 Irreversible Path Built-In Difficulty .9 .9 .1 →.3→.5→.7 .9 .9 .1 →.3→.5→.7→.9 but progressively harder tasks. For instance, in an embodied navigation task, the test configuration includes 20 rooms. Rather than logging source agents’ learning progression in the 20-room maze, we record in a series of mazes with 5, 10, and 15 rooms. We then structure stored episodes first following learning progress and then the increase of layout complexity. This practice naturally creates a curriculum , which resembles curriculum RL that is based on task difficulty [ 54,58]. We find it especially helpful for problems where the source RL agent does not make meaningful progress. curriculum. For the setting of IL from mixed-quality demonstrations, we instantiate a curriculum based on demonstrators’ expertise. This design choice is motivated by literature on learning from heterogeneous demonstrators [ 6,81], with the intuition that there is little to learn from novices but a lot from experts. To realize this idea, we leverage the Multi-Human dataset from RoboMimic [ 53]. Since it contains demonstrations collected by human demonstrators with varying proficiency, we organize offline demonstration trajectories following the increase of expertise to construct the curriculum . 3 Experimental Setup In this section, we elaborate on the experimental setup of our case studies. Our investigation spans two representative and distinct settings: 1) online reinforcement learning with 3D maze environments of DMLab [ 5], and 2) imitation learning from mixed-quality human demonstrations of RoboMimic [ 53]. For each of them, we discuss task selection, baselines, and training and evaluation protocols. Teasers of these tasks are shown in Figure 2. 3.1 Task Settings and Environments DeepMind Lab [5] is a 3D learning environment with diverse tasks. Agents spawn in visually complex worlds, receive ego-centric (thus partially observable) RGB pixel inputs, and execute joystick actions. We consider three levels from this benchmark: Goal Maze, Watermaze [ 56], and Sky Maze with Irreversible Path. They challenge agents to explore, memorize, and plan over a long horizon. Their goals are similar — to navigate in complicated mazes and find a randomly spawned goal, upon which sparse rewards will be released. Episodes start with randomly spawned agents and goals and terminate once goals are reached or elapsed steps have exceeded pre-defined horizons. RoboMimic [53] is a framework designed for studying robot manipulation and learning from demonstrations. Agents control robot arms with fixed bases, receive proprioceptive measurements and image observations from mounted cameras, and operate with continuous control. We evaluate two simulated tasks: “Lift” and “Can”. In the “Lift” task, robots are tasked with picking up a small cube. In the “Can” task, robots are required to pick up a soda can from a large bin and place it into a smaller target bin. Episodes start with randomly initialized object configuration and terminate upon successfully completing the task or exceeding pre-defined horizons. 3.2 Baselines The primary goal of these case studies is to assess the effectiveness of our proposed cross-episodic curriculum in increasing the sample efficiency and boosting the generalization capability of Trans- former agents. Therefore, in online RL settings, we compare against source RL agents which generate training data for our method and refer to them as oracles . These include a) PPO agents directly 5 Ours (T ask Difficulty), Aut oOurs (T ask Difficulty), Fix edOurs (Learning Pr ogr ess)BC w/ Exper t DataD T (Mix ed Difficulty)D T (Single Difficulty)A T (Mix ed Difficulty)A T (Single Difficulty)RL ( RL (Oracle) Figure 3: Evaluation results on DMLab. OurCEC agents perform comparable to RL oracles and on average outperform other baseline methods. On the hardest task Irreversible Path where the RL oracle and BC baseline completely fail, our agents outperform the curriculum RL oracle by 50% even in azero-shot manner. For our methods, DT, AT, and the BC w/ expert data baselines, we conduct 20 independent evaluation runs, each consisting of 100 episodes for Goal Maze and Watermaze and 50 episodes for Irreversible Path due to longer episode length. We test RL oracles for 100 episodes. The error bars represent the standard deviations over 20 runs. trained on test task distributions, denoted as “ RL (Oracle) ” hereafter, and b) curriculum PPO agents that are gradually adapted from easier tasks to the test difficulty, which is referred to as “ Curriculum RL (Oracle) ”. Furthermore, we compare against one concurrent and competitive method Agen- tic Transformer [ 47], denoted as “ AT”. It is closely related to our method, training Transformers on sequences of trajectory ascending sorted according to their rewards. We also compare against popular offline RL method Decision Transformer [ 13], denoted as “ DT”. Additionally, we include another behavior cloning agent that has the same model architecture as ours but is trained on opti- mal data without cross-episodic attention. This baseline is denoted as “ BC w/ Expert Data ”. For the case study on IL from mixed-quality demonstrations, we adopt the most competing approach, BC-RNN , from Mandlekar et al. [53] as the main baseline. We also include comparisons against other offline RL methods [ 44] such as Q-learning ( BCQ ) [25] and Conservative Q-Learning ( CQL ) [41]. 3.3 Training and Evaluation We follow the best practice to train Transformer agents, including adopting AdamW optimizer [49], learning rate warm-up and cosine annealing [ 48], etc. Training is performed on NVIDIA V100 GPUs. During evaluation, for agents resulting from our method, each run involves several test rollouts to fill the context. We keep hidden states of Transformer-XL [ 16] propagating across episodes. We run other baselines and oracles for 100 episodes to estimate their performances. For our methods on RL settings, we compute the maximum success rate averaged across a sliding window over all test episodes to account for in-context improvement. The size of the sliding window equals one-quarter of the total test episodes. These values are averaged over 20 runs to constitute the final reporting metric. For our methods on the IL setting, since all training data are successful trajectories, we follow Mandlekar et al. [53] to report the maximum success rate achieved over the course of training, directly averaged over test episodes. 4 Experiments We aim to answer the following four research questions through comprehensive experiments. 1.To what extent can our cross-episodic curriculum increase the sample efficiency of Trans- former agents and boost their generalization capability? 2. Is CEC consistently effective and generally applicable across distinct learning settings? 3. What are the major components that contribute to the effectiveness of our method? 6 Ours (T ask (Learning Pr ogr ess)BC w/ Exper t DataD T (Mix ed Difficulty)D T (Single Difficulty)A T (Mix ed Difficulty)A T (Single Difficulty)RL ( RL (Oracle) Figure 4: Generalization results on DMLab. Top row : Evaluation results on Goal Maze with unseen maze mechanism and Irreversible Path with difficulty levels. Bottom row : Evaluation results on three levels with environment dynamics differing from training ones. CEC agents display robustness and generalization across various dimensions, outperforming curriculum RL oracles by up to 1.6×. We follow the same evaluation protocol as in Figure 3. The error bars represent the standard deviations over 20 runs. 4.1 Main Evaluations We answer the first two questions above by comparing learned agents from our method against 1) Reinforcement Learning (RL) oracles in online RL settings and 2) baselines on learning from mixed-quality demonstrations in the Imitation Learning (IL) setting. We first examine agents learned from and curricula in challenging 3D maze environments. The first type of agent is denoted as “ Ours (Learning Progress) ”. For the second type, to ensure that the evaluation also contains a series of tasks with increasing difficulty, we adopt two mechanisms that control the task sequencing [ 58]: 1) fixed sequencing where agents try each level of difficulty for a fixed amount of times regardless of their performance and 2) dynamic sequencing where agents are automatically promoted to the next difficulty level if they consecutively succeed in the previous level for three times. We denote these two variants as “ Ours (Task Difficulty), Fixed ” and “ Ours (Task Difficulty), Auto ”, respectively. Note that because the curriculum does not contain any training data on test configurations, these two settings are zero-shot evaluated on test task distributions. We summarize these differences in Table 1. We denote AT and DT trained on data consisting of a mixture of task difficulties as “ AT (Mixed Difficulty) ” and “ DT (Mixed Difficulty) ”. Note that these data are the same used to train “Ours (Task Difficulty)”. Similarly, we denote AT and DT directly trained on test difficulty as “AT (Single Difficulty)” and “DT (Single Difficulty)”. These data are the same used to train “Ours (Learning Progress)”. Cross-episodic curriculum results in agents. As shown in Figure 3, on two out of three examined DMLab levels, CEC agents perform comparable to RL oracles and outperform the BC baselines trained on expert data by at most 2.8×. On the hardest level Irreversible Path where agents have to plan the route ahead and cannot backtrack, both the BC baseline and RL oracle fail. However, our agents succeed in proposing correct paths that lead to goals and significantly outperform the curriculum RL oracle by 50% even in a zero-shot manner. Because CEC only requires environment interactions generated during the course of training of online source agents (the curriculum even contains fewer samples compared to the curriculum RL, as illustrated in Table 1), the comparable and even better performance demonstrates that our method yields highly embodied policies. On average, our method with curriculum performs the best during evaluation (Table A.5), confirming the benefit over the 7 Table 2: Evaluation results on RoboMimic. Visuomotor policies trained with our curriculum outperform the most competing behavior cloning baseline, as well as other offline RL algorithms. For our method on the Lift task, we conduct 5 independent runs each with 10 rollout episodes. On the Can task, we conduct 10 independent runs each with 5 rollout episodes due to the longer horizon required to complete the task. Standard deviations are included. Task Ours BC-RNN [53] BCQ [25] CQL [41] Lift 100.0±0.0 100 .0±0.093.3±0.9 11 .3±9.3 Can 100.0±0.0 96.0±1.6 77 .3±6.8 0 .0±0.0 Table 3: Ablation on the importance of cross-episodic attention. Transformer agents trained with the same curricular data but without cross-episodic attention degrade significantly during evaluation, suggesting its indispensable role in learning highly performant policies. DMLab RoboMimic Goal Maze Watermaze Irreversible Path Lift Can Ours 65.2±6.7 50 .9±6.6 38 .2±7.0 100.0±0.0 100 .0±0.0 Ours w/o Cross-Episodic Attention 35.0±7.1 20 .0±2.5 3 .8±4.9 75.9±12.3 99.3±0.9 concurrent AT approach that leverages experiences. When compared to DT, it outperforms by a significant margin, which suggests that our cross-episodic curriculum helps to squeeze learning signals that are useful for downstream . Cross-episodic curriculum boosts the generalization capability. To further investigate whether CEC can improve generalization at test time, we construct settings with unseen maze mechanisms (randomly open/closed doors), difficulty, and different environment dynamics. See the Appendix, Sec. C.2 for the exact setups. As demonstrated in Figure 4, CEC generally improves Transformer agents in learning robust policies that can generalize to perturbations across various axes. On three settings where the BC w/ Expert Data baseline still manages to make progress, CEC agents are up to 2×better. Compared to oracle curriculum RL agents, our policies significantly outperform them under three out of five examined scenarios. It is notable that on Irreversible Path with difficulty, CEC agent is 1.6×better than the curriculum RL oracle trained on the same data. These results highlight the benefit of learning with curricular contexts. On average, our method surpasses the concurrent AT baseline and achieves significantly better performance than other baselines (Table A.6). This empirically suggests that CEC helps to learn policies that are robust to environmental perturbations and can quickly generalize to new changes. Cross-episodic curriculum is effective across a wide variety of learning scenarios. We now move beyond RL settings and study the effectiveness of the curriculum in the IL setting with mixed-quality demonstrations. This is a common scenario, especially in robotics, where demonstrations are collected by human operators with varying proficiency [ 52]. As presented in Table 2, visuomotor policies trained with the curriculum are able to match and outperform the baseline [ 53] on two simulated robotic manipulation tasks and achieve significantly better performance than agents learned from prevalent offline RL algorithms [ 25,41]. These results suggest that our cross-episodic curriculum is effective and broadly applicable across various problem settings. More importantly, it provides a promising approach to utilizing limited but sub-optimal data in data-scarce regimes such as robot learning. 4.2 Ablation Studies In this section, we seek to answer the third research question to identify the components critical to the effectiveness of our approach. We focus on three parts: the importance of cross-episodic attention, the influence of curriculum granularity, and the effect of varying context length. Finally, we delve into the fourth question, identifying scenarios where CEC is expected to be helpful. Importance of cross-episodic attention. The underlying hypothesis behind our method is that cross-episodic attention enables Transformer agents to distill policy improvement when trajectories are viewed collectively. To test this, on DMLab levels and RoboMimic tasks, we train the same Transformer agents with the same curricular data and training epochs but 8 without cross-episodic attention. We denote such agents as “ Ours w/o Cross-Episodic Attention ” in Table 3. Results demonstrate that the ablated variants experience dramatic performance degradation on four out of five examined tasks, which suggests that naively behaviorally cloning sub-optimal data can be problematic and detrimental. Cross-episodic attention views curricular data collectively, facilitating the extraction of knowledge and patterns crucial for refining , thereby optimizing the use of sub-optimal data. Fine Medium Coarse Curriculum Performance (%) Goal Maze Watermaze Irreversible Path Figure 5: We compare the performance relative to agents trained with the fine- grained curricula. Performance mono- tonically degrades as - based curricula become granularity. We perform this ablation with the curriculum on DMLab levels, due to the ease of adjusting granularity. We treat the curricula listed in the column “Ours (Task Difficulty)” in Table 1 as “Fine”, and gradually make them coarser to study the impact. Note that we ensure the same amount of training data. See the Appendix, Sec. C.4 for how we define granularity levels “Medium” and “Coarse”. We visualize the performance relative to the most fine-grained in Figure 5. The monotonic degradation of policy perfor- mance with respect to curriculum coarseness suggests that fine-grained curricula are critical for Transformer agents to mostly benefit from cross-episodic training. Varying context length. Lastly, we study the effect of varying context length on DMLab and visualize it in Figure 6. We normalize all performance values relative to those of “Ours (Task Difficulty), Auto” reported in Figure 3. It turns out that both too short and unnecessarily long context windows are harmful. On two out of three levels, using a shorter context decreases the performance even more. This finding coincides with Laskin et al. [42] that a sufficiently long Transformer context is necessary to retain cross-episodic information. Furthermore, we also discover that an unnecessarily long context is also harmful. We hypothesize that this is due to the consequent training and optimization instability. Goal Maze Watermaze Irrevers. Performance (%)300 400500 8001600 Figure 6: Both short and unnecessarily long context windows decrease the per- formance. Numbers in the legend de- note context lengths. Performance val- ues are relative to those of “Ours (Task Difficulty), Auto” reported in Figure 3. “Irrevers. Path” stands for the task “Irre- versible selection based on task complexities and data sources. For RL tasks, we recommend starting with the curriculum. However, if the task itself is too challenging, such that source algorithms barely make progress, we recommend the - based curriculum. In IL settings, we further investigate the performance of the curriculum on RoboMimic tasks considered in this work. Detailed setup and results are included in Appendix, Sec C.5. To summa- rize, if human demonstrations are available, even if they are generated to be heterogeneous in quality, we recom- mend using the curriculum. However, in the absence of human demonstrations and only with access to data (e.g., generated by RL agents), our curriculum is recommended because it achieves non-trivial performance and signifi- cantly outperforms offline RL methods such as CQL [ 41]. 5 Related Work Sequential with Transformer agents. There are many ongoing efforts to repli- cate the strong emergent properties demonstrated by Transformer models for sequential decision- making problems [ 80]. Decision Transformer [ 13] and Trajectory Transformer [ 37] pioneered this thread by casting offline RL [ 44] as sequence modeling problems. Gato [ 68] learns a massively multi- task agent that can be prompted to complete embodied tasks. MineDojo [ 22] and VPT [ 4] utilize numerous YouTube videos for large-scale pre-training in the video game Minecraft . VIMA [ 38] and RT-1 [ 9] build Transformer agents trained at scale for robotic manipulation tasks. BeT [ 71] and C-BeT [ 14] design novel techniques to learn from demonstrations with multiple modes with Trans- 9 formers. Our causal policy most resembles to VPT [ 4]. But we focus on designing learning techniques that are generally effective across a wide spectrum of learning scenarios and application domains. Cross-episodic learning. Cross-episodic learning is a less-explored terrain despite that it has been discussed together with meta-RL [ 77] for a long time. RL2[18] uses recurrent neural networks for online meta-RL by optimizing multi-episodic value functions. [ 21] instead learns multi-episodic value functions in an offline manner. Algorithm Distillation (AD) [ 42] and Adaptive Agent (AdA) [ 1] are two recent, inspiring methods in cross-episodic learning. Though at first glance our curriculum appears similar to AD, significant differences emerge. Unlike AD, which focuses on in-context improvements at test time and requires numerous single-task source agents for data generation, our approach improves data efficiency for Transformer agents by structuring data in curricula, requiring only a single multi-task agent and allowing for diverse task instances during evaluations. Meanwhile, AdA, although using cross-episodic attention with a Transformer backbone, is rooted in online RL within a proprietary environment. In contrast, we focus on offline behavior cloning in accessible, open-source environments, also extending to IL scenarios unexplored by other meta-learning techniques. Complementary to this, another recent study [43] provides theoretical insight into cross-episodic learning. Curriculum learning. Curriculum learning represents training strategies that organize learning samples in meaningful orders to facilitate learning [ 7]. It has been proven effective in numerous works that adaptively select simpler task [ ] or auxiliary rewards[ 35,72]. Tasks are also parameterized to form curricula by manipulating goals [ 24,30,66], environment layouts[ 79, 3,64], and reward functions [ 28,34]. Inspired by this paradigm, our work harnesses the improving nature of sequential experiences to boost learning efficiency and generalization for embodied tasks. 6 Conclusion In this work, we introduce a new learning algorithm named Cross-Episodic Curriculum to enhance the sample efficiency of policy learning and generalization capability of Transformer agents. It leverages the shifting distributions of past learning experiences or human demonstrations when they are viewed as curricula. Combined with cross-episodic attention, CEC yields embodied policies that attain high performance and robust generalization across distinct and representative RL and IL settings. CEC represents a solid step toward policy learning and is promising for data-scarce problems and real-world domains. Limitations and future work. The CEC algorithm relies on the accurate formulation of curricular sequences that capture the improving nature of multiple experiences. However, defining these sequences accurately can be challenging, especially when dealing with complex environments or tasks. Incorrect or suboptimal formulations of these sequences could negatively impact the algorithm’s effectiveness and the overall learning efficiency of the agents. A thorough exploration regarding the attainability of curricular data is elaborated upon in Appendix, Sec D. In subsequent research, the applicability of CEC to real-world tasks, especially where task difficulty remains ambiguous, merits investigation. A deeper assessment of a demonstrator’s proficiency trajectory — from initial unfamiliarity to the establishment of muscle memory — could offer a valuable learning signal. Moreover, integrating real-time human feedback to dynamically adjust the curriculum poses an intriguing challenge, potentially enabling CEC to efficiently operate in extended contexts, multi-agent environments, and tangible real-world tasks. and Disclosure of Funding We thank Guanzhi Wang and Annie Xie for helpful discussions. We are grateful to Yifeng Zhu, Zhenyu Jiang, Soroush Nasiriany, Huihan Liu, and Rutav Shah for constructive feedback on an early draft of this paper. We also thank the anonymous reviewers for offering us insightful suggestions and kind encouragement during the review period. This work was partially supported by research funds from Salesforce and JP Morgan. 10  
ecosystem level analysis of deployed machine learning reveals homogeneous outcomes 	 Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes Connor Toups∗ Stanford Bommasani∗† Stanford University Kathleen A. Creel Northeastern H. Bana Chapman UniversityDan Jurafsky Stanford Liang Stanford University Abstract Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models. In practice, however, the societal impact of any machine learning model depends on the context into which it is deployed. To capture this, we introduce analysis : rather than analyzing a single model, we consider the collection of models that are deployed in a given context. For example, analysis in hiring recognizes that a job candidate’s outcomes are determined not only by a single hiring algorithm or firm but instead by the collective decisions of all the firms to which the candidate applied. Across three modalities (text, images, speech) and eleven datasets, we establish a clear trend: deployed machine learning is prone to systemic failure , meaning some users are exclusively misclassified by all models available. Even when individual models improve over time, we find these improvements rarely reduce the prevalence of systemic failure. Instead, the benefits of these improvements predominantly accrue to individuals who are already correctly classified by other models. In light of these trends, we analyze medical imaging for dermatology, a setting where the costs of systemic failure are especially high. While traditional analyses reveal that both models and humans exhibit racial performance disparities, analysis reveals new forms of racial disparity in model predictions that do not present in human predictions. These examples demonstrate that analysis has unique strengths in characterizing the societal impact of machine learning.1 1 Introduction Machine learning (ML) is pervasively deployed. Systems based on ML mediate our communication and healthcare, influence where we shop or what we eat, and allocate opportunities like loans and jobs. Research on the societal impact of ML typically focuses on the behavior of individual models. If we center people, however, we recognize that the impact of ML on our lives depends on the aggregate result of our many interactions with ML models. In this work, we introduce analysis to better characterize the societal impact of machine learning on people. Our insight is that when a ML model is deployed, the impact on users depends not only on its behavior but also on the behavior of other models and (left of Figure 1). For example, the decision of a single hiring algorithm to reject or accept a candidate does ∗Equal contribution. †Corresponding author: . 1All code is available at . 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Figure 1: analysis. Individuals interact with ( left), receiving outcomes that constitute the failure matrix ( right ). not determine whether or not the candidate secures a job; the outcome of her search depends on the decisions made by all the firms to which she applied. Likewise, in selecting consumer products like voice assistants, users choose from options such as Amazon Alexa, Apple Siri, or Google Assistant. From the user’s perspective, what is important is that at least one product works. In both settings, there is a significant difference from the user’s perspective between systemic failure , in which zero systems correctly evaluate them or work for them, and any other state. The difference in marginal utility between zero acceptances and one acceptance is typically much higher than the difference between one acceptance and two acceptances. This non-linearity is not captured by average error metrics. For example, imagine that three companies are hiring and there are ten great candidates. All three companies wrongly reject the same two great candidates for a false negative error rate of 20%. Now imagine that each company wrongly rejects different candidates. The second decision ecosystem has the same false negative error rate, 20%, but no systemic failures or jobless candidates. analysis is a methodology that centers on the failure matrix (F; right of Figure 1): Fencapsulates the outcomes individuals receive from all . Of special interest aresystemic failures : exclusively negative outcomes for individuals such as or rejections from all [Bommasani et al., 2022]. To establish general trends in deployed machine learning, we draw upon a large-scale audit [ HAPI ; Chen et al., 2022a] that spans three modalities (images, speech, text), three commercial systems per modality, and eleven datasets overall. Because HAPI contains predictions from some of the largest commercial ML providers – including Google, Microsoft, and Amazon – and the models evaluated are deployed models that real users interact with, evaluating HAPI has real-world implications. Across all settings, analysis reveals a consistent pattern of homogeneous outcomes . In each of the HAPI datasets, many instances are classified correctly by all three commercial systems and many instances are classified incorrectly by all three systems. The pattern of outcomes across the decision ecosystem is homogenous if the rates of systemic failure and consistent classification success significantly exceed the rate predicted by independent instance-level behavior. Since the commercial systems analyzed in this dataset are popular and widely used, being failed by all systems in the dataset has societally meaningful consequences. analysis enriches our understanding not only of the status quo, but also of how models change over time. In particular, it allows us to ask, when individual models improve, how do outcomes change? Since HAPI tracks the performance of the same systems over a three-year period, we consider all cases where at least one of the commercial systems improves. For example, Amazon’s sentiment analysis API reduced its error rate on the WAIMAI dataset by 2.5% from 2020 to 2021; however, this improvement did not decrease the systemic failure rate at all. Precisely 0 out of the model’s 303 improvements are on instances on which all other models had failed. These findings generalize across all cases: on average, just 10% of the instance-level improvement of a single commercial system occurs on instances misclassified by all other models. This is true even though systemic failures account for 27% of the instances on which the models could have improved. Thus most model improvements do not significantly reduce systemic failures. 2 To build on these trends, we study medical imaging, a setting chosen because the costs to individuals of systemic failure of medical imaging classification are especially high. We compare outcomes from prominent dermatology models and dermatologists on the DDI dataset [Daneshjou et al., 2022]: both models and humans demonstrate homogeneous outcomes, though human outcomes are more homogenous. Given established racial disparities in medicine for both models and humans, fairness analyses in prior work show that both humans and models consistently perform worse for darker skin tones (e.g. Daneshjou et al. [2022] show lower ROC-AUC on DDI). analysis surfaces new forms of racial disparity in models that do not present in humans: models are more homogenous when evaluating images with darker skin tones, meaning that all systems agree in their correct or incorrect classification, whereas human homogeneity is consistent across skin tones. Our work contributes to a growing literature on the homogeneous outcomes of modern machine learning [Ajunwa, 2019, Engler, 2021, Creel and Hellman, 2022, Bommasani et al., 2022, Fishman and Hancox-Li, 2022, Wang and Russakovsky, 2023, Jain et al., 2023]. While prior work conceptual- izes these phenomena, our work introduces new methodology to study these problems and provides concrete findings for a range of ML deployments spanning natural language processing, computer vision, speech, and medical imaging. Further, by centering individuals, we complement established group-centric methods [Barocas and Selbst, 2016, Buolamwini and Gebru, 2018, Koenecke et al., 2020], unveiling new forms of racial disparity. analysis builds on this existing work, providing a new tool that contributes to holistic evaluations of the societal impact of machine learning. Developing better methodologies for analysis of deployed machine learning systems is important for two reasons. First, systemic failures in socially consequential domains could exclude people from accessing goods such as jobs, welfare benefits, or correct diagnoses. Individuals who are failed by only one model can gain informal redress by switching to another model, for example by seeking second doctor’s opinion or switching banks. Individuals failed by allmodels cannot. Socially consequential systemic failures can happen due to reliance on APIs, such as image recognition APIs used to identify cancers, speech recognition APIs used to verify individuals for banking, or facial recognition APIs used to unlock devices. Systemic failures can also occur in algorithmic decision- making systems such as those used for hiring, lending, and criminal justice. The social importance of avoiding systemic failures in all of these systems is clear. Second, as become more likely to rely on the same or similar algorithms to make decisions [Kleinberg and Raghavan, 2021] or to use the same or similar components in building their decision pipelines [Bommasani et al., 2022], we believe that the prevalence of systemic failures could increase. Measuring systemic failures as they arise with the tools we present in this paper will expand our understanding of their prevalence and likely causes. 2 Analysis How individuals interact with deployed machine learning models determines ML’s impact on their lives. In some contexts, individuals routinely interact with multiple ML models. For example, when a candidate applies to jobs, they typically apply to several firms. The decision each company makes to accept or reject the candidate may be mediated by a single hiring algorithm. In other contexts, individuals select a single model from a set of options. For example, when a consumer purchases a voice assistant, they typically choose between several options (e.g. Amazon Alexa, Google Assistant, Apple Siri) to purchase a single product (e.g. Amazon Alexa). Centering people reveals a simple but critical insight: exclusively receiving negative outcomes, as when individuals are rejected from every job or unable to use every voice assistant, has more severe consequences than receiving even one positive outcome. 2.1 Definition Recognizing how ML is deployed, we introduce analysis as a methodology for characterizing ML’s cumulative impacts on individuals. Consider Nindividuals that do, or could, interact with that apply ˆylabels according to their processes h1, . . . , h k. Individual iis associated with input xi, label yi, and receives the label ˆyj i=hj(xi)from decision-maker j. 3 Outcomes. Define the failure matrix F∈ {0,1}N×ksuch that F[i, j] = Ih ˆyj i̸=yii . The failure outcome profiles fifor individual i, which we refer to as the outcome profile for brevity, denotes F[i,:]. Thefailure rate ¯fjfor decision-maker jis¯fj=PN i=1F[i,j] N(i.e. the empirical classification error in classification). For consistency, we order the entries of (and, thereby, the columns of the failure matrix) in order of ascending failure rate: F[:,1]is the outcome profile associated with the decision-maker with the fewest failures and F[:, k]is the outcome profile associated with the decision-maker with the most failures. The failure matrix is the central object in analysis (see Figure 1). Systemic Failures. Individual iexperiences systemic failure if they exclusively experience failure across the domain of interest: F[i,:] = [1 , . . . ,1]. Not only are systemic failures the worst possible outcomes, but they also often result in additional harms. If an individual applying to jobs is rejected everywhere, they may be unemployed. If no commercial voice assistant can recognize an individual’s voice, they may be fully locked out of accessing a class of technology. In our analysis, we focus on systemic failures as a consequential subset of the broader class of homogeneous outcomes [Bommasani et al., 2022]. 3 Homogeneous Outcomes in Commercial ML APIs (HAPI) To establish general trends made visible through analysis, we draw upon a large-scale three-year audit of commercial ML APIs [ HAPI ; Chen et al., 2022a] to study the behavior of deployed ML systems across three modalities, eleven datasets, and nine commercial systems. 3.1 Data Chen et al. [2022a] audit commercial ML APIs, tracking predictions across these APIs when evaluated on the same eleven standard datasets over a period of three years (2020 – 2022). We consider ML APIs spanning three modalities (text, images, speech), where each modality is associated with a task (SA: sentiment analysis, FER: facial emotion recognition, SCR: spoken command recognition) and 3 APIs per modality (e.g. IBM, Google, Microsoft for spoken command recognition). The models evaluated are from Google (SA, SCR, FER), Microsoft (SCR, FER), Amazon (SA), IBM (SCR), Baidu (SA), and Face++ (FER). Additionally, each modality is associated with three to four datasets, amounting to eleven datasets total; further details are deferred to the supplement. To situate our notation, consider the DIGIT dataset for spoken command recognition and the associated APIs (IBM, Google, Microsoft). For each instance (i.e. image) xiinDIGIT , the outcome profile fi∈ {0,1}3is the vector of outcomes. The entries are ordered by ascending model failure rate: F[:,1] corresponds to the most accurate model (Microsoft) and to the least accurate model (Google). Facial emotion recognition Spoken command recognition Sentiment analysis RAFDB AFNET EXPW FER + FLUENT DIGIT AMNIST SHOP YELP IMDB WAIMAI Dataset size 15.3k 287.4k 31.5k 6.4k 30.0k 2.0k 30.0k 62.8k 20.0k 25.0k 12.0k Number of classes 7 7 7 7 31 10 10 2 2 2 2 h1failure rate (i.e. error) 0.283 0.277 0.272 0.156 0.019 0.217 0.015 0.078 0.043 0.136 0.110 h2failure rate (i.e. error) 0.343 0.317 0.348 0.316 0.025 0.259 0.015 0.095 0.111 0.219 0.151 h3failure rate (i.e. error) 0.388 0.359 0.378 0.323 0.081 0.472 0.043 0.122 0.486 0.484 0.181 Systemic failure rate 0.152 0.178 0.181 0.066 0.01 0.129 0.002 0.039 0.021 0.043 0.065 Table 1: Basic statistics on HAPI datasets including the (observed) systemic failure rate (i.e. fraction of instances misclassified by all models). Descriptive statistics. To build general understanding of model performance in HAPI , we provide basic descriptive statistics (Table 1). For most datasets, all APIs achieve accuracies within 5–10% of each other (exceptions include DIGIT ,YELP ,IMDB ). Interestingly, we often find the systemic failure rate is roughly half the failure rate of the most accurate model h1. 4 3.2 Behavior In order for a measure of systemic failure to be useful, it must be (i) meaningful and (ii) comparable across systems. A challenge to the meaningfulness of any proposed metric is that systemic failures occur more often in an ecosystem with many inaccurate models. A metric for systemic failure that primarily communicated the aggregate error rates of models in the ecosystem would not be meaningful as an independent metric. It also would not support goal (ii) because we could not compare the rates of systemic failure across ecosystems with varying model accuracies. It would be difficult to identify a system with a ‘large’ rate of systemic failure because the systemic failure properties would be swamped by the error rates of the models in the ecosystem. Therefore, achieving meaningfulness and comparability requires the metric to incorporate error correction. Assuming model independence is a helpful baseline because it adjusts for model error rates without making assumptions about the correlation between models in the ecosystem. To avoid assumptions and for the sake of simplicity, therefore, we juxtapose the observed behavior with a simple theoretical model in which we assume models fail independently of each other. Under this assumption, the distribution of the baseline number of model failures t∈ {0, . . . , k }follows a distribution parameterized by their failure rates (Equation 2). The baseline of independence also means that our metric does not attempt to quantify whether it is “reasonable” that the models all fail on some instances. For example, some instances might be harder (or easier) than others, making it more likely that all models will fail (or succeed) to classify that instance. However, “hardness” is . What is hard for one class of models might be easy for another class of model, and likewise with humans with particular capabilities or training. Therefore ’correcting’ the metric to account for hardness would relativize the metric to the group of humans or class of models for whom that instance is hard. We choose independence as a baseline to be neutral on this point. However, we depart from independence in Appendix §A.3, exploring how a baseline that assumes some level of correlation between models can more accurately model the observed distribution of outcomes. Comparing the true observed distribution of outcomes with the baseline distribution helps illuminate how correlated outcomes are across models. Below we define Pobserved (Equation 1) andPbaseline (Equation 2). Pobserved (tfailures ) =PN i=1Ih t=Pk j=1F[i, j]i N(1) Pbaseline (tfailures ) = (¯f1, . . . , ¯fk)[t] (2) Finding 1: Homogenous Outcomes. In Figure 2a, we compare the observed and baseline distri- butions for the spoken command recognition dataset DIGIT . We find the observed outcomes are more clearly homogenous compared to the baseline distribution: the fraction of in- stances that receive either extreme outcome (all right or all wrong) exceeds the baseline rate. These findings generalize to all the datasets (Figure 2b): the observed rate always exceeds the baseline rate for the homogeneous outcomes (above the line y=x) and the reverse mostly holds for intermediary outcomes. 4 Do Model Improvements Improve Systemic Failures? The performance of a deployed machine learning system changes over time. Developers serve new versions to improve performance [Chen et al., 2022b,a], the test distribution shifts over time [Koh et al., 2021], and the users (sometimes strategically) change their behavior [Björkegren et al., 2020]. In spite of this reality, most analyses of the societal impact of machine learning only consider static models. analysis provides a new means for understanding how models change and how those changes impact people. When models change, what are the broader consequences across their model ecosystem? Do single-model improvements on average improve outcomes by reducing systemic failures? And to what extent are the individuals for whom the model improves the same individuals were previously systemically failed? 5 (a) outcomes for DIGIT dataset. (b) outcomes for all datasets. Figure 2: Homogeneous outcomes. analysis surfaces the general trend of ho- mogeneous outcomes : the observed rates that all models succeed/fail consistently exceeds the corresponding baseline rates. Figure 2a shows that models in the DIGIT dataset are more likely to all fail or all succeed on an instance than baseline. Figure 2b shows that across all datasets, systemic failure (red dots) and consistent success (blue dots) of all three models on an instance are both more common than baseline, whereas intermediate results are less common than baseline. (a) Outcome profile distribution for (Baidu, Google) when Amazon improves on WAIMAI from 2020 to 2021. (b) The distribution of outcome profiles for all year-over- year model improvements across all datasets. Figure 3: Model improvement is not concentrated on systemic failures. When a model improves, we compare the distribution of outcome profiles of the other two models on its initial failures ( potential improvements ) to the distribution on the instances it improved on ( observed improvements ). Across all improvements, including Amazon’s improvement on WAIMAI (left), there is a clear on [✓,✓] (above y=xonright ) and on [X, X] (below the identity line on right ). Setup. Chen et al. [2022a] evaluated the performance of the commercial APIs on the same eleven evaluation datasets each year in 2020–2022. Of all year-over-year comparisons, we restrict our attention to cases where one of the three APIs for a given task improves by at least 0.5% accuracy.2 Lethimpdenote the model that improved. We identify the instances that himpinitially misclassified in the first year as potential improvements and the subset of these instances that himpcorrectly classified in the second year as improvements . Considering the initial distribution of failures for himp, we can ask where does the himpimprove? We answer this by comparing the distribution of outcome profiles for the other models (besides himp) between the potential improvement and improvement sets. 2The supplement §B.3 contains an analysis that confirms our findings are robust to alternate thresholds. 6 Finding 2: Model improvements make little progress on systemic failures. As a case study, we consider Amazon’s improvement on the WAIMAI dataset from 2020 to 2021. In Figure 3a, from left to right, [X, X] indicates the other APIs (Baidu, Google) both fail, [X, ✓] and [✓, X] indicate exactly one of the other APIs succeed, and [ ✓,✓] indicates both succeed. The majority (64.7%) of Amazon’s improvement is on instances already classified correctly by the other two APIs, far exceeding the fraction of potential improvements that were classified correctly by Baidu and Google (34.1%) in 2020. In contrast, for systemic failures, the improvement of 11.6% falls far short of the potential improvement of 36.7%. In fact, since models can also fail on instances they previously classified correctly, the model’s improvement on systemic failures is even worse in terms of net improvement. Amazon’s improvement amounts to no net reduction of systemic failures : the model improves on 78 systemic failures but also regresses on 78 instances that become new systemic failures, amounting to no net improvement. The Baidu and Google APIs similarly show little improvement on systemic failures even as models improve. This finding is not unique to Amazon’s improvement on the WAIMAI dataset: in the 11 datasets we study, we observe the same pattern of homogeneous improvements from 2020-2022. In Figure 3b, we compare the observed improvement ) to the potential improvement distributions ( x axis) across all model improvements. We find a clear pattern: systemic failures (the [X, X] category) are represented less often in the observed improvement set than in the potential improvement set. This finding indicates that when models improve, they under-improve on users that are already being failed by other models. Instead, model improvements especially concentrate on instances where both other models succeeded already. analysis in the context of model improvements disambiguates two plausible situations that are otherwise conflated: does single-model improvement (i) marginally or (ii) substantively reduce systemic failures? We find the reduction of systemic failures is consistently marginal: in every case, the reduction fails to match the the distribution seen in the previous year (i.e. every [X, X] red point is below the line in Figure 3b). 5 Analysis in Dermatology (DDI) Having demonstrated that analysis reveals homogeneous outcomes across machine learning deployments, we apply the methodology to medical imaging. We consider this setting to be an important use of analysis because machine learning makes predictions that inform the high-stakes treatment decisions made by dermatologists. 5.1 Data Daneshjou et al. [2022] introduced the Diverse Dermatology Images ( DDI) dataset of 656 skin lesion images to evaluate binary classification performance at detecting whether a lesion was malignant or benign. Images were labelled with the ground-truth based on confirmation from an external medical procedure (biopsy). In addition, each image is annotated with skintone metadata using the Fitzpatrick scale according to one of three categories: Fitzpatrick I & II (light skin), Fitzpatrick III & IV (medium skin), and Fitzpatrick V & VI (dark skin). We use the predictions from Daneshjou et al. [2022] on DDI for two prominent ML models (ModelDerm [Han et al., 2020] and DeepDerm [Esteva et al., 2017]) and two defer further details to Appendix C. 5.2 Results Finding 3: Both humans and models yield homogeneous outcomes; humans are more ho- mogeneous. We compare observed andbaseline outcomes on DDI for models (Figure 4a) and humans (Figure 4b). Consistent with the general trends in HAPI , model predictions yield homogeneous outcomes. For human predictions from dermatologists, we also 3The supplement §B.4 contains analysis comparing ‘net improvements’ to ‘potential improvements’ as well; the trends are consistent across both analyses. 4Daneshjou et al. [2022] also evaluated a third model [HAM10K; Tschandl et al., 2018] that almost always predicts the majority class in this setting. We exclude this model since its failures are not interesting, but replicate our analyses in the supplement Appendix C to show the findings still hold if it is included. 7 (a) Outcome profiles for models. (b) Outcome profiles for humans. Figure 4: Homogeneous outcomes for models and humans. Consistent with HAPI , model predictions ( left) yield homogenous outcomes on DDI. Human predictions ( right ) are even more homogenous than models. (a) Model outcome profiles by skin tone. (b) Human outcome profiles by skin tone. Figure 5: Racial disparities for models but not humans. We stratify analysis in DDI by the three skin tone categories, plotting the difference between observed andbaseline rates. Models ( left) show clear racial disparities, exhibiting the most homogeneity for the darkest skin tones, whereas humans ( right ) show no significant racial disparity. see that outcomes are homogeneous. However, in comparing the two, we find that humans yield even more homogeneous outcomes. We take this to be an important reminder: while we predict that models are likely to produce homogeneous outcomes, we should also expect humans to produce homogeneous outcomes, and in some cases more homogeneous outcomes. Finding 4: analysis reveals new racial disparities in models but not humans. Standard analyses of machine learning’s impact focus on model performance across groups [e.g. Buolamwini and Gebru, 2018, Koenecke et al., 2020]. In AI for medicine, several works have taken such an approach towards understanding fairness [Obermeyer et al., 2019, et al., 2021, Kim et al., 2022, Colwell et al., 2022]. Daneshjou et al. [2022] demonstrate racial disparities for both model and human predictions on DDI and find that predictive performance is worst for darkest skin tones, aligning with broader trends of racial discrimination in medicine and healthcare [Vyas et al., 2020, Williams and Wyatt, 2015, Fiscella and Sanders, 2016]. analysis can build on these efforts. We conduct the same analysis from Figure 4 stratified by skin tone. In previous experiments, we had observed systemic failures across the whole population. Here we measure systemic failures for subpopulations grouped by skin tone. In Figure 5, we plot the difference between the observed and baseline rates on the yaxis: the Allbars ( blue) reproduce the homogeneity results from Figure 4. 8 Strikingly, surfaces an important contrast between model behavior ( left) and human behavior ( right ). Models (Figure 5a) are most homogenous for darkest skin tones (Fitzpatrick V & VI; dark brown ) and least homogeneous for the lightest skin tones (Fitzpatrick I & II; cream ): The observed systemic failure rate for the darkest skin tones is 8.2% higher than the baseline, while for the lightest skin tones it is 1.5% lower than the baseline. By contrast, humans (Figure 5b) show no significant variation as a function of skin tone. analysis therefore identifies a new form of racial disparity not previously documented. Critically, while prior works show racial disparities for both models and humans, here we find a form of racial disparity that is salient for models but not present for humans. We note that, in absolute terms, homogeneity is higher for all racial groups in human predictions than model predictions, though human predictions don’t display significant differences in homogeneity across racial group. This demonstrates that analysis can reveal new dimensions of fairness, allowing stakeholders to identify the metrics that are most relevant in their context, be that model error or systemic failure rate. Our tool helps researchers and stakeholders evaluate the holistic ecosystem of algorithmic – and human – judgements that ultimately shapes outcomes for those subject to algorithmic judgement. 6 Commentary While analysis reveals new dimensions of machine learning’s societal impact, it also opens the door for further questions. We prioritize two here, deferring further discussion and a longer related work section to the supplement. How can we explain the pervasive homogeneous outcomes we have observed in deployed machine learning? And what are the implications of this work for both researchers and policymakers? 6.1 Explanations for Homogeneous Outcomes Our findings provide evidence across several settings for homogeneous outcomes in deployed machine learning (Finding 1; §3) that are mostly unabated by model improvement (Finding 2; §4). Data-centric explanations. We posit that “example difficulty” may give rise to homogeneous outcomes and provide three analyses that instantiate variants of this hypothesis. First, human ambiguity on the ground-truth may predict homogeneous outcomes. To test this, we make use of the ten human annotations per example in the FER+dataset within HAPI . We find that the systemic failure rate is monotonically correlated with annotator disagreement with the majority label. This suggests that some systemic failures are correlated with the ambiguity or “hardness” of the image. However, we find that even some of the least ambiguous images, namely images on which all or most annotators agree, have systemic failures. . This indicates that human ambiguity is only partially explanatory of model outcomes. We explore this further in §A.1. Second, human error may predict homogeneous outcomes. To test this, we compare human pre- dictions on DDI with the ground truth biopsy results. We stratify analysis by dermatologist performance, comparing (i) instances both dermatologists get right, (ii) instances they both get wrong, and (iii) instances where they disagree and exactly one is right. We find that when both dermatologists fail, there continues to be outcome homogenization. However, when both derma- tologists succeed, there is no homogeneity and the observed rates almost exactly match the baseline rates for every image. This suggests human error is also partially predictive of model outcomes. We explore this further in §A.2. Finally, more expressive theoretical models can better capture the observed trends than our simple full instance-level independence model. We introduce a two-parameter model. αfraction of instances are categorized as difficult and the remaining 1−αare easy. A model’s failure rate ¯fjover all examples scales to (1 + ∆) ¯fjon hard examples and 1−α∆ 1−α ¯fjon easy examples. Partitioning examples in this way, while continuing to assume instance-level independence, inherently homogenizes: models are more likely to systemically succeed on easy instances and systemically fail on hard instances. To best fit the HAPI data, the resulting average α(0.2−0.3) and ∆(1−4, meaning these examples are2−5×harder) values are quite extreme. In other words, for this theoretical model to fit the 9 data, a significant fraction ( ≈25%) would need to be considerably harder ( ≈3.5×) than the overall performance. We explore this further in §A.3. These analyses contribute to an overarching hypothesis that example difficulty partially explains homogeneous outcomes. While we discuss the construct of difficulty in the supplement, we draw attention to two points. First, difficulty is largely in the eye of the beholder: what humans or models perceive as difficult can differ [e.g. adversarial examples; Goodfellow et al., 2014]. Thus while example difficulty could be caused by inherent properties of the example (such as noisy speech or corrupted images), it could just as well be due to model properties, such as all the models having made similar architectural assumptions or having parallel limitations in their training data. Second, whether or not homogeneous outcomes are caused by example difficulty does not change their societal impact. The consequences of systemic failure can be material and serious (e.g. unemployment). Model-centric Explanations and Algorithmic Monoculture. An alternative family of explana- tions put forth in several works is that correlated outcomes occur when different deployed machine learning models share training data, model architectures, model components, learning objectives or broader methodologies [Ajunwa, 2019, Engler, 2021, Bommasani et al., 2021, Creel and Hellman, 2022, Fishman and Hancox-Li, 2022, Bommasani et al., 2022, Wang and Russakovsky, 2023]. Such algorithmic monoculture [Kleinberg and Raghavan, 2021, Bommasani et al., 2022] in fact appears to be increasingly common as many deployed systems, including for the tasks in HAPI , are derived from the same few foundation models [Bommasani et al., 2023, M ˛ adry, 2023]. Unfortunately, we are unable to test these hypotheses because we know very little about these deployed commercial systems, but we expect they form part of the explanation and encourage future work in this direction. Implications for Researchers. Our paper shows that research can reveal social impacts of machine learning. We believe this methodology concretizes the impact of in real contexts in which individuals would typically be affected by decisions from many actors (e.g. job applications, medical treatment, loan applications, or rent pricing). As we demonstrate in DDI, our methodology applies equally to human-only, machine-only, and more complex intertwined . We anticipate understanding of homogeneous outcomes arising from any or all of these sources will be valuable. Implications for Policymakers. Given the pervasive and persisting homogeneous outcomes we document, there may be a need for policy intervention. In many cases no single decision-maker can observe the decisions made by others in the ecosystem, so individual (such as companies) may not know that systemic failures exist. In addition, systemic failures are not currently the responsibility of any single decision-maker, so no decision-maker is incentivized to act alone. Consequently, policy could implement mechanisms to better monitor outcomes and incentivize improvement. In parallel, regulators should establish mechanisms for recourse or redress for those currently systemically failed by machine learning [see V oigt and von dem Bussche, 2017, Cen and Raghavan, 2023]. 7 Conclusion We introduce analysis as a new methodology for understanding the cumulative societal impact of machine learning on individuals. Our analysis on HAPI establishes general trends towards homogeneous outcomes that are largely unaddressed even when models improve. Our analysis on DDI exposes new forms of racial disparity in medical imaging that arise in model predictions but not in human predictions. Moving forward, we hope that future research will build on these empirical findings by providing greater theoretical backing, deeper causal explanations, and satisfactory sociotechnical mitigations. To ensure machine learning advances the public interest, we should use approaches like analysis to holistically characterize its impact. 10  and Disclosure of Funding We would like to thank Shibani Santurkar, Mina Lee, Deb Raji, Meena Jagadeesan, Judy Shen, p-lambda, and the Stanford ML group for their feedback on this work. We would like to thank James Zou and Lingjiao Chen for guidance with using the HAPI dataset. We would like to thank Roxana Daneshjou for providing the DDI dataset along with guidance on how to analyze the dataset. In addition, the authors would like to thank the Stanford Center for Research on Foundation Models (CRFM) and Institute for Human-Centered Artificial Intelligence (HAI) for providing the ideal home for conducting this research. RB was supported by the NSF Graduate Research Fellowship Program under grant number DGE-1655618. This work was supported in part by a Stanford HAI/Microsoft Azure cloud credit grant and in part by the AI2050 program at Schmidt Futures (Grant G-22-63429). References Rishi Bommasani, Kathleen Creel, Ananya Kumar, Dan Jurafsky, and Percy Liang. Picking on the same person: Does algorithmic monoculture lead to outcome homogenization? In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems , 2022. URL . Lingjiao Chen, Zhihua Jin, Evan Sabri Eyuboglu, Christopher Ré, Matei Zaharia, and James Y Zou. HAPI: A Large-scale Longitudinal Dataset of Commercial ML API Predictions. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 24571–24585. Curran Associates, Inc., 2022a. URL / . Roxana Daneshjou, Kailas V odrahalli, Roberto A. Novoa, Melissa Jenkins, Weixin Liang, Veronica Rotemberg, Justin Ko, Susan M. Swetter, Elizabeth E. Bailey, Olivier Gevaert, Pritam Mukherjee, Michelle Phung, Kiana Yekrang, Bradley Fong, Rachna Sahasrabudhe, Johan A. C. Allerup, Utako Okata-Karigane, James Zou, and Albert S. Chiou. Disparities in dermatology ai performance on a diverse, curated clinical image set. Science Advances , 8(32):eabq6147, 2022. doi: 10.1126/sciadv. abq6147. URL . Ifeoma Ajunwa. The paradox of automation as anti-bias intervention. Cardozo L. Rev. , 41:1671, 2019. URL . Alex Engler. Enrollment algorithms are contributing to the crises of higher education. report, The Brookings Institution, September 2021. URL / / . Kathleen Creel and Deborah Hellman. The algorithmic leviathan: Arbitrariness, fairness, and opportunity in algorithmic systems. Canadian Journal of Philosophy , 52(1): 26–43, 2022. doi: . URL ? . Nic Fishman and Leif Hancox-Li. Should attention be all we need? the epistemic and ethical implications of unification in machine learning. 2022 ACM Conference on Fairness, Accountabil- ity, and Transparency , 2022. URL . 3533206 . Angelina Wang and Olga Russakovsky. Overcoming bias in pretrained models by manipulating the finetuning dataset. arXiv preprint , 2023. URL / 2303.06167 . Shomik Jain, Vinith M. Suriyakumar, Kathleen Creel, and Ashia C. Wilson. Algorithmic pluralism: A structural approach towards equal opportunity. ArXiv , abs/2305.08157, 2023. URL https: // . Solon Barocas and Andrew D. Selbst. Big data’s disparate impact. California Law Review , 104:671, 2016. URL . 11 Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in com- mercial gender classification. In FAT, 2018. URL / . Allison Koenecke, Andrew Joo Hun Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor Toups, John R. Rickford, Dan Jurafsky, and Sharad Goel. Racial disparities in automated speech recognition. Proceedings of the National Academy of Sciences of the United States of Amer- ica, 117:7684 – 7689, 2020. URL . Jon Kleinberg and Manish Raghavan. Algorithmic monoculture and social welfare. Proceedings of the National Academy of Sciences , , 2021. doi: . URL . Lingjiao Chen, Matei Zaharia, and James Zou. How did the model change? efficiently assessing machine learning API shifts. In International Conference on Learning , 2022b. URL . Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal- subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning , pages 5637–5664. PMLR, 2021. URL . Daniel Björkegren, Joshua E. Blumenstock, and Samsun Knight. machine learning, 2020. URL . Seung Han, Ilwoo Park, Sung Chang, Woohyung Lim, Myoung Kim, Gyeong Park, Jebyeong Chae, Chang-Hun Huh, and Jung-Im Na. Augment intelligence dermatology : Deep neural networks empower medical professionals in diagnosing skin cancer and predicting treatment options for 134 skin disorders. Journal of Investigative Dermatology , 140, 03 2020. doi: 10. . URL / . Andre Esteva, Brett Kuprel, Roberto Novoa, Justin Ko, Susan Swetter, Helen Blau, and Sebastian Thrun. classification of skin cancer with deep neural networks. Nature , 542, 01 2017. doi: . URL . Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset: A large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific Data , 5, 08 2018. doi: . URL . Ziad Obermeyer, Brian Powers, Christine V ogeli, and Sendhil Mullainathan. Dissecting racial bias in an algorithm used to manage the health of populations. Science , , 2019. URL . Laleh , Haoran Zhang, Matthew BA McDermott, Irene Y Chen, and Marzyeh Ghassemi. Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations. Nature medicine , , 2021. URL https: // . Yong-hun Kim, Ajdin Kobic, and Nahid Y . Vidal. Distribution of race and fitzpatrick skin types in data sets for deep learning in dermatology: A systematic review. Journal of the American Academy of Dermatology , 87(2):460–461, August 2022. doi: . URL . Rebecca L. Colwell, Anand K. Narayan, and Andrew B. Ross. Patient race or ethnicity and the use of diagnostic imaging: A systematic review. Journal of the American College of Radiology , 19 (4):521–528, April 2022. doi: . URL . . Darshali A. Vyas, Leo G. Eisenstein, and David S. Jones. Hidden in plain sight — reconsidering the use of race correction in clinical algorithms. New England Journal of Medicine , 383(9): 874–882, August 2020. doi: . URL / nejmms2004740 . 12 David R. Williams and Ronald Wyatt. Racial bias in health care and health. JAMA , 314(6):555, August 2015. doi: . URL . Kevin Fiscella and Mechelle R. Sanders. Racial and ethnic disparities in the qual- ity of health care. Annual Review of Public Health , 37(1):375–394, March 2016. doi: . URL / . Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv 1412.6572 , 12 2014. URL . Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, D. Card, Rodrigo Castellon, Niladri S. Chatterji, Annie Chen, Kathleen Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jackson K. Ryan, Christopher R’e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. ArXiv , abs/2108.07258, 2021. URL . Rishi Bommasani, Dilara Soylu, Thomas I Liao, Kathleen A Creel, and Percy Liang. Ecosystem graphs: The social footprint of foundation models. arXiv preprint , 2023. URL . Aleksander M ˛ adry. Advances in ai: Are we ready for a tech revolution? Cybersecurity, Information Technology, and Government Innovation Subcommittee , 2023. URL . . Paul V oigt and Axel von dem Bussche. The eu general data protection regulation (gdpr). 2017. URL . Sarah H Cen and Manish Raghavan. The right to be an exception to a data-driven rule. 2023. URL . Emad Barsoum, Cha Zhang, Cristian Canton-Ferrer, and Zhengyou Zhang. Training deep networks for facial expression recognition with crowd-sourced label distribution. CoRR , abs/1608.01041, 2016. URL . Lisa Feldman Barrett, Ralph Adolphs, Stacy Marsella, Aleix M. Martinez, and Seth D. Pollak. Emo- tional expressions reconsidered: Challenges to inferring emotion from human facial movements. Psychological Science in the Public Interest , 20(1):1–68, 2019. doi: . URL . PMID: 31313636. Tuan Le Mau, Katie Hoemann, Sam H. Lyons, Jennifer M. B. Fugate, Emery N. Brown, Maria Gendron, and Lisa Feldman Barrett. Professional actors demonstrate variability, not stereotypical expressions, when portraying emotional states in photographs. Nature Communications , 12 (1), August 2021. doi: . URL / . 13 Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why exacerbates spurious correlations. In Hal Daumé III and Aarti Singh, ed- itors, Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 8346–8356. PMLR, 13–18 Jul 2020. URL . Shan Li, Weihong Deng, and Junping Du. Reliable crowdsourcing and deep learning for expression recognition in the wild. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017 , pages 2584–2593. IEEE Computer Society, 2017. doi: . URL / CVPR.2017.277 . Ali Mollahosseini, Behzad Hassani, and Mohammad H. Mahoor. Affectnet: A database for facial expression, valence, and arousal computing in the wild. IEEE Trans. Affect. Comput. , 10(1):18–31, 2019. doi: . URL . 2740923 . Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. From facial expression recognition to interpersonal relation prediction. CoRR , abs/1609.06426, 2016. URL / abs/1609.06426 . Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Vikrant Singh Tomar, and Yoshua Bengio. Speech model pre-training for end-to-end spoken language understanding. In Gernot Kubin and Zdravko Kacic, editors, Interspeech 2019, 20th Annual Conference of the International Speech Communica- tion Association, Graz, Austria, 15-19 September 2019 , pages 814–818. ISCA, 2019. doi: 10.21437/ . URL . Zohar Jackson, César Souza, Jason Flaks, Yuxin Pan, Hereman Nicolas, and Adhish Thite. : v1.0.8, August 2018. URL / zenodo.1342401 . Sören Becker, Marcel Ackermann, Sebastian Lapuschkin, Klaus-Robert Müller, and Wojciech Samek. Interpreting and explaining deep neural networks for classification of audio signals. CoRR , abs/1807.03418, 2018. URL . Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA , pages 142–150. The Association for Computer Linguistics, 2011. URL https: /// . 14 Figure 6: We stratify analysis on FER+by instance-level annotator disagreement – which we take as a proxy for the ambiguity inherent to the input instance – and plot the difference between observed andbaseline rates for each instance subset. We observe homogeneous outcomes for all subsets of data, regardless of the level of annotator disagreement on the instance. A Data-centric Explanations for Homogeneous Outcomes Prior work has explored model-centric explanations for homogeneous outcomes [Ajunwa, 2019, Engler, 2021, Bommasani et al., 2021, Creel and Hellman, 2022, Fishman and Hancox-Li, 2022, Bommasani et al., 2022, Wang and Russakovsky, 2023]. However, data-centric explanations are comparatively less explored. We posit that properties of the underlying data could contribute to the homogeneous outcomes that we observe in all of the datasets we examine. Intuitively, if we believe that some examples are ‘hard’ and others are ‘easy’, then we might expect to see models all fail for the ‘hard’ examples and all succeed for the ‘easy’ examples. We test three variants of this hypothesis. In §A.1, we examine how the level of annotator disagreement in the ground truth label impacts behavior. In §A.2, we test how the accuracy of human dermatologists in predicting the malignancy of a skin lesion image correlates with homoge- neous outcomes. Finally, to build on these finer-grained empirical analyses, in §A.3, we introduce a more express theoretical model. Under this model, parameterized by two difficulty parameters, we compute a different baseline rate for outcomes, showing it can better recover the observation distribution. A.1 Annotator disagreement To study the effects of annotator disagreement, we make use of the FER+dataset. Each instance of theFER+dataset, a facial emotion recognition dataset, contains emotion annotations from 10 human annotators; the emotion label is determined by majority vote [Barsoum et al., 2016]. Because each instance has been annotated by multiple annotators, we can calculate the annotator disagreement for each instance and use this as a proxy for the ‘ambiguity’ of the instance. For example, an instance where all 10 annotators agree that the label is ‘sad’ is less ambiguous than an instance where 6 annotators vote the label should be ‘fear’ and 4 vote that the label should be ‘surprise’. The test set of FER+provided in HAPI contains instances with disagreement percentages ranging from 0% to 50%. We stratify on the disagreement percentage of the instances and compare baseline andobserved outcomes for each subset of instances. 15 Figure 7: Examples of homogeneous outcomes Instances that are sampled uniformly at random from “0 models correct” ( top row ) or ”3 models correct” ( bottom row ) in FER+. The systemic failures (top row ) do not appear to be inherently harder for humans to classify; more extensive analysis appears in the supplement. Figure 8: Interpretation of the relationship between instance-level annotator disagreement and homogeneous outcomes depends on if the strength of the effect is quantified as the ratio between observed and baseline rates (left plot) or as the difference between observed and baseline rates (right plot). Examples. To build further intuition, we present several randomly sampled instances from the FER+facial emotion recognition dataset in Figure 7.5We emphasize that while systemic failures may share structure, we do not believe these instances are inherently harder than ones on which models perform well. The authors did not have difficulty labelling these examples, nor would other human labelers. We address the question of why systemic failures arise in §6. Homogenous Outcomes manifest regardless of annotator disagreement. In Figure 6, we find that homogenous outcomes appear for all levels of annotator disagreement. While more ambiguous examples exhibit higher model error rates and systemic failure rates, the observed rate of homogeneous outcomes exceeds the baseline rate of homogeneous outcomes for all instance subsets. This suggests that instance-level ambiguity does not (fully) explain the existence of homogenous outcomes— at least in FER+. The intensity of the effect varies by disagreement level, but the direction of the relationship depends on how strength is quantified. In light of the observed existence of homogeneous 5We acknowledge the task of facial emotion recognition has been the subject of extensive critique [e.g. Barrett et al., 2019, Mau et al., 2021]). We provide examples for this task due to ease of visualization, but our claims also hold for examples from the text and speech modalities that are provided in the supplement. 16 (a) Subset of instances that both dermatologists classify correctly. (b) Subset of instances that pre- cisely one dermatologist misclas- sifies. (c) Subset of instances that both dermatologists misclassify. Figure 9: Homogeneous outcomes exists for the subset of instances where both dermatologists misclassify the image but doesn’t exist for the subset where both dermatologists correctly classify the image. outcomes across all levels of disagreement, we now examine how the intensity of this effect varies with annotator disagreement. The relationship between annotator disagreement and the intensity of homogeneous outcomes depends on the quantification method used to measure homogeneous outcomes intensity. When quantifying the intensity as the difference between observed and baseline rates, the effect becomes more pronounced as disagreement increases. However, when considering the intensity as the ratio between observed and baseline rates, as in the homogenization metric introduced by Bommasani et al. [2022], the effect is most pronounced at lower levels of disagreement. A.2 Human accuracy Each instance of the DDI dataset — a medical imaging skin lesion dataset — contains predictions from 2 models and 2 humans with the ground truth generated from an external process (in this case, a biopsy of the lesion). To understand how difficulty relates to homogeneous outcomes, we stratify instances on the dermatologist accuracy of that instance (each instance has a dermatologist accuracy of 0%, 50%, or 100%) and examine homogeneous outcomes for each subset of instances. Model outcomes are homogenous for instances on which dermatologists fail and heterogeneous for on which dermatologists succeed. In Figure 9, we find that model outcomes exhibit homoge- neous outcomes for the subset of instances that both human dermatologists fail at to an even greater extent than observed at the population level. In contrast, the observed and baseline distributions match each other (meaning there is no homogeneous outcomes, heterogeneity or other form of deviation between the distributions) for the subset of instances that both dermatologists get right. Note that in the instances that both dermatologists fail at, both models systemically fail more than the baseline, but both also succeed more than the baseline. While the correlation between human systemic failures and model systemic failures seems intuitive, it’s less clear why models jointly succeed more than the baseline on human systemic failures. Given these surprising and unintuitive findings, we encourage future work to explain what features of these instances lead models to pattern together. We speculate that these instances lack sufficient informational cues, prompting the models to place excess emphasis on a narrow selection of features, consistent with the literature of machine learning models being susceptible to spurious correlations [e.g. Sagawa et al., 2020]. At face value, these results would suggest that human-level difficulty can be predictive of outcome homogeneity, but we emphasize two caveats: (i) the DDI dataset’s sample size is limited (155 instances of dermatologists both failing and 356 instances of dermatologists both succeeding) and (ii) we rely on the judgments of just two human domain experts, meaning the findings may not generalize to larger annotator pools of non-experts (as are common in many NLP or computer vision datasets). In general, we caution against this finding. We provide it as an initial foray into understanding example difficulty in a unique setting where we have data that supports an approximation of difficulty. 17 A.3 More expressive theoretical models Finally, while we consistently find that observed behaviors yield more homogeneous outcomes than the instance-level independent model predictions would predict, we might intuit there exists instance-level structure that should be encoded into the prior. Therefore, we consider a theoretical framework to encode richer priors on what we might expect of models. As a simple model, we will assume some instances are universally ‘hard’, meaning all models will perform worse on average across these instances, and others are conversely ‘easy‘, meaning all models will perform better on average across these instances. As before, we note that there is not a universal standard according to which examples can be considered ‘hard.’ Some examples are easy for humans but hard for models; other examples are easy for models but challenging for humans; and still other examples are challenging for some models but not others. The three hypotheses we consider in this section explore these dimensions of hardness. We use two parameters ( α,∆) to parameterize this model, thereby adjusting the baseline rate that we calculate for outcomes. αspecifies the composition of ‘hard’ vs ‘easy’ instances in a dataset and ∆controls how much harder or easier the hard or easy examples, respectively, are expected to be. Concretely, αfraction of instances are categorized as difficult and the remaining 1−αare easy. A model’s failure rate ¯fjover all examples scales to ¯fjhard= (1 + ∆) ¯fjon hard examples and ¯fjeasy= 1−α∆ 1−α ¯fjon easy examples. The distribution of the baseline number of model failures t∈ {0, . . . , k }follows a weighted sum of two distributions parameterized by the scaled hard ¯fjhardand easy ¯fjeasymodel error rates. Phard baseline (tfailures ) = ( (¯f1hard, . . . , ¯fkhard)[t] (3) Peasy baseline (tfailures ) = (1 − (¯f1easy, . . . , ¯fkeasy)[t] (4) Pbaseline (tfailures ) =Phard baseline [t] +Peasy baseline [t] (5) Identifying αand∆values that recover the observed outcomes in HAPI. We utilize this framework to identify which ( generate baseline distributions that recover the observed distributions in the HAPI datasets. We perform a grid search for α∈ [0.1,0.5]with a step size of a step size of 0.2. Note that certain (α,∆) combinations can result in invalid error rates depending on the original error rates of models – i.e. when 1−α∆ 1−α ¯fj<0. For each dataset, we identify the ( that minimizes the L1 distance (equivalently the total variation distance) between the observed and baseline distributions. In Figure 10, we visualize the observed and baseline distributions for the (α,∆)pair for each dataset, and in Figure 11 we plot the L1 distance for each dataset as a function of αand∆. The majority of the bestαvalues are 0.2or0.3while the ∆values can range from 1to4– indicating that the ¯fjhardcan be up to 5x higher than ¯fj. High ∆and low αrepresents a small group of very difficult instances that all models struggle at. The combination of high ∆and low αvalues performing well suggests that we would need to expect that a relatively small fraction of the dataset contains instances that are significantly harder for all models to perform well at. Note that this framework assumes that all models consider the same instances to be ‘hard’: this agreement is a form of homogenization that could be caused about something inherent to the data or something about how the models are constructed. B HAPI Experiments In the main paper, we work extensively with the HAPI dataset of Chen et al. [2022a]. While we defer extensive details about the dataset to their work, we include additional relevant details here as well as any relevant decisions we made in using the HAPI dataset. 18 Figure 10: Observed and baseline distributions of outcomes for the (α,∆)combina- tion that yields the lowest L1 distance for each dataset. Figure 11: L1 distance between observed and baseline distributions for all datasets as a function of α and∆. 19 Figure 12: Change in the probability of observing each outcome profile for the other models upon observing hifail. B.1 Data We work with a subset of HAPI , a dataset introduced by Chen et al. [2022a] which contains predictions from commercial ML APIs on a variety of standard benchmark datasets from 2020-2022. HAPI contains benchmark datasets for three classification tasks and three structured prediction tasks. We only work with the classification tasks – spoken command recognition (SCR), sentiment analysis (SA), and facial emotion recognition (FER) – because they contain a single ground truth label where the notion of a ‘failure’ is clear (i.e. a ). The three classifiation tasks span 3 modalities (text, images, speech), and each task is associated with 4 datasets. However, we exclude one of these datasets: the COMMAND dataset has duplicate example IDs with differing predictions from the same ML API provider in the same time period. This makes calculating outcomes impossible. After excluding the COMMAND dataset, there are 11 datasets that we conduct analysis on: RAFDB [Li et al., 2017], AFNET [Mollahosseini et al., 2019], EXPW [Zhang et al., 2016], FER+[Barsoum et al., 2016], FLUENT [Lugosch et al., 2019], DIGIT [Jackson et al., 2018], AMNIST [Becker et al., 2018], SHOP ,6YELP ,7, IMDB [Maas et al., 2011] and WAIMAI .8 Each dataset contains predictions from 3 commercial ML APIs in 2020, 2021, and 2022; however theHAPI API did not return predictions from the Face++ model on AFNET in 2022, so we use 2021 predictions for AFNET when conducting experiments that only use predictions from a single year. TheHAPI dataset is distributed at under Apache License 2.0. B.2 Leader Following Effects in Systemic Failure One consequence of homogeneous outcomes is that it concentrates failures on the same users, so a user who interacts with a model and experiences a failure from that model is now more likely to experience a failure from every other model in the ecosystem. To quantify the strength of this effect, we examine how the probability of a user experiencing each outcome profile changes after that user experiences one failure from a model. In Figure 12, we find that, consistent with the observed homogeneous outcomes in HAPI , observing a single model failure significantly increases the probability that the user will now experience failures from all other models in the ecosystem. However, we also find that the strength of this effect is strongly graded by the accuracy of the model for which we initially observe a failure. Upon observing the most accurate model in the ecosystem fail for a user, the probability of that user experiencing a systemic failure increases by 37% – whereas it only increases by around 16% when we observe the least accurate model fail. 20 Figure 13: We replicate the graph from Figure 3 but with various thresholds for how much a model must improve for us to include it in the analysis. The patterns discussed in §4 are consistent across choice of threshold. This result suggests that instances that the most accurate model fails on are likely to be failed by the less accurate models as well. This ‘leader following’ phenomenon has implications for users in a model ecosystem: users failed by the most accurate model likely have few options for alternative models that could work for them. B.3 Model improvement analysis is insensitive to threshold In §4, we study how the improvement of a single model, in the sense that it becomes more impact, manifests at the . To define improvement, we set the (slightly arbitrary) threshold that the model’s accuracy improve by 0.5%, which we found to be large enough to be material to a model’s performance while small enough to capture most model improvements. In Figure 13, we plot the outcome profile distribution in the observed improvements set against the distribution over the potential improvements (as in Figure 3b) for 6 different thresholds of change. Across all thresholds, the patterns we discuss in §4 hold: namely, models consistently under-improve on systemic failures. This confirms that the findings and qualitative understanding we present is not particularly sensitive to the exact value of this threshold. B.4 Net Improvements In §4, we define improvements as the specific instances that in the first year and correctly classifies in the second year. However, the size of this set — hereafter, gross improvements — is always larger than the number of net improvements the model makes because updates to the model tend to improve on some instances and regress on others. That is, there are two competing forces when models change: the instances the model flips from incorrect to correct, but also the instances the models from correct to incorrect. The difference of (i) and (ii) is the number of net improvements. In Figure 14, we use net improvements instead of gross improvements, replicating the plot from in Figure 3. To calculate the outcome profile distribution over net improvements, we subtract the number of gross declines from the number of gross improvements for each outcome profile: the denominator is the number of gross improvements minus the number of gross declines across all outcome profiles. 21 (a) Outcome profile distribution using net improvements instead of gross improvements for (Baidu, Google) when Amazon improves on WAIMAI from 2020 to 2021. (b) The distribution of outcome profiles for net improve- ments instead of gross improvements for all year-over- year model improvements across all datasets. Figure 14: We replicate the graphs in Figure 3 but using ‘net improvements’ instead of ‘gross improvements’. The overall trends are consistent between the two experiments: namely, himp under-improves on systemic failures. The trends are generally consistent with what we observed when using gross improvements: himp tends to make little progress on systemic failures. In fact, when considering net improvements, models make even less improvement on systemic failures than before. We highlight WAIMAI as a striking case study: there is no net improvements on systemic failures, despite a 2.5% decrease in model error at the population level. Overall, we emphasize that we recommend future research conducts analyses with both notions of improvements. While we expect in many cases, as we have seen here, that the qualitative trends will be similar, the may differ. For example, gross improvements more directly attend to the concern that there are some individuals who, year-over-year, continue to be failed by some or all models in the system. In contrast, net improvements more directly matches the sense in which models are improving. B.5 When Models Get Worse As a related question to what we examine in Figure 3b, we examine what happens to outcomes when a model gets worse. We find that, when models get worse, they introduce new systemic failures into the system by ’over-declining’ on instances that other models were already failing for. This further highlights how single-model measurements often fails to align with outcomes. C Dermatology Experiments C.1 Data We work with DDI (Diverse Dermatology Images), a dataset introduced by Daneshjou et al. [2022], which contains predictions from 3 models and 2 dermatologists on 656 skin lesion images; the task is to predict whether a lesion is malignant or benign. The ground truth label comes from an : in this case, a biopsy of the lesion, which is considered the gold-standard labeling procedure in this domain. The 3 evaluated models include ModelDerm [Han et al., 2020], a publicly available ML API, and two models from the academic literature – DeepDerm [Esteva et al., 2017] and HAM10k [Tschandl et al., 2018] – that were chosen by Daneshjou et al. [2022] on the basis of their "popularity, availability, and previous demonstrations of performance." Note that, in this case, none of the models have been trained on any portion of the DDI dataset; the entire dataset serves as a test set. 22 Figure 15: The distribution of outcome profiles for all year-over- year model declines across all datasets In addition, each image is annotated with skintone metadata using the Fitzpatrick scale according to one of three categories: Fitzpatrick I & II (light skin), Fitzpatrick III & IV (medium skin), and Fitzpatrick V & VI (dark skin). For all instances, the Fitzpatrick classification was determined using consensus review of two dermatologists. Additionally, a separate group of dermatologists rated the image quality of each image and discarded any low quality images; there was no significant difference in image quality ratings between images of different FST . Data on model and dermatologist predictions was graciously provided by Daneshjou et al. [2022], subject to the terms of their standard research use agreement described in . . C.2 Analyses are insensitive to HAM10k In §5, we don’t include predictions from Ham10K because the model predicts a negative on almost all instances: it has a precision of 0.99but a recall of 0.06. We decided to remove HAM10k because the pattern of near-universal negative predictions does not reflect model behavior we would expect of models deployed in clinical settings. Namely, even if deployed, the structure of the model errors are not particularly interesting and are largely predictable (in direction). Beyond these fundamental reasons for excluding the model, we also removed the model for reasons unique to our analysis. Including the model would have introduced an explicit class correlation in systemic failures (i.e. almost all systemic failures would be malignant instances and none would be benign instances) and would have complicated the comparison with humans since there would be three models but only two humans. To confirm that our findings hold independent of this choice, in Figure 16, we replicate Figure 4 and Figure 5 but with outcomes from HAM10k included. We find that inclusion of HAM10k exacerbates the homogeneous outcomes of model outcomes and the racial disparities in model outcomes. 23 (a) Model outcomes when including HAM10K yield even more homogenous outcomes on DDI than in Fig- ure 4 (b) The inclusion of HAM10K yields more pronounced racial disparities than in Figure 5 . Figure 16: We replicate the graphs in Figure 4 and Figure 5 but with the inclusion of HAM10K. Homogeneous outcomes and racial disparities in models are even more pronounced when including HAM10k. 24 
evaluating neuron interpretation methods of nlp models 	Evaluating Neuron Interpretation Methods of NLP Models Yimin Fan♡Fahim Dalvi♢Nadir Durrani♢Hassan Sajjad♣ ♡The Chinese University of Hong Kong, Hong Kong, China ♢Qatar Computing Research Institute, HBKU, Qatar ♣Faculty of Computer Science, Dalhousie University, Canada Abstract Neuron interpretation offers valuable insights into how knowledge is structured within a deep neural network model. While a number of neuron interpretation methods have been proposed in the literature, the field lacks a comprehensive comparison among these methods. This gap hampers progress due to the absence of standardized metrics and benchmarks. The commonly used evaluation metric has limitations, and creating ground truth annotations for neurons is impractical. Addressing these challenges, we propose an evaluation on voting theory. Our hypothesis posits that neurons consistently identified by different methods carry more significant information. We rigorously assess our framework across a diverse array of neuron interpretation methods. Notable findings include: i) despite the theoretical differences among the methods, neuron ranking methods share over 60% of their rankings when identifying salient neurons, ii) the neuron interpretation methods are most sensitive to the last layer , iii) Probeless neuron ranking emerges as the most consistent method. 1 Introduction The advent of deep neural network (DNN) models and their success in Natural Language Processing (NLP) has opened a new research frontier to interpret the learned within these models. Representation analysis provides a holistic view of the knowledge learned within the representation. Whereas neuron analysis gives insight into how knowledge is structured within the representation. To this end, an ample amount of work has been done to understand the knowledge captured within the learned (Belinkov et al., 2017; Liu et al., 2019a; Tenney et al., 2019; Dalvi et al., 2022) and individual neurons (Karpathy et al., 2015; Kádár et al., 2017a; Dalvi et al., 2019). Neuron analysis provides a fine-grained interpretation of the representation. It discovers salient neu- rons of the network w.r.t to a concept such as morphological or syntactic concepts (Lakretz et al., 2019; Durrani et al., 2020), and provides insight into how they may be used during inference (Lundberg & Lee, 2017; Dhamdhere et al., 2018). Beyond understanding the inner dynamics of the DNN models, neuron analysis has various potential applications such as network manipulation (Bau et al., 2019), domain adaptation (Antverg et al., 2022), model distillation (Dalvi et al., 2020) and architectural search (Prasanna et al., 2020). A number of neuron analysis methods have been proposed in the literature to interpret deep NLP models. While these methods have shown promising results, there is a notable absence of systematic comparisons among them in existing studies. Conducting a fair comparison of neuron interpretation methods is crucial for comprehending the merits and limitations of each technique. Such comparisons 37th Conference on Neural Information Processing Systems (NeurIPS 2023). serve as a guiding light for the community, steering efforts towards the development of improved and more effective interpretation methodologies. In this work, we study a variety of neuron interpretation methods under a controlled setting and provide a thorough comparative analysis. There are two main challenges in comparing neuron interpretation methods: i) absence of a standard evaluation metric , ii) lack of an evaluation benchmark (Sajjad et al., 2022b). The most common evaluation metric used in the neuron analysis literature is to train a classifier using the discovered neurons as features. The performance of the classifier in predicting the concept of interest serves as evidence of the quality of the discovered neurons, in other words, the correctness of the neuron interpretation method. We argue that the classifier as an evaluation metric is suboptimal for two reasons: i) training a classifier provides an opportunity to learn the task and it may result in good performance irrespective of the quality of the neurons used as features (Hewitt & Liang, 2019), ii) the classifier as an evaluation method may favor the neuron interpretation methods that are similar (for instance, methods that use a classifier to discover salient neurons). Antverg & Belinkov (2021) shed light on the latter issue and showed that the methodological similarity between the neuron interpretation methods and the evaluation methods may result in unreasonably high accuracy scores. We empirically demonstrate the issue in Section 3. Another bottleneck in comparing neuron interpretation methods is the absence of evaluation benchmark . Neurons are distributive in nature and exhibit polysemy (capturing multiple concepts at the same time). Moreover, there can be different sets of neurons learning a concept, and there exists no single correct answer. It is therefore intricately challenging to create the ground truth annotations and infeasible to scale to a large set of models and concepts. Given the lack of a standard evaluation metric and the infeasibility of creating ground truth annota- tions, how can we compare neuron interpretation methods? We rely on the voting theory (O’Connor & Robertson, 2003) that performs a systematic aggregation of results in order to achieve a consen- sus (Patrão Neves, 2016). It is effectively used in political, social, and communities for cases wherein there are distinct preferences among voters (models). Examples of specific applica- tions of voting theory include ranking search results obtained from various algorithms in information retrieval (Lin et al., 2017; Yilmaz et al., 2008), finding the best machine translation system using pair-wise ranking (Lapata, 2006) and aggregating results of several NLP tasks (Colombo et al., 2022). We hypothesize that neurons that are commonly discovered by different interpretation methods are more informative than others , and may serve as a signal of correctness for the selected neurons. Motivated by this, we devise two voting-based compatibility metrics. Our first metric, AvgOverlap , is based on plurality voting (Cooper & Zillante, 2012). AvgOverlap computes an average overlap of the discovered neurons across all voters. Our second method, NeuronVote , inspired by Borda Count (Lippman, 2012), takes into account the ranking of neurons of each method while calculating the average pair-wise intersection over union with all the other methods. The compatibility metrics provide a single score for each method, where a higher score refers to a neuron interpretation method whose discovered neurons are more compatible with the other methods. We further extend the evaluation of neuron interpretation methods to pair-wise comparison, providing insights into how any two given methods relate in terms of their discovered neurons. We conduct a comparative analysis of six neuron interpretation methods using diverse concepts consisting of morphological, semantic, and syntactic properties and using three pre-trained models. The analysis suggests that the best performing methods, despite their methodological differences, share more than 60% of the top neurons. Probeless ranking Antverg & Belinkov (2022) consistently exhibited the highest overlap with other methods. Because neurons in the last layer are highly correlated, they are the most challenging to interpret. Finally, we present a case study that demonstrates the usefulness of our evaluation methodology for any new method proposed in the future (Appendix 7.1). Our contributions are as follows: •We conduct the first thorough comparative analysis of a large set of neuron interpretation methods of NLP across 3 pre-trained models and using a diverse set of linguistic concepts. •Our proposed evaluation methodology consisting of two compatibility metrics and a pair- wise comparison targets one of the critical limitations of the neuron interpretation studies. •We provide the evaluation methodology as a framework to facilitate future studies and comparison of methods. 2 2 Neuron Interpretation Methods The goal of neuron interpretation methods is to rank Nneurons with respect to some concept C. These methods can be broadly classified into two classes: corpus-based methods and probing-based methods (Sajjad et al., 2022b). The former discovers the role of neurons by aggregating statistics over neuron activations and the latter trains classifiers to achieve the same. LetDbe a dataset of sentences (represented as a list of tokens), where each token wappears in a specific and has a contextual representation associated with it. Let z(n, w)be the activation of the nthneuron and the wthtoken in D. The neuron n∈ N can be from any component of the original model, such as a specific layer. Also note that the activation value z(n, w) in , so the same wcan result in different z(n, w)values depending on context. A concept Crepresents a property we want to discover neurons for and ˆCrepresents a random concept, i.e. containing words that are not associated with the concept C. For example, Ccountry ={Rome, Paris, London, New York, . . . } represent a concept of country names and ˆCwould consist of all other words in the corpus that are not country names. More formally, ˆC=D \ C . the score of a neuron nwith respect to a concept C. Each neuron interpretation method implements R, and provides a ranked list of neurons with respect to a concept. In the following, we present six interpretation in this paper. 2.1 Corpus-based Methods Corpus-based methods align neurons with a concept by accumulating co-occurrence patterns between neuron activations and the presence of the concept of interest. We use two corpus-based methods in this work presented as follows. 2.1.1 Probeless The Probeless method (Antverg & Belinkov, 2021) obtains neuron rankings based on an accumulative strategy. The score of a given neuron nis defined as follows: R(n,C) =µ(C)−µ(ˆC) (1) where µ(C)is the average of activations z(n, w), w∈ C.µ(ˆC)is the average of activations over the random concept ˆC. Note that the ranking for each neuron nis computed independently. 2.1.2 IoU Method Mu & Andreas (2020) proposed IoU to generate compositional explanations for neurons. For each token in the dataset, they create i) a binary mask of a neuron by thresholding activations above a percentile and ii) a binary mask of a concept by checking its presence in the token. They compute Intersection over Union (IoU) over the two masks: R(n,C) =P w1(z(n, w)> δ)∧ 1(w∈ C)P w1(z(n, w)> δ)∨ 1(w∈ C)(2) where 1is the indicator function. z(n, w)> δis a binary mask over neuron activations based on the threshold δ.w∈ C is a binary mask, created by checking if the current word represents the concept Cof interest. Mu & Andreas (2020) used IoU scores to generate alternative explanations for neurons on the task of image classification. Here we apply their method in the NLP domain to generate neuron ranking for any concept of interest. Like Probeless, the ranking for every neuron is computed independently of other neurons. 2.2 Probing Methods Probing methods train a classifier using neuron activations as features for the task of predicting the concept of interest (Belinkov et al., 2017; Hupkes et al., 2018). The internals of the trained classifier (e.g. its weights) are used to rank the features (neurons). 2Available in the NeuroX toolkit Dalvi et al. (2023). 3 2.2.1 Lasso Regularizer (L1) Radford et al. (2019) trained a linear classifier with L1 regularization and used the weights of the classifier as a proxy for the importance of neurons for the given concept. The loss function of the classifier is as follows: L(θ) =−X wlogPθ(c|zN(w)) +λ1∥θ∥1 (3) where, c∈ {C,ˆC}is the correct class for w,zN(w)is the vector of all neuron activations for word w, i.e. [z(0, w),z(1, w),...,z(n, w)], and Pθ(c|zN(w))is the probability that word wbelongs to the classc. The learned weights θserve as the ranking of neurons. Specifically, R(n,C)is defined as the absolute weights for the class i.e. |θC(n)|. L1 adds the “absolute value of magnitude” of the coefficient as a penalty term to the L(θ), which shrinks the less important weights to zero, leading to sparse weight distribution. Radford et al. (2019) used this regularization to force the classifier to learn spiky weights, indicating the selection of a few specialized neurons learning a concept, while setting the majority of neurons’ weight to zero. Such an assumption is useful in discovering focused neurons that learned one particular concept only. 2.2.2 Ridge Regularizer (L2) Lakretz et al. (2019) used L2 regularization to train the linear classifier. The loss is as follows: L(θ) =−X wlogPθ(c|zN(w)) +λ2∥θ∥2 2 (4) L2 forces weights to be close to zero (but not zero). It is useful to deal with (neurons that are highly correlated) scenarios, through constricting the coefficient while keeping all the features. Intuitively, this encourages grouping of features, thus discovering group neurons that jointly learn a concept. The score R(n,C)is computed in a similar fashion as the Lasso Regularizer method. 2.2.3 ElasticNet Regularizier (LCA) Both L1 and L2 capture properties that are desirable when selecting the most important neurons. L1 facilitates sparsity, identifying focused neurons while L2 encourages identifying groups of highly correlated features into account. Dalvi et al. (2019) used ElasticNet regularization (Zou & Hastie, 2005) that balances the trade-off between them. L(θ) =−X wlogPθ(c|zN(w)) +λ1∥θ∥1+λ2∥θ∥2 2 (5) λ1andλ2are that are tuned to optimize the effect of L1 and L2 regularization. 2.2.4 Gaussian Classifier Hennigen et al. (2020) assumes that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian, say Pover all neurons. Since multivariate Gaussian is decomposable by nature, they are able to extract individual probes for any subset of input features without additional work. Formally, they are able to extract PF, where F ⊂ N . The classifier itself is trained using the Bayesian framework, specifically the maximum a posteriori estimate to compute the parameters θ. Once a multivariate Gaussian is trained, the neuron selection is performed in a greedy fashion: F= (), F=F⊕arg max nP(C|F⊕ {n})foriin1..|N| R(n,C) =|N| − index (F, n) (6) In essence, probes for individual neurons are first extracted, and the neuron probe with the highest log- likelihood is considered the most important neuron. Next, probes are extracted for pairs of neurons, with one of them being the selected neuron. The pair with the highest log-likelihood then contributes the second most important neuron to the ranking. The full ranking is compared iteratively, with each step being a greedy selection of the next best neuron to add based on the log-likelihood. This method makes two fundamental assumptions: i) neuron activations follow a Gaussian distribution and ii) neuron ranking can be done in a greedy fashion. 4 3 Evaluation Methodology 3.1 Classifier Accuracy The commonly used evaluation metric i.e. classification accuracy of the selected neurons is inadequate: i) because it favors the neuron ranking methods that are based on the probing framework (Antverg & Belinkov, 2021), ii) it is not clear whether the classifier performance is a reflection of the knowledge learned in the discovered neurons or it is due to the capacity of the classifier to learn and memorize the task (Hewitt & Liang, 2019; Zhang & Bowman, 2018). We demonstrate these issues empirically, by using a linear classifier without regularization as an evaluation metric to evaluate various neuron interpretation methods. Given sdiscovered neurons with respect to a concept C, we train the classifier using these discovered neurons as features. The performance of the classifier serves as a measure of the correctness of s. Table 1 presents the average accuracy across all parts of speech (POS) concepts and across all layers of BERT for s= 30,50,70,100. Random refers to a random selection of neurons. The details of the dataset and the experimental setup are provided in Section 4. A few notable observations are: the classifier accuracy is higher for the probing methods (Lasso and Ridge) than the corpus-based methods (Probeless and IoU), ii) the classifier has the capacity to memorize as a Random selection of neurons performs reasonably well when more than a certain number of neurons are used in the evaluation. These results complement the issues raised earlier i.e. the classifier may result in high performance irrespective of the correctness of the neurons, and it favors the interpretation methods that are similar to itself. Table 1: Task: POS, Model: BERT, Accuracy scores using classifier accuracy as an evalua- tion metric. Bold and underline are the first and second best results. Neurons 30 50 70 100 Probeless 0.92 0.94 0.95 0.96 IoU 0.91 0.93 0.94 0.95 Lasso 0.93 0.95 0.95 0.96 Ridge 0.95 0.97 0.98 0.98 Random 0.81 0.86 0.89 0.92Table 2: Task: POS, Model: BERT, Av- erage compatibility scores AvgOverlap and NeuronVote when selecting the top 10, 30, 50 neurons from layers 1, 6, 12. Bold and underline are the first and second best scores. AvgOverlap NeuronVote Random 0.021 0.021 Gaussian 0.086 0.169 LCA 0.258 0.514 Lasso 0.240 0.473 Ridge 0.177 0.362 Probeless 0.269 0.532 IoU 0.156 0.365 3.2 Compatibility Metrics Given the absence of a good evaluation metric and gold annotations, we rely on voting theory. The consensus serves as a tool to measure the relation between rankings. In NLP, it has been widely used in information retrieval to combine search results of various algorithms (Lin et al., 2017; Yilmaz et al., 2008), in machine translation to compare different systems’ output using pair-wise ranking (Lapata, 2006) and to aggregate results of NLP tasks (Colombo et al., 2022). One way to compare rankings is to perform pairwise comparison using rank correlation or similarity functions such as Kendall’s τ(Kendall, 1938) and Spearman correlation (Daniel, 1990). These methods assume conjoint rankings where all rankings contain identical elements. In the case of neuron rankings, for every neuron interpretation method, we only consider the stop-ranked neurons with respect to a concept. The rationale behind this setting is to minimize the noise and randomness in rankings that may occur for neurons that are not learning the concept. This results in disjoint sets and the rank correlation methods can not be directly applied in such cases (Lin et al., 2017). Motivated by the voting theory, we devise two metrics to compare disjoint rankings. Our first metric, AvgOverlap , is based on plurality voting (Cooper & Zillante, 2012). AvgOverlap computes the pair-wise average neuron overlap across methods. It is a set-based method and it does not consider the internal ranking of neurons into consideration. For example, the first top neuron and fifth top neuron in a ranking will get equal weight when compared with another ranking. Our second method, NeuronVote , inspired by Borda Count (Lippman, 2012), considers the order of neurons into consideration in comparing rankings. We call our methods compatibility metrics, which when given 5 a neuron ranking, provide its compatibility score with respect to other methods. The metrics give a high score to a method that is most aligned with the rankings of the other neuron interpretation methods. To build a deeper understanding of how any two methods relate in terms of the resulting neurons, we also present a pair-wise comparison of the discovered neurons across methods. Formally, let Sm=n1, n2, ..., n srepresent the stop-ranked neurons for a method m, where sis a hyperparameter that can be adjusted to vary the number of top neurons selected for evaluation. We describe the compatibility metrics and the pair-wise comparison method as follows. 3.2.1 Average Overlap AvgOverlap scores an interpretation method based on its average neuron overlap with other methods. We define the overlap o(m1, m2)between two methods as the intersection over union of their respective top-neuron sets: o(m1, m2) =|Sm1∧Sm2| |Sm1∨Sm2|(7) AvgOverlap is defined as: AvgOverlapmi=1 M − 1MX j=1,j̸=io(mi, mj) (8) where Mis the set of all methods. A large value of AvgOverlap means that the ranking of that method is well aligned with the ranking of other neuron interpretation methods. 3.2.2 NeuronVote AvgOverlap treats each neuron equally in a set of discovered neurons and ignores the ranking assigned by the methods. However, intuitively a neuron interpretation method whose top choice is endorsed by all other methods should get a higher score than an interpretation method whose 10th top choice is endorsed by all other methods. To take this into account, we follow the Borda count strategy (Lippman, 2012) and propose NeuronVote . It considers each neuron’s ranking in the voting process. We first create Sbest, an aggregated ranking based on the position of individual neurons in the rankings of various methods. Sbest=argsort MX ms−index (Sm, n)forn∈ N! (9) where argsort is an ascending sort function and index gives the position of neuron nin the ranked list. The accumulated an aggregation on weighted votes, with neurons appearing at the top of various methods’ ranked lists getting a higher weight than others. Following the notation in AvgOverlap , we define NeuronVote mi=o(Smi, Sbest). The method with high NeuronVote implies that its ranking of neurons is most endorsed by other interpretation methods. 3.3 Pairwise Comparison The compatibility metrics provide a holistic evaluation of a neuron interpretation method with respect to other methods. We further extend the analysis to pair-wise comparison to understand how the discovered neurons of a method related to another method. The pair-wise comparison is calculated as an intersection between the output of two methods i.e. o(m1, m2) =Sm1∧Sm2 4 Evaluation 4.1 Settings We consider three 12-layered pre-trained models: (BERT, Devlin et al., 2019), (RoBERTa, Liu et al., 2019b) and (XLMR, Conneau et al., 2019). Each layer consists of 768 neurons. We consider concepts from three linguistic tasks: parts of speech tags (POS, Marcus et al., 1993), semantic tags (SEM, Abzianidze et al., 2017) and syntactic chunking (Chunking) using CoNLL 2000 shared task dataset (Tjong Kim Sang & Buchholz, 2000). Dataset details are provided in Appendix 7.2. We consider every tag as a concept and identify neurons with respect to the concept. In the case of methods, we use a binary classification setup where the contextualized representation of words belonging to the concept serves as positive 6 Table 3: Task: POS, Average NeuronVote compatibility scores across concepts when selecting the top 10, 30, and 50 neurons from layers 1, 6 and 12. Bold numbers, underline numbers, and dashed numbers show the first, second, and third best scores respectively BERT RoBERTa XLMR Layers 1 6 12 1 6 12 1 6 12 Random 0.019 0.021 0.023 0.020 0.019 0.017 0.020 0.017 0.023 Gaussian 0.147 0.177 0.183 0.140 0.179 0.179 0.118 0.135 0.153 LCA 0.501 0.544 0.391 0.550 0.495 0.380 0.450 0.531 0.373 Lasso 0.496 0.545 0.395 0.475 0.453 0.350 0.410 0.471 0.267 Ridge 0.332 0.405 0.360 0.452 0.491 0.510 0.358 0.446 0.455 Probeless 0.497 0.550 0.515 0.590 0.589 0.580 0.534 0.590 0.497 IoU 0.343 0.380 0.348 0.315 0.338 0.275 0.334 0.320 0.344 examples. We randomly select the negative class words from the rest of the data, equal in size to the positive class examples. We drop the concepts that have less than 200 examples to ensure stable training. We split the binary classification dataset into train/dev/test splits of 70/15/15 percent. We useλ1= 0.01andλ2= 0.01for methods. We present the results of POS only in the main paper and provide the rest in Appx. 7.3. Number of top neurons: We did not optimize the number of neurons sselected by each method as this would result in a varying number of neurons and will make the compatibility score incompatible across methods. We consider covers a diverse range to generalize the findings. Baseline: We generate a random ranking of neurons to compare the behavior of a neuron interpretation method with a random selection of neurons. We refer to it as Random . Procedure: We extract layer-wise contextualized of words in the dataset. For every layer representation, we discover the top neurons with respect to a concept using each neuron interpretation method. We follow a leave-one-out strategy to calculate the compatibility score of each method with respect to other methods. More specifically, we consider one neuron interpretation method as a test method and select the rest as the database of the sets of top neurons. We then calculate the AvgOverlap andNeuronVote scores of the test method. 4.2 Compatibility Scores Table 2 presents the average compatibility scores across all concepts and when selecting the top 10, 30, and 50 neurons from layers 1, 6, and 12. AvgOverlap andNeuronVote show a similar pattern: While the scale of the compatibility score is different between both metrics, they are consistent in highlighting the top interpretation methods i.e. Lasso, LCA, and Probeless. In the rest of the paper, we mainly present the results of NeuronVote , and discuss the AvgOverlap only when it is necessary to show a different trend. Overall comparison across methods: Table 3 presents the NeuronVote scores averaged over all POS concepts and when selecting the top 10, 30, and 50 neurons. The bold, underline, and dashed numbers represent the first, second, and third best compatibility scores respectively. Probeless consistently achieves the highest compatibility across all layers and models. In other words, the neurons selected by Probeless have the highest overlap with the neurons selected by other methods. We did not observe a similar trend for IoU which is also a corpus-based method. LCA and Lasso achieve the second-best compatibility score (see underlined and dashed numbers in Table 3). Ridge, although similar to LCA and Lasso, did not consistently show high compatibility across models. We discuss these methods later in Section 4.3. Gaussian achieved the worst compatibility score which is lower by 0.354 points from the best NeuronVote score on layer 1 and is only 0.128 better than Random for BERT. The result of Gaussian presents an interesting scenario where the compatibility score suggests that the neurons identified by Gaussian are least similar to all other methods. However, the difference with Random, although small, suggests that the selection of neurons is not random. The low results of Gaussian are inline with Antverg & Belinkov (2021) where they found that the Gaussian method memorizes the probing task and it may not provide the most faithful ranking of neurons. These scores, therefore, suggest that further investigation may be required to confirm the efficacy of the Gaussian method. 7 A notable point about the top 3 methods (Probeless, LCA, and Lasso) is their methodological diversity, despite of which they selected similar neurons to other methods. On the other hand, similar techniques such as IoU and Probeless did not result in similar compatibility scores. This shows that the compatibility metric is not biased by methodological similarity, which was one of the primary concerns with the commonly used evaluation metrics. Layer-wise trend: Ethayarajh (2019) showed that each layer is different in terms of representation geometry. Sajjad et al. (2022a) revealed that due to the geometry of the representation space, particularly in the last layers, knowledge of a concept may not be readily available. We hypothesize that the performance of certain neuron interpretation methods may vary based on the nature of the representation space. Our compatibility methods facilitate quantitative evidence to support this hypothesis by showing that certain neuron analysis methods suffer in selecting the most compatible neurons from the higher layers. LCA and Lasso both showed a substantial drop in their compatibility scores for the 12th layer (for example, Table 3: NeuronVote LCA – 0.544 for layer 6 and 0.391 for layer 12 for BERT). In contrast, Probeless did not show any substantial difference in the scores of the last layers and earlier layers ( NeuronVote : Probeless – 0.550 vs 0.515 for layer 6 and 12 respectively for BERT). This further support Probeless as the most reliable method and encourages further investigation into the robustness of Lasso and LCA with varying space. Compatibility across models: The overall trend of top-performing neuron interpretation methods is similar across BERT, RoBERTa, and XLMR. One notable difference is the compatibility score of Ridge for the last layer of the RoBERTa (0.510) and XLM-R (0.439) models which is substantially higher compared to the BERT model (0.360). Moreover, it achieved the second best score for XLM-R after Probeless and is substantially better than LCA and Lasso. Ridge prefers correlated features in contrast to Lasso, which prefers a few spiky features highly predictive of the concept. Based on the results, we hypothesize that the last layers of RoBERTa and XLM-R consist of highly correlated neurons and Ridge is effective in discovering salient neurons of correlated nature. LCA balances between spiky and correlated neurons, and results in higher scores than Lasso in these cases. (a) Layer 1 (b) Layer 6 (c) Layer 12 Figure 1: Task: POS, Model: BERT, Comparing average overlap of top 10-50 neurons Random selection of neurons: The compatibility score of Random serves as a baseline that provides a perspective of the correctness of a newly proposed neuron interpretation method. It gave substantially lower scores than all neuron interpretation methods (Table 3). Even though the Gaussian showed the lowest compatibility score among interpretation methods, the difference with Random adds a factor of confidence to its ranking. Moreover, a low compatibility score for Random also implies that each of the other methods are bringing in important neurons to some degree. 4.3 Pairwise Comparison The compatibility metrics provide a single score to understand an interpretation method in the context of a large set of other interpretation methods. We further extend the analysis to pair-wise comparison and provide insights into the relationship between different methods. Figure 1 presents the heatmaps comparing the average overlap of the top 10 – 50 neurons across methods using BERT. The light color in the heatmap shows higher matches while the dark color shows lower matches. We added Random – a random selection of neurons as a baseline. It showed an average overlap of less than 3% only (see the first row in heatmaps). Comparing Methods: LCA and Lasso showed a high overlap of up to 64% in the discovered neurons (see Figure 1(a), 1(b)). However, Ridge results in a lower overlap of at most 8 29% compared to other methods. Note that the Ridge regularization does not force the coefficients of neurons to zero, thus considers most of the neurons during the training phase. The resulting top neurons may represent correlated neurons capturing similar and redundant knowledge (Dalvi et al., 2020) and may not be a representative of the neurons with the most predictive power with respect to the understudied concept. Lasso prefers individual neurons most predictive of the concept and ElasticNet (LCA) provides a good balance between selecting individual neurons and correlated neurons highly predictive of the concept. Corpus-based vs. Methods: Probeless and IoU both directly rely on the activation value of a neuron. However, we did not observe a consistent overlap between their discovered neurons (at most 30% in Figure1(c)). In comparison to other classes of methods, Probeless showed a better and consistent overlap with LCA and Lasso. For example, in Figure 1(a) we observed an overlap of 66% between Probeless and LCA, and 55% between Probeless and Lasso. The relatively high overlap of Probeless with methods using Lasso points towards the selection of identical focused neurons by these methods despite their methodological differences. Outliers: Both Ridge and IoU show a smaller but consistent overlap with most of the methods. As discussed earlier, Ridge works well for correlated features which may not be highly predictive of the concept. IoU relies on activation values and it may select neurons that consistently activate higher than the threshold irrespective of the concept. Gaussian did not show substantial overlap with any methods or with any combination of methods. Further investigation is needed to evaluate its’ efficacy. 5 Related Work The area of interpreting deep learning models constitutes a broad expanse of research. This section provides a synthesized overview of diverse subareas within deep learning models applied to Natural Language Processing (NLP), while also outlining the scope of our study. Feature importance and attribution methods identifies the contribution of input features to predictions. These methodologies predominantly rely on the gradient of the output concerning the input feature and determine input feature importance by evaluating the magnitude of gradient values (Denil et al., 2014; Sundararajan et al., 2017; Danilevsky et al., 2020). Counterfactual Intervention revolves around an intricate analysis of the interplay between input features and predictions. This approach involves manipulating inputs and quantifying resulting output alterations. Diverse intervention strategies, including erasing input words and substituting words with different meanings have been used (Li et al., 2016b; Ribeiro et al., 2018). Attention Weights: Numerous investigations have been directed towards interpreting components of deep learning models at varying levels of granularity. For instance, attention weights have emerged as a viable metric to gauge the interrelation between input instances and model outputs (Martins & Astudillo, 2016; Vig, 2019). Along these lines, Geva et al. (2021) delved into the analysis of feedforward neural network components within the transformer model, revealing their functionality as key-value memories. Additionally, V oita et al. (2019) demonstrated that pruning many attention heads has minimal impact on performance. Mechanistic puts a focus on reverse engineering of network weights to comprehend their behavior (Nanda et al., 2023). Building upon the Distill Circuits thread, Elhage et al. (2021) investigated two-layered transformer models with attention blocks, identifying attention heads con- tributing to in-context learning. This understanding was further extended to larger language models by Olsson et al. (2022). To enhance neuron , Elhage et al. (2022) introduced a Softmax Linear unit as an activation function replacement. Wang et al. (2022a) attempted to bridge mechanistic findings in small networks to large ones, particularly GPT-2 small. Their approach involved iteratively tracing influential model components from predictions using causal intervention. They showcased the potential of mechanistic in understand- ing extensive models, while also highlighting associated challenges. Similarly, Bricken et al. (2023) uses a sparse autoencoder to disentangle polysemantic neurons. Representation Analysis involves probing network concerning predefined concepts, to quantify the extent of knowledge captured in these (Conneau et al., 2018; Liu et al., 2019a; Tenney et al., 2019; Durrani et al., 2019; Arps et al., 2022). This is often realized through 9 training diagnostic classifiers for linguistic concepts, wherein classifier accuracy serves as an indicator of concept knowledge within . See Belinkov & Glass (2019) for a comprehensive survey. Neuron Interpretation A more intricate form of representation analysis, termed neuron interpretation, delves into how knowledge is structured within the network (Sajjad et al., 2022b). This approach establishes connections between neurons and predefined concepts, offering insights into where and how specific concept knowledge is assimilated. Work done on neuron analysis can be broadly classified into three groups: Neuron visualization involves manual identification of patterns across a set of sentences (Li et al., 2016a; Karpathy et al., 2015). More recently Foote et al. (2023) proposed an automated approach to enhance of Large Language Models (LLMs) by extracting and visualizing individual neuron behaviors as interpretable graphs. Corpus-based Methods explore the role of a neuron through techniques such as ranking sentences in a corpus (Kádár et al., 2017b), generating synthetic sentences (Poerner et al., 2018) that maximize its activation, or computing neuron-level statistics over a corpus (Mu & Andreas, 2020; Suau et al., 2020; Antverg & Belinkov, 2022). Bills et al. (2023); Mousi et al. (2023) proposed the use of LLM to interpret neurons. Probing Methods identify salient neurons for a concept by training a classifier using neuron activations as features (Radford et al., 2019; Lakretz et al., 2019; Durrani et al., 2022) or fitting a multivariate Gaussian over all neurons and then extracting individual probes for single neurons (Hennigen et al., 2020). A number of works identify neurons with respect to the output class (Wang et al., 2022b; Dai et al., 2022). They are effective in finding neurons that play a role in the prediction. In this paper, we focus on the neuron interpretation methods that take a concept as input and find neurons with respect to the concept. We propose an evaluation framework to formalize the evaluation and comparison of results across methods. Moreover, we propose a novel method, MeanSelect and present a case study of using the evaluation framework. 6 Conclusion and Limitation We provided a thorough comparative analysis of neuron interpretation methods in NLP. We overcame the challenge of the lack of a standard evaluation metric and gold annotations by proposing an evaluation strategy consisting of two compatibility metrics and a pair-wise comparison. In addition to developing a capability to evaluate a new neuron analysis method, we presented various insights into existing neuron interpretation methods. For example, due to the correlated nature of last layer , they are most challenging for interpretation methods. The selection of top neurons overlaps substantially irrespective of the methodological differences among techniques. We made the evaluation framework available to the research community. Limitations: Our methodology relied on consensus and thus inherits the limitations of that paradigm. The consensus may mislead the evaluation under certain settings e.g. if the majority of the voters form a lobby then they can skew the results of the consensus. This scenario can happen if a set of methods always produce close to identical ranking, they will cause a decrease in the compatibility score of a new method that produces a ranking different from theirs. Another possible issue is if a new method discovers an entirely different set of neurons than the ones discovered by other methods. The compatibility score of such a method will be low. We intend to mitigate these issues by including a large number of diverse neuron interpretation methods that are theoretically different from each other. We believe that under these settings, the scenario of an entirely different set of discovered neurons is more theoretical than practical. We further present a pair-wise analysis that provides an analysis at a more granular level and bring insight into how a method compares with other methods. The pairwise comparison heatmap will highlight if a set of methods form a lobby. Another limitation of current neuron interpretation methods is that they do not explicitly target the discovery of neurons of diverse nature such as polysemantic, and superposition. Theoretically, only the ElasticNet regularization is capable of discovering neurons learning a singular function and multiple functions. The other methods such as Probeless are incapable of discovering multifunction neurons. None of the neuron interpretation methods explicitly identify superposition neurons. Explicit modeling of neurons of different nature in a neuron interpretation method may result in discovering novel sets of neurons. The evaluation methods including our proposed framework do not explicitly compare neurons of different types. 10 References Abzianidze, L., Bjerva, J., Evang, K., Haagsma, H., van Noord, R., Ludmann, P., Nguyen, D.-D., and Bos, J. The parallel meaning bank: Towards a multilingual corpus of translations annotated with compositional meaning . In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics , EACL ’17, pp. 242–247, Valencia, Spain, 2017. Antverg, O. and Belinkov, Y . On the pitfalls of analyzing individual neurons in language models. In International Conference on Learning , 2021. Antverg, O. and Belinkov, Y . On the pitfalls of analyzing individual neurons in language models. In International Conference on Learning , 2022. URL / . Antverg, O., Ben-David, E., and Belinkov, Y . Idani: Inference-time domain adaptation via neuron- level interventions, 2022. URL . Arps, D., Samih, Y ., Kallmeyer, L., and Sajjad, H. Probing for constituency structure in neural language models. In Findings of the Association for Computational Linguistics: EMNLP 2022 , pp. 6738–6757, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: . URL / . Bau, A., Belinkov, Y ., Sajjad, H., Durrani, N., Dalvi, F., and Glass, J. Identifying and control- ling important neurons in neural machine translation. In International Conference on Learning , 2019. URL . Belinkov, Y . and Glass, J. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics , 7:49–72, March 2019. doi: . URL . Belinkov, Y ., Durrani, N., Dalvi, F., Sajjad, H., and Glass, J. What do Neural Machine Translation Models Learn about Morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL) , Vancouver, July 2017. Association for Computational Lin- guistics. URL . Bills, S., Cammarata, N., Mossing, D., Tillman, H., Gao, L., Goh, G., Sutskever, I., Leike, J., Wu, J., and Saunders, W. Language models can explain neurons in language models. https: // , 2023. Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., Lasenby, R., Wu, Y ., Kravec, S., Schiefer, N., Maxwell, T., Joseph, N., Hatfield- Dodds, Z., Tamkin, A., Nguyen, K., McLean, B., Burke, J. E., Hume, T., Carter, S., Henighan, T., and Olah, C. Towards : Decomposing language models with dictionary learning. Transformer Circuits Thread , 2023. - . Colombo, P., Noiry, N., Irurozki, E., and CLEMENCON, S. What are the best systems? new perspectives on NLP benchmarking. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems , 2022. URL / . Conneau, A., Kruszewski, G., Lample, G., Barrault, L., and Baroni, M. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL) , July 2018. Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V ., Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V . Unsupervised cross-lingual representation learning at scale. CoRR , abs/1911.02116, 2019. URL . Cooper, D. and Zillante, A. A comparison of cumulative voting and generalized plurality voting. In Public Choice , 2012. doi: . 11 Dai, D., Dong, L., Hao, Y ., Sui, Z., Chang, B., and Wei, F. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 8493–8502, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: . URL . . Dalvi, F., Durrani, N., Sajjad, H., Belinkov, Y ., Bau, D. A., and Glass, J. What is one grain of sand in the desert? analyzing individual neurons in deep nlp models. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI, Oral presentation) , January 2019. Dalvi, F., Sajjad, H., Durrani, N., and Belinkov, Y . Analyzing redundancy in pretrained transformer models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP-2020) , Online, November 2020. Dalvi, F., Khan, A. R., Alam, F., Durrani, N., Xu, J., and Sajjad, H. Discovering latent concepts learned in BERT. In International Conference on Learning , 2022. URL https: // . Dalvi, F., Durrani, N., and Sajjad, H. Neurox library for neuron analysis of deep nlp models. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics: System Demonstrations , pp. 75–83, Toronto, Canada, July 2023. Association for Computational Linguistics. Daniel, W. Applied Nonparametric Statistics . Duxbury advanced series in statistics and decision sciences. PWS-KENT Pub., 1990. ISBN 9780534919764. URL / . Danilevsky, M., Qian, K., Aharonov, R., Katsis, Y ., Kawas, B., and Sen, P. A survey of the state of explainable AI for natural language processing. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pp. 447–459, Suzhou, China, December 2020. Association for Computational Linguistics. URL . aacl-main.46 . Denil, M., Demiraj, A., and de Freitas, N. Extraction of salient sentences from labelled documents. CoRR , abs/1412.6815, 2014. URL . Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers) , Minneapolis, Minnesota, 2019. Association for Computational Linguistics. Dhamdhere, K., Sundararajan, M., and Yan, Q. How important is a neuron? CoRR , abs/1805.12233, 2018. URL . Durrani, N., Dalvi, F., Sajjad, H., Belinkov, Y ., and Nakov, P. One size does not fit all: Comparing NMT of different granularities. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 1504–1516, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: . URL https: // . Durrani, N., Sajjad, H., Dalvi, F., and Belinkov, Y . Analyzing individual neurons in pre- trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 4865–4880, Online, November 2020. Associa- tion for Computational Linguistics. doi: . URL https: // . Durrani, N., Dalvi, F., and Sajjad, H. Linguistic correlation analysis: Discovering salient neurons in deepnlp models, 2022. URL . 12 Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y ., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. A mathematical framework for transformer circuits. Transformer Circuits Thread , 2021. . Elhage, N., Hume, T., Olsson, C., Nanda, N., Henighan, T., Johnston, S., ElShowk, S., Joseph, N., DasSarma, N., Mann, B., Hernandez, D., Askell, A., Ndousse, K., Drain, D., Chen, A., Bai, Y ., Ganguli, D., Lovitt, L., Hatfield-Dodds, Z., Kernion, J., Conerly, T., Kravec, S., Fort, S., Kadavath, S., Jacobson, J., Tran-Johnson, E., Kaplan, J., Clark, J., Brown, T., McCandlish, S., Amodei, D., and Olah, C. Softmax linear units. Transformer Circuits Thread , 2022. - . Ethayarajh, K. How contextual are contextualized word ? comparing the ge- ometry of BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Con- ference on Natural Language Processing (EMNLP-IJCNLP) , pp. 55–65, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: . URL . Foote, A., Nanda, N., Kran, E., Konstas, I., Cohen, S., and Barez, F. Neuron to graph: Interpreting language model neurons at scale, 2023. Geva, M., Schuster, R., Berant, J., and Levy, O. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 5484–5495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: . URL https: // . Hennigen, L. T., Williams, A., and Cotterell, R. Intrinsic probing through dimension selection. In Pro- ceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 197–216, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/ . URL . Hewitt, J. and Liang, P. Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 2733–2743, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: . URL . Hupkes, D., Veldhoen, S., and Zuidema, W. Visualisation and ’diagnostic classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure, 2018. Kádár, Á., Chrupała, G., and Alishahi, A. Representation of linguistic form and function in recurrent neural networks. Computational Linguistics , 43(4):761–780, December 2017a. doi: 10.1162/ COLI_a_00300. URL . Kádár, A., Chrupała, G., and Alishahi, A. Representation of linguistic form and function in recurrent neural networks. Computational Linguistics , 43(4):761–780, 2017b. Karpathy, A., Johnson, J., and Fei-Fei, L. Visualizing and understanding recurrent networks. arXiv preprint , 2015. Kendall, M. A new measure of rank correlation. Biometrika , 1938. Lakretz, Y ., Kruszewski, G., Desbordes, T., Hupkes, D., Dehaene, S., and Baroni, M. The emergence of number and syntax units in LSTM language models. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 11–20, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: . URL https: // . 13 Lapata, M. Automatic evaluation of information ordering: Kendall’s tau. Computational Linguistics , 32(4):471–484, 2006. doi: . URL / J06-4002 . Li, J., Chen, X., Hovy, E., and Jurafsky, D. Visualizing and understanding neural models in NLP. InProceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 681–691, San Diego, California, June 2016a. Association for Computational Linguistics. doi: . URL . Li, J., Monroe, W., and Jurafsky, D. Understanding neural networks through representation erasure. CoRR , abs/1612.08220, 2016b. URL . Lin, Z., Li, Y ., and Guo, X. Consensus of rankings. CoRR , abs/1704.08464, 2017. URL http: // . Lippman, D. Voting Theory, Math in Society . Pierce College Ft Steilacoom, 2012. Liu, N. F., Gardner, M., Belinkov, Y ., Peters, M. E., and Smith, N. A. Linguistic knowledge and of contextual . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 1073–1094, Minneapolis, Minnesota, June 2019a. Association for Computational Linguistics. URL / N19-1112 . Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V . Roberta: A robustly optimized BERT pretraining approach. CoRR , abs/1907.11692, 2019b. URL . Lundberg, S. M. and Lee, S.-I. A unified approach to interpreting model predictions. In Guyon, I., Luxburg, U. V ., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30 , pp. 4765–4774. Curran Associates, Inc., 2017. URL / . Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics , 19(2):313–330, 1993. URL https: // . Martins, A. and Astudillo, R. From softmax to sparsemax: A sparse model of attention and multi-label classification. In Balcan, M. F. and Weinberger, K. Q. (eds.), Proceedings of The 33rd International Conference on Machine Learning , volume 48 of Proceedings of Machine Learning Research , pp. 1614–1623, New York, New York, USA, 20–22 Jun 2016. PMLR. URL . Mousi, B., Durrani, N., and Dalvi, F. Can llms facilitate interpretation of pre-trained language models? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP) , Singapore, dec 2023. Association for Computational Linguistics. Mu, J. and Andreas, J. Compositional explanations of neurons. CoRR , abs/2006.14032, 2020. URL . Na, S., Choe, Y . J., Lee, D., and Kim, G. Discovery of natural language concepts in individual units of CNNs. CoRR , abs/1902.07249, 2019. URL . Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J. Progress measures for grokking via mechanistic . In The Eleventh International Conference on Learning Representa- tions , 2023. URL . O’Connor, J. and Robertson, E. The mactutor history of mathematics archive, 2003. 14 Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y ., Chen, A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. In-context learning and induction heads. Transformer Circuits Thread , 2022. . Patrão Neves, M. Consensus , pp. 19–29. Encyclopedia of Global Bioethics, Springer, 06 2016. doi: . Poerner, N., Roth, B., and Schütze, H. Interpretable textual neuron for NLP. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. 325–327, Brussels, Belgium, November 2018. Association for Computa- tional Linguistics. doi: . URL / W18-5437 . Prasanna, S., Rogers, A., and Rumshisky, A. When BERT Plays the Lottery, All Tickets Are Winning. InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 3208–3229, Online, November 2020. Association for Computational Linguistics. doi: . URL . 259. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. Ribeiro, M. T., Singh, S., and Guestrin, C. Anchors: High-precision model-agnostic explanations. InProceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence , . AAAI Press, 2018. ISBN . Sajjad, H., Alam, F., Dalvi, F., and Durrani, N. Effect of on contextualized word . In Proceedings of the 29th International Conference on Computational Linguis- tics, pp. 3127–3142, Gyeongju, Republic of Korea, October 2022a. International Committee on Computational Linguistics. URL . Sajjad, H., Durrani, N., and Dalvi, F. Neuron-level Interpretation of Deep NLP Models: A Survey. Transactions of the Association for Computational Linguistics , 2022b. Suau, X., Zappella, L., and Apostoloff, N. Finding experts in transformer models. CoRR , abs/2005.07647, 2020. URL . Sundararajan, M., Taly, A., and Yan, Q. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , ICML’17, pp. 3319–3328. JMLR.org, 2017. Tenney, I., Das, D., and Pavlick, E. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 4593–4601, Florence, Italy, July 2019. Association for Computational Linguistics. doi: . URL . Tjong Kim Sang, E. F. and Buchholz, S. Introduction to the CoNLL-2000 shared task chunking. InFourth Conference on Computational Natural Language Learning and the Second Learning Language in Logic Workshop , 2000. URL . Vig, J. Visualizing attention in language representation models. CoRR , abs/1904.02679, 2019. URL . V oita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 5797–5808, Florence, Italy, July 2019. Association for Computational Linguistics. doi: . URL . 15 Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. in the wild: a circuit for indirect object identification in gpt-2 small, 2022a. Wang, X., Wen, K., Zhang, Z., Hou, L., Liu, Z., and Li, J. Finding skill neurons in pre-trained language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp. 11132–11152, Abu Dhabi, United Arab Emirates, December 2022b. Association for Computational Linguistics. doi: . URL . Yilmaz, E., Aslam, J. A., and Robertson, S. A new rank correlation coefficient for information retrieval. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR ’08, pp. 587–594, New York, NY , USA, 2008. Association for Computing Machinery. ISBN 9781605581644. doi: . URL . Zhang, K. and Bowman, S. Language modeling teaches you more than translation does: Lessons learned through auxiliary syntactic task analysis. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. 359–361, Brussels, Bel- gium, November 2018. Association for Computational Linguistics. doi: . URL . Zou, H. and Hastie, T. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B , 67:301–320, 2005. 16 7 Appendix (a) BERT (b) RoBERTa (c) XLMR Figure 2: Task: POS, Pairwise comparison of MeanSelect with other methods using 10–50 neurons 7.1 Case Study Mean Select: We propose a new corpus-based method, MeanSelect , for neuron ranking as a case-study to illustrate how researchers can use our proposed methodology. Kádár et al. (2017b) generated explanations for neurons by extracting top-N 5-gram context for each neuron based on the magnitude of their activations, followed by human annotation. Na et al. (2019) removed the by extracting concepts of various granularities from a parsed tree and aligned highly activating neurons to the concepts. Inspired by these works, we propose a novel method, MeanSelect , that generates a ranking of neurons with respect to a concept. The intuition is that a neuron learning a concept will have consistently high activations across different contexts where that concept appears. However, there may be a neuron that always activates with high value irrespective of the concept. A difference in the mean value of the neuron activating the concept and all other concepts will provide the true behavior of the neuron for the concept. Following our notation in Section 2, the score of a given neuron nis defined as follows: R(n,C) =µ(C)−µ(ˆC) nmax−nmin(10) where µ(C)is the average, nmax is the max and nminis the min of activations z(n, w)where w∈ C, andµ(ˆC)is the average of activations over the random concept set. Compatibility Metrics Table 4 shows the compatibility score of MeanSelect using the repre- sentation of 3 different layers and compared it with the Random selection of neurons. The high compatibility scores show that MeanSelect discovers neurons that are endorsed by other neuron interpretation methods. This serves as a measure of confidence in the proposed method. Moreover, one may observe that the method has a relatively lower score for the last layer compared to other layers, giving insight into potential improvements that can be made to the behavior of this method. Pairwise Comparison The pairwise comparison of methods further provides insights into how the new method relates to other methods in terms of the resulting neurons. Figure 2 shows the heatmaps of three pre-trained models. MeanSelect has the highest overlap with the Probeless method and except Gaussian, it shows an overlap of at least 0.23 points with other methods. While the high overlap of MeanSelect with Probeless is not surprising given both are based on similar intuitions, the overlap with LCA and L1 shows that the method is selecting a diverse set of neurons captured by a variety of methods. Similar to the discussion on compatibility score, here we observe a substantial overlap drop in the last layer and this highlights the potential vulnerability of the method to certain . 7.2 Concept Datasets We consider concepts from three linguistic tasks: parts of speech tags (POS, Marcus et al., 1993), semantic tags (SEM, Abzianidze et al., 2017) and syntactic chunking (Chunking) using CoNLL 2000 shared task dataset (Tjong Kim Sang & Buchholz, 2000). For the POS dataset, we used 20 concepts 17 Table 4: Task: POS, Model: BERT, Average NeuronVote score of MeanSelect using 10–50 neurons BERT RoBERTa XLMR Layers 1 6 12 1 6 12 1 6 12 Random 0.019 0.021 0.023 0.020 0.019 0.017 0.019 0.017 0.022 MeanSelect 0.464 0.476 0.402 0.392 0.451 0.328 0.368 0.408 0.253 Table 5: Task: Semantic tagging, Average NeuronVote compatibility scores across Semantic tagging concepts when selecting the top 10, 30, and 50 neurons from layers 1, 6 and 12. Bold numbers, underline numbers, and dashed numbers show the first, second, and third best scores respectively BERT RoBERTa XLMR Layers 1 6 12 1 6 12 1 6 12 Random 0.018 0.013 0.017 0.011 0.026 0.017 0.026 0.014 0.026 Gaussian 0.257 0.256 0.222 0.237 0.282 0.245 0.176 0.256 0.195 LCA 0.474 0.541 0.385 0.488 0.493 0.347 0.309 0.455 0.367 Lasso 0.407 0.492 0.322 0.391 0.460 0.328 0.294 0.396 0.358 Ridge 0.316 0.343 0.361 0.468 0.492 0.575 0.372 0.430 0.473 Probeless 0.450 0.501 0.476 0.547 0.586 0.640 0.495 0.571 0.464 IoU 0.344 0.332 0.380 0.288 0.287 0.242 0.277 0.262 0.282 which have a total dataset size of 40137. These concepts include VBG (777), VBZ (908), NNPS (204), DT (4015), TO (1177), CD (1935), JJ (2836), PRP (801), MD (463), RB (1348), VBP (534), VB (1244), NNS (3021), VBN (1082), POS (433), IN (5039), NN (6660), CC (1220), NNP (4698), and VBD (1742). For the SEM dataset, we used three concepts which have a total dataset size of 120941. These concepts include IST (72240), NOW (24137) and EXS (24564). We used 10 concepts from Chunking which have a total dataset size of 220606. These concepts include B-ADJP (2493), B-ADVP (5081), B-NP (67285), B-PP (26005), B-VP (26078), I-ADJP (805), I-ADVP (532), I-NP (77368) I-PP (339) and I-VP (14620). For all the datasets used in the experiments, we use split 70%, 15% and 15%. 7.3 Results 7.3.1 Semantic Tagging Concepts For the SEM dataset, we sample 20000 sentences for experimental validation with split 70%, 15% and 15%. We select three tags: IST (intersective), NOW (present tense) and EXS (untensed simple event). Table 5 presents the average NeuronVote scores across three models. We observed identical trends to that of POS i.e. Probeless is the most consistent method, LCA and Lasso are second best methods but they suffer on the last layers. 7.4 Chunking Concepts Figures 3 and 4 show layer-wise results for the two voting methods proposed in the paper, across the three understudied models (BERT, RoBERTa and XLM-R). The results show that voting methods consistently rank the Probeless method as the most compatible in terms of neuron rankings across the layers. We are including detailed results in Tables 5–10 give detailed results with exact numbers. 18 Table 6: Task: Chunking, Average NeuronVote compatibility scores across Chunking concepts when selecting the top 10, 30, and 50 neurons from layers 1, 6 and 12. Bold numbers, underline numbers, and dashed numbers show the first, second, and third best scores respectively BERT RoBERTa XLMR Layers 1 6 12 1 6 12 1 6 12 Random 0.018 0.017 0.023 0.022 0.018 0.022 0.023 0.019 0.014 Gaussian 0.122 0.181 0.174 0.111 0.163 0.164 0.110 0.121 0.143 LCA 0.395 0.469 0.300 0.440 0.422 0.328 0.336 0.447 0.388 Lasso 0.396 0.472 0.301 0.399 0.395 0.322 0.366 0.425 0.395 Ridge 0.235 0.255 0.256 0.303 0.330 0.386 0.254 0.289 0.410 Probeless 0.465 0.506 0.422 0.502 0.500 0.514 0.499 0.507 0.463 IoU 0.346 0.361 0.321 0.285 0.298 0.244 0.319 0.283 0.352 (a) BERT (b) RoBERTa (c) XLMR Figure 3: AvgOverlap score across different Layers in different models. All methods perform better than the baseline, Random. Probeless is the most consistent method across all models, concepts and across all layers. It is among the top methods with LCA and Lasso. However, LCA and Lasso show low score on last layers. The performance of Gaussian deteriorates significantly and is closer to Random when applied on XLMR. (a) BERT (b) RoBERTa (c) XLMR Figure 4: NeuronVote score across different Layers in different models. All methods perform better than the baseline, Random. Probeless is the most consistent method across all models, concepts and across all layers. It is among the top methods with LCA and Lasso. However, LCA and Lasso show low score on last layers. 19 Table 7: This is an extension of Table.3. Average AvgOverlap compatibility scores across concepts when selecting the top 10, 30, and 50 neurons. Bold numbers show the best scores. Probeless, LCA are the top performing methods. However, Probeless is most consistent performing method. LCA drops substantially for the last layers. BERT Layers 0 1 2 3 4 5 6 7 8 9 10 11 12 Random 0.022 0.021 0.019 0.018 0.019 0.024 0.020 0.017 0.021 0.022 0.021 0.022 0.023 Gaussian 0.043 0.072 0.079 0.081 0.088 0.085 0.100 0.101 0.103 0.092 0.093 0.182 0.183 LCA 0.189 0.263 0.296 0.296 0.302 0.307 0.303 0.290 0.27 0.266 0.254 0.242 0.245 Lasso 0.171 0.246 0.271 0.270 0.287 0.289 0.289 0.269 0.245 0.247 0.24 0.253 0.243 Ridge 0.118 0.16 0.187 0.192 0.213 0.212 0.204 0.199 0.184 0.161 0.163 0.202 0.224 Probeless 0.195 0.262 0.292 0.293 0.3 0.305 0.303 0.29 0.273 0.265 0.258 0.325 0.346 IoU 0.099 0.145 0.167 0.168 0.178 0.175 0.175 0.167 0.157 0.165 0.161 0.162 0.183 Table 8: This is an extension of Table.3. Average NeuronVote compatibility scores across concepts when selecting the top 10, 30, and 50 neurons. BERT Layers 0 1 2 3 4 5 6 7 8 9 10 11 12 Random 0.022 0.019 0.018 0.016 0.019 0.025 0.021 0.018 0.014 0.022 0.026 0.020 0.023 Gaussian 0.097 0.147 0.154 0.151 0.160 0.164 0.177 0.186 0.184 0.195 0.180 0.174 0.183 LCA 0.454 0.501 0.577 0.573 0.593 0.581 0.544 0.574 0.572 0.529 0.530 0.512 0.391 Lasso 0.413 0.496 0.518 0.522 0.553 0.546 0.545 0.546 0.511 0.468 0.487 0.465 0.395 Ridge 0.277 0.332 0.364 0.384 0.405 0.392 0.405 0.382 0.379 0.361 0.313 0.338 0.360 Probeless 0.451 0.497 0.543 0.558 0.567 0.563 0.550 0.566 0.562 0.538 0.531 0.509 0.515 IoU 0.272 0.343 0.381 0.372 0.385 0.380 0.380 0.379 0.365 0.359 0.373 0.372 0.348 Table 9: This is an extension of Table.3. Average AvgOverlap compatibility scores across concepts when selecting the top 10, 30, and 50 neurons. RoBERTa Layers 0 1 2 3 4 5 6 7 8 9 10 11 12 Random 0.021 0.020 0.019 0.023 0.016 0.017 0.020 0.021 0.023 0.014 0.018 0.024 0.018 Gaussian 0.231 0.240 0.259 0.265 0.260 0.263 0.265 0.268 0.264 0.267 0.266 0.262 0.255 LCA 0.322 0.452 0.457 0.457 0.449 0.446 0.441 0.441 0.445 0.450 0.439 0.428 0.365 Lasso 0.314 0.425 0.438 0.430 0.428 0.427 0.425 0.423 0.425 0.433 0.419 0.418 0.355 Ridge 0.338 0.387 0.407 0.414 0.409 0.406 0.402 0.390 0.386 0.405 0.384 0.384 0.391 Probeless 0.388 0.462 0.474 0.474 0.469 0.467 0.461 0.457 0.457 0.460 0.449 0.445 0.424 IoU 0.306 0.318 0.319 0.324 0.324 0.320 0.321 0.324 0.319 0.313 0.305 0.299 0.282 Table 10: This is an extension of Table.3. Average NeuronVote compatibility scores across concepts when selecting the top 10, 30, and 50 neurons. RoBERTa Layers 0 1 2 3 4 5 6 7 8 9 10 11 12 Random 0.021 0.020 0.018 0.025 0.013 0.015 0.019 0.021 0.024 0.013 0.016 0.026 0.017 Gaussian 0.142 0.140 0.166 0.180 0.171 0.169 0.179 0.189 0.176 0.187 0.180 0.178 0.179 LCA 0.338 0.550 0.544 0.532 0.513 0.506 0.495 0.518 0.522 0.520 0.509 0.477 0.382 Lasso 0.308 0.475 0.482 0.463 0.468 0.451 0.453 0.467 0.469 0.478 0.455 0.452 0.358 Ridge 0.405 0.452 0.488 0.496 0.505 0.495 0.491 0.462 0.461 0.477 0.464 0.468 0.516 Probeless 0.524 0.590 0.606 0.597 0.583 0.599 0.589 0.580 0.597 0.587 0.575 0.563 0.580 IoU 0.377 0.315 0.314 0.329 0.333 0.335 0.338 0.335 0.333 0.317 0.304 0.301 0.275 Table 11: This is an extension of Table.3. Average AvgOverlap compatibility scores across concepts when selecting the top 10, 30, and 50 neurons. XLMR Layers 0 1 2 3 4 5 6 7 8 9 10 11 12 Random 0.020 0.021 0.022 0.021 0.018 0.018 0.019 0.019 0.019 0.020 0.021 0.022 0.023 Gaussian 0.185 0.186 0.188 0.183 0.185 0.184 0.181 0.181 0.180 0.181 0.186 0.181 0.183 LCA 0.264 0.377 0.428 0.437 0.439 0.442 0.438 0.435 0.438 0.433 0.431 0.408 0.245 Lasso 0.257 0.362 0.408 0.414 0.412 0.421 0.420 0.418 0.415 0.412 0.409 0.389 0.243 Ridge 0.292 0.312 0.327 0.343 0.357 0.355 0.361 0.367 0.370 0.356 0.353 0.338 0.324 Probeless 0.335 0.403 0.434 0.433 0.440 0.441 0.440 0.436 0.439 0.437 0.429 0.412 0.346 IoU 0.279 0.291 0.282 0.281 0.285 0.292 0.299 0.301 0.303 0.296 0.293 0.284 0.263 20 Table 12: This is an extension of Table.3. Average NeuronVote compatibility scores across concepts when selecting the top 10, 30, and 50 neurons. XLMR Layers 0 1 2 3 4 5 6 7 8 9 10 11 12 Random 0.022 0.020 0.022 0.022 0.019 0.017 0.017 0.019 0.021 0.017 0.019 0.022 0.023 Gaussian 0.051 0.118 0.044 0.038 0.037 0.037 0.135 0.033 0.032 0.032 0.041 0.033 0.153 LCA 0.262 0.450 0.542 0.562 0.554 0.558 0.531 0.523 0.547 0.547 0.528 0.485 0.373 Lasso 0.241 0.410 0.498 0.501 0.476 0.502 0.471 0.495 0.496 0.488 0.478 0.446 0.267 Ridge 0.353 0.358 0.348 0.391 0.427 0.411 0.446 0.447 0.436 0.432 0.407 0.411 0.455 Probeless 0.491 0.534 0.578 0.567 0.586 0.587 0.590 0.574 0.583 0.575 0.560 0.548 0.497 IoU 0.372 0.334 0.284 0.272 0.288 0.311 0.320 0.331 0.324 0.320 0.320 0.302 0.344 Table 13: This is an extension of Table.4. Average AvgOverlap score of MeanSelect when selecting 10, 30, and 50 neurons for all layers Layers 0 1 2 3 4 5 6 7 8 9 10 11 12 BERT Random 0.022 0.021 0.019 0.018 0.019 0.022 0.020 0.017 0.021 0.022 0.021 0.022 0.023 MeanSelect 0.266 0.351 0.363 0.362 0.365 0.369 0.374 0.370 0.356 0.349 0.343 0.327 0.283 RoBERTa Random 0.021 0.020 0.019 0.023 0.016 0.017 0.020 0.021 0.023 0.014 0.018 0.024 0.018 MeanSelect 0.252 0.294 0.307 0.317 0.326 0.328 0.323 0.320 0.314 0.308 0.283 0.269 0.239 XLMR Random 0.020 0.021 0.022 0.021 0.018 0.018 0.019 0.019 0.019 0.020 0.021 0.022 0.023 MeanSelect 0.233 0.246 0.245 0.247 0.264 0.273 0.294 0.292 0.281 0.261 0.259 0.241 0.159 Table 14: This is an extension of Table.4. Average NeuronVote score of MeanSelect when selecting 10, 30, and 50 neurons for all layers Layers 0 1 2 3 4 5 6 7 8 9 10 11 12 BERT Random 0.022 0.019 0.018 0.016 0.019 0.025 0.021 0.018 0.014 0.022 0.026 0.020 0.023 MeanSelect 0.361 0.476 0.462 0.458 0.470 0.468 0.479 0.488 0.486 0.463 0.457 0.432 0.400 RoBERTa Random 0.021 0.020 0.018 0.025 0.013 0.015 0.019 0.021 0.024 0.013 0.016 0.026 0.017 MeanSelect 0.358 0.388 0.395 0.403 0.426 0.423 0.455 0.438 0.437 0.406 0.383 0.374 0.326 XLMR Random 0.022 0.020 0.022 0.022 0.019 0.017 0.017 0.019 0.021 0.017 0.019 0.022 0.023 MeanSelect 0.376 0.357 0.356 0.342 0.364 0.392 0.414 0.416 0.400 0.372 0.381 0.360 0.243 Table 15: Comparison of NeuronV ote score of Probeless and LCA under different number of consensus methods. In majority of the cases, Probeless outperformed LCA irrespective of the methods used for consensus. Moreover, in three cases where LCA is better than Probeless, their results are comparable. Num. Methods Consensus Methods Probeless LCA 1 Gaussian 0.110 0.096 IoU 0.274 0.216 Lasso 0.449 0.516 Ridge 0.270 0.240 2 Gaussian,IoU 0.233 0.177 Gaussian,Ridge 0.202 0.174 Gaussian,Lasso 0.272 0.275 Lasso,IoU 0.351 0.329 Lasso,Ridge 0.404 0.410 Ridge,IoU 0.292 0.243 3 0.265 0.229 0.337 0.327 0.327 0.305 0.373 0.351 4 0.368 0.342 21 (a) Layer 1 (b) Layer 2 (c) Layer 3 (d) Layer 4 (e) Layer 5 (f) Layer 6 (g) Layer 7 (h) Layer 8 (i) Layer 9 (j) Layer 10 (k) Layer 11 (l) Layer 12 Figure 5: This is an extension of Figure.1. Comparing average overlap of top 10-50 neurons across methods for BERT 22 (a) Layer 1 (b) Layer 2 (c) Layer 3 (d) Layer 4 (e) Layer 5 (f) Layer 6 (g) Layer 7 (h) Layer 8 (i) Layer 9 (j) Layer 10 (k) Layer 11 (l) Layer 12 Figure 6: This is an extension of Figure.1. Comparing average overlap of top 10-50 neurons across methods for RoBERTa 23 (a) Layer 1 (b) Layer 2 (c) Layer 3 (d) Layer 4 (e) Layer 5 (f) Layer 6 (g) Layer 7 (h) Layer 8 (i) Layer 9 (j) Layer 10 (k) Layer 11 (l) Layer 12 Figure 7: This is an extension of Figure.1. Comparing average overlap of top 10-50 neurons across methods for XLMR 24 Table 16: Comparison of NeuronV ote score of Probeless and Lasso under different number of consensus methods. In all cases, Probeless shows better results irrespective of the methods used for consensus. Num of Consensus Methods Consensus Methods Probeless Lasso 1 Gaussian 0.110 0.092 IoU 0.274 0.195 LCA 0.524 0.516 Ridge 0.270 0.237 2 Gaussian,IoU 0.233 0.159 Gaussian,LCA 0.291 0.267 Gaussian,Ridge 0.202 0.170 LCA,IoU 0.357 0.300 LCA,Ridge 0.387 0.329 Ridge,IoU 0.292 0.227 3 0.346 0.289 0.356 0.321 0.265 0.214 LCA,Ridge,IoU 0.424 0.396 4 0.382 0.324 Table 17: Comparison of NeuronV ote score of Probeless and IoU under different number of consensus methods. In all cases, Probeless shows better results irrespective of the methods used for consensus. Num of Consensus Methods Consensus Methods Probeless IoU 1 Gaussian 0.110 0.085 IoU 0.449 0.195 Lasso-01 0.524 0.216 Ridge-01 0.270 0.160 2 Gaussian,IoU 0.272 0.155 0.291 0.158 0.202 0.136 Lasso-01,IoU 0.404 0.206 0.498 0.209 Ridge-01,IoU 0.424 0.217 3 0.337 0.194 0.356 0.200 0.475 0.224 0.434 0.206 4 0.426 0.221 25 
finding counterfactually optimal action sequences in continuous state spaces 	Finding Optimal Action Sequences in Continuous State Spaces Stratis Tsirtsis Max Planck Institute for Software Systems Kaiserslautern, Germany Max Planck Institute for Software Systems Kaiserslautern, Germany Abstract Whenever a clinician reﬂects on the efﬁcacy of a sequence of treatment decisions for a patient, they may try to identify critical time steps where, had they made different decisions, the patient’s health would have improved. While recent methods at the intersection of causal inference and reinforcement learning promise to aid human experts, as the clinician above, to analyze sequential decision making processes, they have focused on environments with ﬁnitely many discrete states. However, in many practical applications, the state of the environment is inherently continuous in nature. In this paper, we aim to ﬁll this gap. We start by formally characterizing a sequence of discrete actions and continuous states using ﬁnite horizon Markov decision processes and a broad class of bijective structural causal models. Building upon this , we formalize the problem of ﬁnding optimal action sequences and show that, in general, we cannot expect to solve it in polynomial time. Then, we develop a search method based on the A∗algorithm that, under a natural form of Lipschitz continuity of the environment’s dynamics, is guaranteed to return the optimal solution to the problem. Experiments on real clinical data show that our method is very efﬁcient in practice, and it has the potential to offer interesting insights for sequential decision making tasks. 1 Introduction Had the chess player moved the king one round later, would they have avoided losing the game? Had the physician administered antibiotics one day earlier, would the patient have recovered? The process of mentally simulating alternative worlds where events of the past play out differently than they did in reality is known as counterfactual reasoning [ 1]. Thoughts of this type are a common by-product of human decisions and they are tightly connected to the way we attribute causality and responsibility to events and others’ actions [ 2]. The last decade has seen a rapid development of reinforcement learning agents, presenting (close to) human-level performance in a variety of sequential decision making tasks, such as gaming [ 3,4], autonomous driving [ 5] and clinical decision support [ 6,7]. In conjunction with the substantial progress made in the ﬁeld of causal inference [ 8,9], this has led to a growing interest in machine learning methods that employ elements of counterfactual reasoning to improve or to analyze decisions in sequential settings [10–15]. In the context of reinforcement learning, sequential decision making is typically modeled using Markov Decision Processes (MDPs) [ 16]. Here, we consider MDPs with a ﬁnite horizon where each episode ( i.e., each sequence of decisions) consists of a ﬁnite number of time steps. As an example, consider a clinician treating a patient in an intensive care unit (ICU). At each time step, the clinician observes the current state of the environment ( e.g., the patient’s vital signs) and they choose among a set of potential actions ( e.g., standardized dosages of a drug). Consequently, the chosen action causes the environment to transition (stochastically) into a new state, and the clinician earns a reward ( e.g., 37th Conference on Neural Information Processing Systems (NeurIPS 2023). satisfaction inversely proportional to the patient’s severity). The process repeats until the horizon is met and the goal of the clinician is to maximize the total reward. In this work, our goal is to aid the retrospective analysis of individual episodes as the example above. For each episode, we aim to ﬁnd an action sequence that differs slightly from the one taken in reality but, under the circumstances of that particular episode, would have led to a higher counterfactual reward. In our example above, assume that the patient’s condition does not improve after a certain period of time. A optimal action sequence could highlight to the clinician a small set of time steps in the treatment process where, had they administered different drug dosages, the patient’s severity would have been lower. In turn, a manual inspection of those time steps could provide insights to the clinician about potential ways to improve their treatment policy. To infer how a particular episode would have evolved under a different action sequence than the one taken in reality, one needs to represent the stochastic state transitions of the environment using a structural causal model (SCM) [ 8,17]. This has been a key aspect of a line of work at the intersection of counterfactual reasoning and reinforcement learning, which has focused on methods to either design better policies using ofﬂine data [ 10,12] or to analyze individual episodes [ 11,13]. Therein, the work most closely related to ours is by Tsirtsis et al. [ 13], which introduces a method to compute optimal action sequences in MDPs with discrete states and actions using a Gumbel-Max SCM to model the environment dynamics [ 11]. However, in many practical applications, such as in critical care, the state of the environment is inherently continuous in nature [ 18]. In our work, we aim to ﬁll this gap by designing a method to compute optimal action sequences in MDPs with continuous states and discrete actions. Refer to Appendix A for a discussion of further related work and to Pearl [ 8] for an overview of the broad ﬁeld of causality. Our contributions. We start by formally characterizing sequential decision making processes with continuous states and discrete actions using ﬁnite horizon MDPs and a general class of bijective SCMs [ 19]. Notably, this class of SCMs includes multiple models introduced in the causal discovery literature [20–25]. Building on this , we make the following contributions: (i)We formalize the problem of ﬁnding a optimal action sequence for a particular episode in environments with continuous states under the constraint that it differs from the observed action sequence in at most kactions. (ii)We show that the above problem is NP-hard using a novel reduction from the classic parti- tion problem [ 26]. This is in contrast with the computational complexity of the problem in environments with discrete states, which allows for polynomial time algorithms [13]. (iii) We develop a search method based on the A∗algorithm that, under a natural form of Lipschitz continuity of the environment’s dynamics, is guaranteed to return the optimal solution to the problem upon termination. Finally, we evaluate the performance and the qualitative insights of our method by performing a series of experiments using real patient data from critical care.1 2 A causal model of sequential decision making processes At each time step t∈[T−1] :={0,1,...,T−1}, whereTis a time horizon, the decision making process is characterized by a D-dimensional vector state st∈S=RD, an actionat∈A, where Ais a ﬁnite set of Nactions, and a reward with each pair of states and actions. Moreover, given an episode of the decision making process, τ={(st,at)}T−1 t=0, the process’s outcomeo(τ) =∑ tR(st,at)is given by the sum of the rewards. In the remainder, we will denote the elements of a vector Further, we characterize the dynamics of the decision making process using the framework of structural causal models (SCMs). In general, an SCM is consisted of four parts: (i) a set of endogenous variables (ii) a set of exogenous (noise) variables (iii) a set of structural equations assigning values to the endogenous variables, and (iv) a set of prior distributions characterizing the exogenous variables [ 8]. In our setting, the endogenous variables of the SCM Care the random variables representing the states S0,...,ST−1and the actions A0,...,AT−1. The action Atat time 1Our code is accessible at . 2Table 1 in Appendix B summarizes the notation used throughout the paper. 2 steptis chosen based on the observed state Stand is given by a structural (policy) equation At:=gA(St,Zt), (1) where Zt∈Z is a vector-valued noise variable, to allow some level of stochasticity in the choice of the action, and its prior distribution PC(Zt)is characterized by a density function fC Zt. Similarly, the stateSt+1in the next time step is given by a structural (transition) equation ), (2) where Ut∈U is a vector-valued noise variable with its prior distribution PC(Ut)having a density functionfC Ut, and we refer to the function gSas the transition mechanism . Note that, in Eq. 2, the noise t=0are mutually independent and, keeping the sequence of actions ﬁxed, they are the only source of stochasticity in the dynamics of the environment. In other words, a sampled sequence of noise values {ut}T−1 t=0and a ﬁxed sequence of actions {at}T−1 t=0result into a single (deterministic) sequence of states {st}T−1 t=0. This implicitly assumes that the state transitions are stationary and there are no unobserved confounders. Figure 4 in Appendix B depicts the causal graph Gcorresponding to the SCM Cdeﬁned above. The above representation of sequential decision making using an SCM Cis a more general refor- mulation of a Markov decision process, where a (stochastic) policy π(a|s)is entailed by Eq. 1, and the transition distribution ( i.e., the conditional distribution of St+1|St,At) is entailed by Eq. 2. Speciﬁcally, the conditional density function of St+1|St,Atis given by ) =) =∫ Ut(u)du,(3) a (hard) intervention on the variable At, whose value is set to at.3Here, the ﬁrst equality holds because St+1andAtare d-separated by Stin the sub-graph obtained from G after removing all outgoing edges of At4and the second equality follows from Eq. 2. Moreover, as argued elsewhere [ 11,13], by using an SCM to represent sequential decision making, instead of a classic MDP, we can answer counterfactual questions. More speciﬁcally, assume that, at time step t, we observed the state St=st, we took action At=atand the next state was St+1=st+1. , we would like to know the probability that the state St+1would have beens′if, at time step t, we had been in a state s, and we had taken an action a, (generally) different fromst,at. Using the SCMC, we can characterize this by a counterfactual transition density function ) =∫ Ut(u)du,(4) Utis the posterior distribution of the noise variable Utwith support such that ). In what follows, we will assume that the transition mechanism gSis continuous with respect to its last argument and the SCM Csatisﬁes the following form of : Deﬁnition 1. An SCMCis iff the transition mechanism gSand the reward R are with respect to their ﬁrst argument, i.e., for eacha∈A,u∈U, there exists a Lipschitz constant Ka,u∈R+such that, for any )∥≤ Ka,u∥s−s′∥, and, for each a∈A, there exists a Lipschitz constant Ca∈R+such that, for any ′∥. In both the Euclidean distance. Note that, although they are not phrased in causal terms, similar Lipschitz continuity assumptions for the environment dynamics are common in prior work analyzing the theoretical guarantees of reinforcement learning algorithms [ 27–35]. Moreover, for practical applications ( e.g., in healthcare), this is a relatively mild assumption to make. Consider two patients whose vitals sands′are similar 3In general, the dooperator also allows for soft interventions ( i.e., setting a probability distribution for At). 4This follows directly from the rules of do-calculus. For further details, refer to Chapter 3of Pearl [8]. 3 at a certain point in time, they receive the same treatment a, and every unobserved factor uthat may affect their health is also the same. Intuitively, Deﬁnition 1 implies that their vitals will also evolve similarly in the immediate future, i.e., the not differ dramatically. In this context, it is worth mentioning that, when the transition mechanism gSis modeled by a neural network, it is possible to control its Lipschitz constant during training, and penalizing high values can be seen as a regularization method [36, 37]. Further, we will focus on bijective SCMs [ 19], a fairly broad class of SCMs, which subsumes multiple models studied in the causal discovery literature, such as additive noise models [ 20], post-nonlinear causal models [ 21], location-scale noise models [ 22] and more complex models with neural network components [23–25]. Deﬁnition 2. An SCMCis bijective iff the transition mechanism gSis bijective with respect to its last argument, i.e., there is a well-deﬁned inverse function g−1 S:S×A×S→U such that, for every combination of ), it holds that ut=g−1 S(st,at,st+1). Importantly, bijective SCMs allow for a more concise of the counterfactual transition density given in Eq. 4. More speciﬁcally, after observing an event , the value utof the noise variable Utcan only be such that ut=g−1 ., the posterior distribution of Utis a point mass and its density is given by Ut(u) =1[u=g−1 S(st,at,st+1)], (5) where 1[·]denotes the indicator function. Then, for a given episode τof the decision making process, we have that the (non-stationary) counterfactual transition density is given by ) :=) =∫ =1[ s′=gS( s,a,g−1 S(st,at,st+1))] . (6) Since this density is also a point mass, the resulting counterfactual dynamics are purely deterministic. That means, under a bijective SCM , the answer to the question “ What would have been the state at timet+ 1, had we been at state sand taken action aat timet, given that, in reality, we were at st, we tookatand the environment transitioned to st+1?” is just given by s′=gS( s,a,g−1 S(st,at,st+1)) . On the counterfactual identiﬁability of bijective SCMs. Very recently, Nasr-Esfahany and Kici- man [ 38] have shown that bijective SCMs are in general not identiﬁable when the exogenous variable Utis . In other words, even with access to an inﬁnite amount of triplets ( from the true SCM C, it is always possible to ﬁnd an SCM M̸=C with transition mechanism hSand distributions PM(Ut)that entails the same transition distributions asC(i.e., it ﬁts the observational data perfectly), but leads to different counterfactual predictions. Although our subsequent algorithmic results do not require the SCM Cto be identiﬁ- able, the subclass of bijective SCMs we will use in our experiments in Section 5 is identiﬁable. The deﬁning attribute of this subclass, which we refer to as element-wise bijective SCMs , is that the transition mechanism gScan be decoupled into Dindependent mechanisms gS,isuch that }. This implies St+1,i⊥ ⊥, not need to be independent. Informally, we have the following identiﬁability result (refer to Appendix C for a formal version of the theorem along with its proof, which follows a similar reasoning to proofs found in related work [12, 19]): Theorem 3 (Informal) .LetCandMbe two element-wise bijective SCMs such that their entailed transition distributions for St+1given any value of St,Atare always identical. Then, all their counterfactual predictions based on an observed transition ( also be identical. On the assumption of no unobserved confounding. The assumption that there are no hidden confounders is a frequent assumption made by work at the intersection of counterfactual reasoning and reinforcement learning [ 10–13] and, more broadly, in the causal inference literature [ 39–43]. That said, there is growing interest in developing off-policy methods for partially observable MPDs (POMDPs) that are robust to certain types of confounding [ 44–46], and in learning dynamic treatment regimes in sequential settings with non-Markovian structure [ 47,48]. Moreover, there is a line of work focusing on the identiﬁcation of counterfactual quantities in non-sequential confounded environments [ 49–51]. In that context, we consider the computation of (approximately) optimal counterfactual action sequences under confounding as a very interesting direction for future work. 4 3 Problem statement Letτbe an observed episode of a decision making process whose dynamics are characterized by a bijective SCM. To characterize the counterfactual outcome that any alternative action sequence would have achieved under the circumstances of the particular episode, we build upon the formulation of Section 2, and we deﬁne a non-stationary counterfactual MDP M+= (S+,A,F+ τ,t,R+,T)with deterministic transitions. Here, S+=S×[T−1]is an enhanced state space such that each s+∈S+is a pair (s,l)indicating that the counterfactual episode would have been at state s∈S withlaction changes already performed. Accordingly, R+is a reward function which takes the form R+((s,l),a) =R(s,a)for all (., it does not change depending on the number of action changes already performed. Finally, the time-dependent transition function F+ τ,t:S+×A→S+is deﬁned as F+ τ,t((s,l),a) ={( gS( s,a,g−1 S(st,at,st+1)) ,l+ 1) if(a̸=at)( gS( s,at,g−1 S(st,at,st+1)) ,l) otherwise.(7) Intuitively, here we set the transition function according to the point mass of the counterfactual transition density given in Eq. 6, and we use the second coordinate to keep track of the changes that have been performed in comparison to the observed action sequence up to the time step t. Now, given the initial state s0of the episode τand any counterfactual action sequence {a′ t}T−1 t=0, we can compute the corresponding counterfactual episode τ′={(s′ t,lt),a′ t}T−1 t=0. Its sequence of states is given recursively by (s′ 1,l1) =F+ τ,0((s0,0),a′ 0)and( s′ t+1,lt+1) =F+ τ,0((s′ t,lt),a′ ) and its counterfactual outcome is given by o+(τ′) :=∑ tR+((s′ t,lt),a′ t) =∑ tR(s′ t,a′ t). Then, similarly as in Tsirtsis et al. [ 13], our ultimate goal is to ﬁnd the counterfactual action sequence {a′ t}T−1 t=0that, starting from the observed initial state s0, maximizes the counterfactual outcome sub- ject to a constraint on the number of counterfactual actions that can differ from the observed ones, i.e., maximize a′ 0,...,a′ T−1o+(τ′) subject to s′ 0=s0andT−1∑ t=01[at̸=a′ t]≤k, (9) the observed actions. Unfortunately, using a reduction from the classic partition problem [ 26], the following theorem shows that we cannot hope to ﬁnd the optimal action sequence in polynomial time:5 Theorem 4. The problem deﬁned by Eq. 9. is NP-Hard. The proof of the theorem relies on a reduction from the partition problem [ 26], which is known to be NP-complete, to our problem, deﬁned in Eq. 9. At a high-level, we map any instance of the partition problem to an instance of our problem, taking special care to construct a reward function and an observed action sequence, such that the optimal counterfactual outcome o+(τ∗)takes a speciﬁc value if and only if there exists a valid partition for the original instance. The hardness result of Theorem 4 motivates our subsequent focus on the design of a method that always ﬁnds the optimal solution to our problem at the expense of a potentially higher runtime for some problem instances. 4 Finding the optimal counterfactual action sequence via A* search To deal with the increased computational complexity of the problem, we develop an optimal search method based on the classic A∗algorithm [ 52], which we have found to be very efﬁcient in practice. Our starting point is the observation that, the problem of Eq. 9 presents an optimal substructure, i.e., its optimal solution can be constructed by combining optimal solutions to smaller sub-problems. For an observed episode τ, letVτ(s,l,t)be the maximum counterfactual reward that could have been achieved in a counterfactual episode where, at time t, the process is at a (counterfactual) state s, and there are so far lactions that have been different in comparison with the observed action sequence. Formally, Vτ(s,l,t) = max a′ t,...,a′ T−1T−1∑ t′=tR(s′ t′,a′ t′) subject to s′ t=sandT−1∑ t′=t1[at′̸=a′ t′]≤k−l. 5The supporting proofs of all Theorems, Lemmas and Propositions can be found in Appendix D. 5 (a) Search graph (b) Heuristic function computation Figure 1: Main components of our search method based on the A* algorithm. Panel (a) shows the search graph for a problem instance with |A|= 2. Here, each box represents a node v= (s,l,t)of the graph, and each edge represents a counterfactual transition. Next to each edge, we include the actiona∈A causing the transition and the associated reward. Panel (b) shows the heuristic function computation, where the two axes represent a (continuous) state space S=R2and the two levels on the z-axis correspond to differences in the (integer) values (l,t)and(la,t+ 1) . Here, the blue squares correspond to the ﬁnite states in the anchor set S†and(sa,la) =F+ τ,t((s,l),a). Then, it is easy to see that the quantity Vτ(s,l,t)can be given by the recursive function Vτ(s,l,t) = max a∈A{R(s,a) +Vτ(sa,la,t+ 1)},for all s∈S, l<k andt<T−1,(10) where (sa,la) =F+ τ,t((s,l),a). In the base case of l=k(i.e., all allowed action changes are already performed), we have Vτ(s,k,t) =R(s,at) +Vτ(sat,lat,t+ 1) for all s∈Sandt<T−1, and Vτ(s,k,T−1) =. Lastly, when t=T−1andl < k , we have Vτ(s,l,T−1) = max a∈AR(s,a)for all s∈S. Given the optimal substructure of the problem, one may be tempted to employ a typical dynamic programming approach to compute the values Vτ(s,l,t)in a bottom-up fashion. However, the complexity of the problem lies in the fact that, the states sare real-valued vectors whose exact values depend on the entire action sequence that led to them. Hence, to enumerate all the possible values thatsmight take, one has to enumerate all possible action sequences in the search space, which is equivalent to solving our problem with a brute force search. In what follows, we present our proposed method to ﬁnd optimal solutions using the A∗algorithm, with the caveat that its runtime varies depending on the problem instance, and it can be equal to that of a brute force search in the worst case. Casting the problem as graph search. We represent the solution space of our problem as a graph, where each node vcorresponds to a tuple (]. Every node v= (s,l,t)withl < k andt < edges, each one associated with an actiona∈A , carrying a reward R(s,a), and leading to a node va= (sa,la,t+ 1) such that (sa,la) =F+ τ,t((s,l),a). In the case of l=k, the nodevhas exactly one edge corresponding to the observed action atat timet. Lastly, when t=T−1, the outgoing edge(s) lead(s) to a common node vT= (s∅,k,T )which we call the goal node , and it has zero outgoing edges itself. Note that, the exact value of s∅is irrelevant, and we only include it for notational completeness. Lets0be the initial state of the observed episode. Then, it is easy to notice that, starting from the root nodev0= (s0,0,0), the ﬁrst elements of each node vion a a sequence of counterfactual states, and the edges that connect those nodes are such that the corresponding counterfactual action sequence differs from the observed one in at most kactions. That said, the counterfactual outcome o+(τ) =∑T−1 t=0R(s′ t,a′ t)is expressed as the sum of the rewards associated with each edge in the path, and the problem deﬁned by Eq. 9 is equivalent to ﬁnding the path of maximum total reward that starts from v0and ends invT. Figure 1a illustrates the search graph for a simple instance of our problem. Unfortunately, since the states sare vectors of real values, even enumerating all the graph’s nodes requires time exponential in the number of actions |A|, which makes classic algorithms that search over the entire graph non-practical. To address this challenge, we resort to the A∗algorithm, which performs a more efﬁcient search over the graph by preferentially exploring only parts of it where we have prior information that they are 6 Algorithm 1: It computes upper bounds ˆVτ(s,l,t)for the values Vτ(s,l,t) Input : StatesS, actionsA, observed action sequence {at}t=T−1 t=0 , horizon T, transition function F+ τ,t, reward function R, constraint k, anchor setS†. Initialize :ˆVτ(s, l, T−1)←maxa∈AR(s, a), s∈S†, l= 0, . . . , k−1. ˆVτ(s, k, T−1)←R(s, at), s∈S†. fort=T−2, . . . , 0do forl=k, . . . , 0do available fors∈S†do bounds←∅ fora∈available _actions do sa, la←F+ τ,t((s, l), a); /* Get the min bound for Vτ(sa, la, t+ 1) Va←min s†∈S†{ˆVτ(s†, la, t+ 1) + Lt+1∥s†−sa∥}; based onS†*/ , a) +Va} end ˆVτ(s, l, t)←max(bounds ); /* Get the max bound over the actions */ end end end return ˆVτ(s, l, t)for all s∈S†, l∈[k], t∈[T−1] more likely to lead to paths of higher total reward. Concretely, the algorithm proceeds iteratively and maintains a queue of nodes to visit, initialized to contain only the root node v0. Then, at each step, it selects one node from the queue, and it retrieves all its children nodes in the graph which are subsequently added to the queue. It terminates when the node being visited is the goal node vT. Refer to Algorithm 2 in Appendix E for a pseudocode implementation of the A∗algorithm. The key element of the A∗algorithm is the criterion based on which it selects which node from the queue to visit next. Let vi= (si,li,t)be a candidate node in the queue and rvibe the total reward of the path that the algorithm has followed so far to reach from v0tovi. Then, theA∗algorithm visits next the node vithat maximizes the sum ), where ˆVτis aheuristic function that aims to estimate the maximum reward that can be achieved via any path starting from vi= (si,li,t) and ending in the goal node vT,i.e., it gives an estimate for the quantity Vτ(si,li,t). Intuitively, the heuristic function can be thought of as an “eye into the future” of the graph search, that guides the algorithm towards nodes that are more likely to lead to the optimal solution and the algorithm’s performance depends on the quality of the approximation of ). Next, we will look for a heuristic function that satisﬁes consistency6to guarantee that the A∗algorithm as described above returns the optimal solution upon termination [52]. Computing a consistent heuristic function. We ﬁrst propose an algorithm that computes the function’s values ˆVτ(s,l,t)for a ﬁnite set of points such that , where S†is a pre-deﬁned ﬁnite set of states—an anchor set —whose construction we discuss later. Then, based on the of the SCM C, we show that these computed values of ˆVτare valid upper bounds of the corresponding values Vτ(s,l,t)and we expand the deﬁnition of the heuristic function ˆVτover all s∈S by expressing it in terms of those upper bounds. Finally, we prove that the function resulting from the aforementioned procedure is consistent. To compute the upper bounds ˆVτ, we exploit the observation that the values a form of , as stated in the following Lemma. Lemma 5. Letut=g−1 = maxa∈AKa,ut,C= maxa∈ACaand the sequence such that ]. Then, it holds ′∥, for . Based on this observation, our algorithm proceeds in a bottom-up fashion and computes valid upper bounds of the values Vτ(s,l,t)for the anchor setS†. To get the 6A heuristic function ˆVτis consistent iff, for nodes v= (s, l, t),va= (sa, la, t+ 1) connected with an edge associated with action a, it satisﬁes ˆVτ(s, l, t)≥R(s, a) +ˆVτ(sa, la, t+ 1) [53]. 7 intuition, assume that, for a given t, the values ˆVτ(s,l,t+ 1) are already computed for all s∈S†, l∈[k], and they are indeed valid upper bounds of the corresponding Vτ(s,l,t+ 1) . Then, let (sa,la) =F+ some s∈S†andl∈[k]. Since saitself may not belong to the ﬁnite anchor setS†, the algorithm uses the values ˆVτ(s†,la,t+ 1) of all anchors s†∈S†in combination with their distance to sa, and it sets the value of ˆVτ(s,l,t)in way that it is also guaranteed to be a (maximally tight) upper bound of Vτ(s,l,t). Figure 1b illustrates the above operation. Algorithm 1 summarizes the overall procedure, which is guaranteed to return upper bounds, as shown by the following proposition: Proposition 6. For all ], it holds that ), where ˆVτ(s,l,t)are the values of the heuristic function computed by Algorithm 1. Next, we use the values by Algorithm 1 to expand the deﬁnition of ˆVτover the entire domain as follows. For some s∈S,a∈A, let(sa,la) =F+ τ,t((s,l),a), then, we have that ˆVτ(s,l,t) =  0 t=T max a∈A′R(s,a) t=T−1 max a∈A′{ R(s,a) + min s†∈S†{ ˆVτ(s†,la,t+ 1) +Lt+1∥s†−sa∥}} otherwise,(11) < k . Finally, the following theorem shows that the resulting heuristic function ˆVτis consistent: Theorem 7. For any nodes v= (s,l,t),va= (sa,la,t+ 1) witht < T−1connected with an edge associated with action a, it holds that ) +ˆVτ(sa,la,t+ 1) . Moreover, for any nodev= (s,l,T−1)and edge connecting it to the goal node vT= (s∅,k,T ), it holds that ) +ˆVτ(s∅,k,T ). Kick-starting the heuristic function computation with Monte Carlo anchor sets. For any s̸∈ S†, whenever we compute Eq. 11, the resulting value is set based on the value ˆVτ(s†,la,t+ 1) of some anchor s†, increased by a penalty termLt+1∥s†−sa∥. Intuitively, this allows us to think of the heuristic function ˆVτas an upper bound of the function Vτwhose looseness depends on the magnitude of the penalty terms encountered during the execution of Algorithm 1 and each subsequent evaluation of Eq. 11. To speed up the A∗algorithm, note that, ideally, one would want all penalty terms to be zero, i.e., an anchor set that includes all the states sof the nodes v= (s,l,t)that are going to appear in the search graph. However, as discussed in the beginning of Sec. 4, an enumeration of those states requires a runtime exponential in the number of actions. To address this issue, we introduce a Monte Carlo simulation technique that adds to the anchor set the observed states { all unique states {s′ 0,...,s′ T−1} resulting by Mrandomly sampled counterfactual action sequences a′ 0,...,a′ T−1. Speciﬁcally, for each action sequence, we ﬁrst sample a number k′of actions to be changed and what those actions are going to be, both uniformly at random from {1,...,k}andAk′, respectively. Then, we sample from {0,...,T−1} thek′time steps where the changes take place, with each time step thaving a probability Lt/∑ t′Lt′ to be selected. This biases the sampling towards earlier time steps, where the penalty terms are larger due to the higher Lipschitz constants. As we will see in the next section, this approach works well in practice, and it allows us to control the runtime of the A∗algorithm by appropriately adjusting the number of samples M. We experiment with additional anchor set selection strategies in Appendix F. 5 Experiments using clinical sepsis management data Experimental setup. To evaluate our method, we use real patient data from MIMIC-III [ 54], a freely accessible critical care dataset commonly used in reinforcement learning for healthcare [ 6,55–57]. We follow the preprocessing steps of Komorowski et al. [ 6] to identify a cohort of 20,926patients treated for sepsis [ 58]. Each patient record contains vital signs and administered treatment information in time steps of 4-hour intervals. As an additional preprocessing step, we discard patient records whose associated time horizon Tis shorter than 10, resulting in a ﬁnal dataset of 15,992patients with horizons between 10and20. 8 0.8 0.9 1.0 1.1 1.2 1.3 Lipschitz constant, Lh1.52.02.5EBF 0204060 A∗average runtime (s) (a) Efﬁciency vs. Lh 500 1000 1500 2000 2500 3000 # of Monte Carlo samples, 1020 A∗average runtime (s) (b) Efﬁciency vs. M 0123456 # of action changes, k123EBF 0200400600 A∗average runtime (s) (c) Efﬁciency vs. k Figure 2: Computational efﬁciency of our method under different conﬁgurations, as measured by the effective branching factor (pink-left axis) and the runtime of the A∗algorithm (green-right axis). In Panel (a), we set M= 2000 andk= 3. In Panel (b), we set Lh= 1.0andk= 3. In Panel (c), we setLh= 1.0andM= 2000 . In all panels, we set Lφ= 0.1and error bars indicate 95% conﬁdence intervals over 200executions of the A∗algorithm for 200patients with horizon T= 12 . To form our state space S=RD, we useD= 13 features. Four of these features are demographic or contextual and thus we always set their counterfactual values to the observed ones. The remaining ˜D= 9features are time-varying and include the SOFA score [ 59]—a standardized score of organ failure rate—along with eight vital signs that are required for its calculation. Since SOFA scores positively correlate with patient mortality [ 60], we assume that each s∈S gives a reward R(s)equal to the negation of its SOFA value. Here, it is easy to see that this reward function is just a projection ofs, therefore, it is Lipschitz continuous with constant Ca= 1 for alla∈A. Following related work [ 6,55,57], we consider an action space Athat consists of 25actions, which correspond to 5×5levels of administered vasopressors and intravenous ﬂuids. Refer to Appendix G for additional details on the features and actions. To model the transition dynamics of the time-varying features, we consider an SCM Cwhose transition mechanism takes a location-scale form gS(St,At,Ut) =h(St,At) +φ(St,At)⊙Ut, whereh,φ:S×A→ R˜D, and⊙denotes the element-wise multiplication [ 22,24]. Notably, this model is element-wise bijective and hence it is identiﬁable, as shown in Section 2. Moreover, we use neural networks to model the location and scale functions handφand enforce their Lipschitz constants to be LhandLφ, respectively. This results in a Lipschitz continuous SCM Cwith |. Further, we assume that the noise variable Utfollows a multivariate Gaussian distribution with zero mean and allow its covariance matrix to be a (trainable) parameter. We jointly train the weights of the networks handφand the covariance matrix of the noise prior on the observed patient transitions using stochastic gradient descent with the negative log-likelihood of each transition as a loss. In our experiments, if not speciﬁed otherwise, we use an SCM with Lipschitz constants Lh= 1.0,Lφ= 0.1that achieves a log-likelihood only 6%lower to that of the best model trained without any Lipschitz constraint. Refer to Appendix G for additional details on the network architectures, the training procedure and the way we enforce Lipschitz continuity.7 Results. We start by evaluating the computational efﬁciency of our method against (i) the Lipschitz constant of the location network Lh, (ii) the number of Monte Carlo samples Mused to generate the anchor setS†, and (iii) the number of actions kthat can differ from the observed ones. We measure efﬁciency using running time and the effective branching factor (EBF) [ 52]. The EBF is deﬁned as a real number b≥1such that the number of nodes expanded by A∗is equal to 1 +b+b2+···+bT, whereTis the horizon, and values close to 1indicate that the heuristic function is the most efﬁcient in guiding the search. Figure 2 summarizes the results, which show that our method maintains overall a fairly low running time that decreases with the number of Monte Carlo samples Mused for the generation of the anchor set and increases with the Lipschitz constant Lhand the number of action changesk. That may not come as a surprise since, as Lhincreases, the heuristic function becomes more loose, and as kincreases, the size of the search space increases exponentially. To put things in perspective, for a problem instance with Lh= 1.0,k= 3 and horizon T= 12 , theA∗search led by our heuristic function is effectively equivalent to an exhaustive search over a full tree with 7All experiments were performed using an internal cluster of machines equipped with 16 Intel(R) Xeon(R) 3.20GHz CPU cores, 512GBs of memory and 2 NVIDIA A40 48GB GPUs. 9 0 1 2 3 4 5 6 # of action changes, c/f improvement (a) Average c/f improvement vs. k 0% 10% 20% 30% 40% Counterfactual of patients20% 30%050100 !(b) Distribution of c/f improvement (c) Observed vs. c/f episode Figure 3: Retrospective analysis of patients’ episodes. Panel (a) shows the average counterfactual improvement as a function of kfor a set of 200patients with horizon T= 12 , where error bars indicate 95% conﬁdence intervals. Panel (b) shows the distribution of counterfactual improvement across all patients fork= 3, where the dashed vertical line indicates the median. Panel (c) shows the observed (solid) and counterfactual (dashed) SOFA score across time for a patient who presents a 19.9% counterfactual improvement when k= 3. Upward (downward) arrows indicate action changes that suggest a higher (lower) dosage of vasopressors (V) and ﬂuids (F). In all panels, we set M= 2000 . while the corresponding search space of our problem consists of more than 3 million action sequences—more than 3million paths to reach from the root node to the goal node. Next, we investigate to what extent the counterfactual action sequences generated by our method would have led the patients in our dataset to better outcomes. For each patient, we measure their counterfactual relative decrease in cumulative SOFA score between the counter- factual and the observed episode. Figures 3a and 3b summarize the results, which show that: (i) the average counterfactual improvement shows a diminishing increase as kincreases; (ii) the median counterfactual improvement is only 5%, indicating that, the treatment choices made by the clinicians for most of the patients were close to optimal, even with the beneﬁt of hindsight; and (iii) there are 176patients for whom our method suggests that a different sequence of actions would have led to an outcome that is at least 15% better. That said, we view patients at the tail of the distribution as “interesting cases” that should be deferred to domain experts for closer inspection, and we present one such example in Fig. 3c. In this example, our method suggests that, had the patient received an early higher dosage of intravenous ﬂuids while some of the later administered ﬂuids where replaced by vasopressors, their SOFA score would have been lower across time. Although we present this case as purely anecdotal, the counterfactual episode is plausible, since there are indications of decreased mortality when intravenous ﬂuids are administered at the early stages of a septic shock [61]. 6 Conclusions In this paper, we have introduced the problem of ﬁnding optimal action sequences in sequential decision making processes with continuous state dynamics. We showed that the problem is NP-hard and, to tackle it, we introduced a search method based on the A∗algorithm that is guaranteed to ﬁnd the optimal solution, with the caveat that its runtime can vary depending on the problem instance. Lastly, using real clinical data, we have found that our method is very efﬁcient in practice, and it has the potential to offer interesting insights to domain experts by highlighting episodes and time-steps of interest for further inspection. Our work opens up many interesting avenues for future work. For example, it would be interesting to develop algorithms with approximation guarantees that run in polynomial time, at the expense of not achieving strict counterfactual optimality. Moreover, since the practicality of methods like ours relies on the assumption that the SCM describing the environment is accurate, it would be interesting to develop methods to learn SCMs that align with human domain knowledge. Finally, it would be interesting to validate our method using real datasets from other applications and carry out user studies in which the counterfactual action sequences found by our method are systematically evaluated by the human experts ( e.g., clinicians) who took the observed actions. . Tsirtsis and acknowledge support from the European Re- search Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 945719). 10  
knowledge augmented reasoning distillation for small language models in knowledge intensive tasks 	 Reasoning Distillation for Small Language Models in Tasks Minki Kang1,2∗, Seanie Lee2, Jinheon Baek2, Kenji Kawaguchi3, Sung Ju Hwang2,4 University of {zzxc1133, lsnfamily02, , , Abstract Large Language Models (LLMs) have shown promising performance in knowledge- intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challeng- ing due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small Language Models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for reasoning tasks due to the lim- ited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge- Augmented Reasoning Distillation ( KARD ), a novel method that fine-tunes small LMs to generate rationales obtained from LLMs with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantly improves the performance of small T5 and GPT models on the challenging reasoning datasets, namely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the 250M T5 models achieve superior performance against the fine-tuned 3B models, having 12 times larger parameters, on both MedQA-USMLE and StrategyQA benchmarks. 1 Introduction Large Language Models (LLMs) [ 5,8] have excelled at various tasks across diverse domains with in-context learning. Recently, scaling up the number of parameters of LLMs has been shown to significantly improve their knowledge encoding and reasoning capability [ 54,24]. Moreover, such LLMs have achieved remarkable performance on tasks in professional domains which are highly challenging, since they require a considerable depth of domain knowledge and reasoning [ 34,48]. For example, in Figure 1 top, answering a medical question requires both domain knowledge and reasoning ability. The LLM should understand that the patient likely has ALS based on the symptoms and recognize SOD1 is the main cause of motor neuron diseases. Furthermore, it needs to reason over the knowledge that a mutation in SOD1 is highly associated with the symptoms. Despite its effectiveness, deploying LLMs can still be challenging, especially in real-world applica- tions. Firstly, utilizing LLMs to make predictions is expensive. It requires 326GB GPU memory to load the GPT3-175B model [ 11]. Moreover, deployment of the LLM potentially poses a risk of privacy leakage since most of the LLMs [ 5,44,8,43] operate in a black-box manner. That is, users cannot access the parameters of LLMs but only their output via some Application Programming Interfaces. Consequently, the need for white-box Small Language Models tailored to address problems requiring knowledge will continue to gain ∗Work done at AITRICS.†Code is available at . 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Question: A 57-year-old man presents with a 2-month history of right upper and lower extremity weakness. (...) His right upper extremity shows forearm atrophy and depressed reflexes while his right lower extremity is hypertonic with a positive Babinski sign. Which of the following is most likely associated with the cause of this patient’s symptoms? A) HLA-B8 haplotype B) HLA-DR2 haplotype C) Mutation in SOD1 D) Viral : The symptoms suggest motor neuron disease and among the options, mutation in SOD1 is the most common cause of inherited motorneuron diseases. A viral infection can cause peripheral neuropathy, but the symptoms described suggest a central nervous system Distillation RationaleLarge LM?Small RationaleSmall with in the first SOD enzyme (SOD1) can cause familial amyotrophic lateral sclerosis (ALS, a form of motor neuron disease). (...)Irrelevant with Question)[HLA A1-B8 haplotype] A1-B8 serotype was associated with a number of diseases as "HL-A"' antigens were first being described. (...) LMRetriever Set Reranked SetLarge 1: Concept. An example of a reasoning task (medical QA [ 23]) on the top. On the bottom, we provide the conceptual illustration of our KARD, compared to existing reasoning distillation. On the right, we provide examples of passages retrieved with rationale and question from the external KB. prominence. To tackle the above challenges of deploying models, previous works [ 33,17,38,12,18] have proposed to transfer the reasoning ability of large models to small models through reasoning distillation (See Figure 1 left). In particular, they leverage the LLM to generate high-quality rationales and fine-tune a small LM to generate the rationale obtained from the LLM. This reasoning distilla- tion improves the performance of small LMs on tasks that require complex reasoning ability (e.g., arithmetic and symbolic reasoning [ 10,55]). Based on this observation, we pose a research question: “Is it possible to transfer both the domain knowledge and reasoning ability of LLMs through reasoning distillation, for tasks requiring specific knowledge to reason for answering a question?” Existing reasoning distillation is suboptimal to solve such reasoning tasks since small, distilled LMs are limited in their capacity to memorize the knowledge that is necessary to solve the tasks due to the small number of parameters. This motivates us to develop a method that distills the reasoning ability of LLMs into smaller LMs while injecting the specific task-relevant knowledge. Specifically, we augment a small LM with the knowledge retrieved from an external Knowledge Base (KB) as a non-parametric memory, and we theoretically show that the non-parametric memory can reduce the number of bits to memorize training data for performing well. Based on this intuition and the theoretical analysis, we propose Reasoning Distillation (KARD) which enables to transfer the reasoning ability of an LLM to a small LM while injecting the knowledge, for reasoning tasks. Specifically, we utilize a retriever [ 46] to obtain passages containing relevant knowledge for generating a rationale from an external knowledge base (e.g., Wikipedia). We then fine-tune the small LM to generate the rationale, obtained from the LLM, based on the question and the retrieved document, and predict the answer. During training, using a rationale as a query helps retrieve pertinent knowledge for generating rationales. However, during inference, relying on the question as a query may result in poor retrieval. As shown in Figure 1, the passage retrieved with the question is not relevant to generating the rationale. To mitigate the issue, we introduce a neural reranker to prioritize passages useful for rationale generation, ensuring a retrieval of relevant documents even with the question as the query. To verify the efficacy of KARD, we empirically show that it significantly improves the performance of small LMs (OPT [ 20,59] and T5 [ 45,53]) on medical Question Answering (QA) (MedQA- USMLE [ 23]), multi-step factual QA (StrategyQA [ 14]), and commonsense reasoining (Open- bookQA [ 39]) datasets compared to few-shot in-context learning, fine-tuning, and reasoning distilla- tion without knowledge augmentation. Also, our extensive analyses demonstrate that our KARD is efficient in terms of both the training data and the model size. Specifically, KARD with 250M models achieves higher accuracy than the fine-tuned 3B models, and KARD outperforms the fine-tuning only with a quarter of the full training data in 780M models. Our findings and contributions are as follows: •We demonstrate that fine-tuning small LMs to generate rationales from large LMs is insufficient for reasoning tasks and a non-parametric external knowledge base plays a crucial role in complementing the lack of knowledge in small LMs. •Moreover, we address the limitations of the existing retriever method by introducing a reranker, in order to obtain pertinent passages for generating rationales in reasoning tasks. 2 •In widely-used medical, multi-step factual, and commonsense QA benchmark datasets, we empiri- cally show that the proposed KARD significantly improves the performance of small LMs. 2 Related Works Large Language Models Large Language Models (LLMs) have shown impressive capabilities across various tasks. One of their notable strengths is their ability to memorize knowledge and leverage that knowledge to solve reasoning tasks. For example, LLMs like GPT-3.5 [44], Med-PaLM [48],ChatGPT [29], and GPT-4 [43] have shown the promising performance on the challenging medical question answering task, the United States Medical Licensing Examination (USMLE) [ 23], even surpassing the passing score by a large margin [ 41]. However, deploying LLMs in offline and environments is still challenging since most of these models are in black-box (accessible via APIs), and expensive. Thus, we need alternative solutions that can leverage the capabilities of LLMs for reasoning tasks. Reasoning Distillation from LLMs Recent works [ 33,17,38,12,18] have attempted to distill the reasoning ability of LLMs into small LMs, where the reasoning ability is an emergent property which enables LLMs to perform better in reasoning tasks through (CoT) prompting (e.g., Let’s think step-by-step ) [28,55]. Unlike arithmetic or symbolic reasoning tasks, however, previous works [ 33,17] have shown that reasoning distillation is less effective for reasoning tasks [ 14] where factual knowledge is important to generate accurate rationale. Therefore, we augment small LMs with documents retrieved from the external knowledge base so that the models can leverage knowledge to generate better rationales that lead to correct answers. LMs LMs have utilized an external Knowledge Base (KB) to supplement their intrinsic knowledge [ 16,32,3,22,58]. One common approach to incorporate external knowledge is by retrieving relevant passages from a KB, such as Wikipedia, based on the input query [ 7]. Retrieving the correct evidence is crucial to generate accurate answers and factually grounded rationales. However, previous works usually have not explored the use of LMs for tasks that require complex reasoning over knowledge. Recently, BehnamGhader et al. [1]examined the reasoning ability of existing LMs and found that the existing retriever [ 25] is insufficient for retrieving relevant passages to solve the reasoning tasks. To address this limitation, we propose a re-ranker for rationale generation that prioritizes passages relevant to the rationale generated by LLMs given the query. This approach can be seen as a form of knowledge distillation for the retriever, as we use the rationale to guide the reranker to retrieve more relevant passages for reasoning, instead of using plain queries. 3 Motivation: Effect of on Memorization Large language models are known to memorize its training data [ 6,49] and the memorization capacity is proven to increase as the size of the model increases [ 27,57]. The previous work [ 4] showed that the memorization of training data is indeed necessary to perform well in a language problem. These results suggest that the reasoning distillation with a small language model (without knowledge augmentation) will degrade the performance because of (1) the incapability of memorizing training data and (2) the necessity of the memorization to perform well. In this section, we demonstrate that using an external Knowledge Base (KB) as a non-parametric memory with a retriever reduces the amount of the memorization needed to perform well and thus allows us to use small models. 3.1 Background without We adopt the exact same problem setting used in Brown et al. [4]. A task distribution P∼qis drawn from q. Given a P, the training dataset X= ((Zi, Yi))n i=1and the test sample (Z, Y)are drawn as X∼P⊗nand(Z, Y)∼P. Here, Zis the input (i.e., the sequence of symbols) andYis the label (i.e., the next symbol to be predicted). The overall error of a learning algorithm A on the qwith sample size nis defined by errq,n(A) = Pr P∼q,X∼P⊗n, ( M=A(X)). Given qandn, there exists an optimal learner AOPT that minimizes this overall error, which will be used as our reference. We adopt the abstracted language problem, i.e., the next-symbol prediction problem with Nreference strings {cj}N j=1where cj∼Uniform  {0,1}d , considered in the main 3 text of Brown et al. [4]with no symbol corruption (see [ 4] orAppendix A.1 for the details). Under this setting, Brown et al. [4]proved that any algorithm Aneeds to memorize the ndbits of training data to achieve where Idenotes the mutual information: Theorem 1 (Brown et al. [4]).LetN=n. Then, any learning algorithm Athat satisfies errq,n(A)≤ errq,n(AOPT) +ϵforϵ=o(1)also satisfies I(X;A(X)|P) = Ω( nd). 3.2 Memorization with In Theorem 1, dcorresponds to the size of KB. Thus, it shows that if the size of KB is small, then a small model can just memorize all KB by memorizing to perform well. However, if the size of KB is large, then a small model cannot memorize and hence the performance is expected to drop significantly when replacing a large model with a small model. In this subsection, we show that reduces the memorization requirement of Ω(nd)bits to that of , allowing the use of small models. We consider an inference algorithm φthat uses a KB with a non-parametric retriever as follows: errφ q,n(A) = Pr P∼q,X∼P⊗n, (Z,Y)∼P(φ(Z, M, S )̸=Ywhere M=A(X)). An inference algorithm φhas no learnable parameters and makes prediction based on both the result of learning algorithm M=A(X)and a KB denoted by S, which is defined as follows. Given a task instance P∼q, we choose a KB such that |S|=N+Rand{cj}N j=1⊆Swhere Ris the number of extra references that are irreverent to this task P; i.e., R= 0in the best scenario. Theorem 2 shows that the reduces the amount of memorization to achieve , from the ndtomin(N, n)mbits, under the same problem setting as Theorem 1: Theorem 2. There exists a pair of inference and learning algorithms (φ,A)such that for any ϵ >0, errφ ) +ϵandI(X;A(X)|P) =O(min( N, n)m)where m= log2((1− (N−1 ) 2ϵ). With n=Nandϵ=o(1), we have I(X;A(X)|P) =O(min( N, n)m) = Appendix A.2 for proof). Thus, it shows that allows the reduction from the ndbits to nlog2(N+R)bits for the amount of memorization needed to perform well. 4 Reasoning Distillation We propose Reasoning Distillation ( KARD ), which consists of two learning processes: (1) reasoning distillation where we leverage Large Language Models (LLMs) to generate a rationale with black-box APIs and then fine-tune small models to generate both rationale and answer given a question and knowledge, in which the knowledge is retrieved from Knowledge Base (KB) with the rationale as a query; (2) reranker training to retrieve relevant passages for the question as a query at the inference time, for generating effective rationales. Our approach is illustrated in Figure 2. 4.1 Teach Small Models to Generate Rationales with External Knowledge Rationale Generation with LLMs In our problem setup, we assume that training dataset ((xi,yi))n i=1for the target task is given, where xiis input sequence (question in QA) and yiis label (answer in QA). Additionally, there are LLMs accessible through black-box APIs [ 5,8,44,43,42]. In other words, the parameters and the architecture of the LLM are unknown and we can only access text sequences generated by the LLM. Since the ability to generate high-quality rationale is known as the emergent ability of LLMs [ 55,28], we want to transfer such ability to a small language model with reasoning distillation. Firstly, we leverage the prompting [28] to elicit the proper l rationales for each training data point with LLMs: all i∈[n]:={1, . . . , n } andj∈[l], where ris the generated rationale and pis the prompt [55, 28, 48]. Fine-tuning Small Models on Rationales Then we fine-tune a small language model pθwith trainable parameters θto generate both rationale rijobtained from the LLM and answer yi, given the question xi. In other words, we minimize the negative log-likelihood of the sequence of rationale rijand the answer yiwhere the rationale must be generated first prior to the answer generation: Ldistill (θ) =−1 n·lnX i=1lX ). (1) 4 Small LMRetriever InferenceLarge LMFrozen?Small Question Training ! KBCandidate Set W& Score External Set RerankerTrain Dataset Figure 2: Overview of KARD. (Left, § 4.1) Illustration of training (top) and inference (bottom) of knowledge- augmented reasoning distillation, where, during training, the small LM learns to generate rationales given the training data and the retrieved knowledge by the rationale. (Right, § 4.2) Illustration of reranker training (top) and inference (bottom). Reranker learns to prioritize the passage which has knowledge relevant to the rationale. Intuitively, the rationale provides a deeper and more comprehensive understanding of the reasoning behind the answer associated with the question, which better guides the small model to correctly answer the question [ 18]. Although previous works [ 33,17,38,12,18] have also leveraged the ratio- nales generated by LLMs to make small models excel at diverse reasoning tasks, generating rationales for tasks with a small LM requires additional care. As previously described in Section 3, the reasoning distillation with a small model but without knowledge augmentation may degrade the quality of the rationale generation due to the incapability of memorizing training data with the small model [ 57,27] and the necessity of the memorization for better performance in language tasks [ 4]. Therefore, the rationale generation should be evidenced by extrinsic knowledge from external memory to enhance the capability of the small LM for generating a high-quality rationale. Integrating External Knowledge Base Motivated by Theorem 2, we propose to retrieve a passage from an external Knowledge Base (KB) which is a corpus of over millions of documents D= {d1, . . . ,dK}to support memorization capacity of the small LM. Note that the acquisition of the relevant document from KB is crucial for training the small LM to generate high-quality rationale which leads to correct answers for given questions. As done in open-domain QA task [ 7], we retrieve a set of relevant passages for a given query with the sparse retriever BM25 [ 46]. In order to obtain the document which is the most relevant to the rationale rijgenerated by the LLM, we utilize the rationale as a query to retrieve a set of passages ), k)⊂ D, where ρdenotes a retriever scoring the document d∈ D based on relevance to the query rijandtopk yields the k passages with the top- khighest relevance scores. Finally, we utilize the retrieved documents ˆDijfor fine-tuning the small LM to generate the rationale rijand answer yifor the question xias follows: Ldistill-KB (θ) =−1 n·lnX i=1lX ), (2) where the rationale and answer are sequentially generated as we did in Equation 1. 4.2 Training Neural Reranker for Rationale Generation The remaining issue is that we cannot use the rationale as a query at the inference time. As an alternative, we can use the question xiinstead of the rationale rijas a query to retrieve a set of passages with the retriever. However, there is no guarantee that the top- kpassages retrieved by the input xias a query contain relevant information to generate correct rationales. In detail, based on the question as a query, the retriever can obtain a set of passages that contain relevant documents for generating rationales with a sufficiently large kbutk≪K. However, the target documents we want for rationale generation may be assigned with low rankings and thus they may not be chosen for knowledge augmentation at the inference time. To remedy this issue, we propose to leverage a neural reranker fϕ[26] with parameter ϕto re-rank the set of passages retrieved by the retriever ρso that we can acquire more relevant documents for generating rationale at the inference time. In order to train the neural reranker, we might manually construct a ground truth passage for each question. However, we assume a realistic setting where the ground truth passage for reranker training 5 is not given. Instead, we train the reranker to imitate how the retriever scores the passage d∈ D with the rationale rijas a query. Specifically, we first utilize the retriever ρto obtain a set of passages from Dwith the rationale rijas a query as follows: ˜), ), κ2) where κ1andκ2are the number of candidate documents (Figure 2 is the case where κ2= 0). Then, we normalize the score ρ(d|rij;D)of the document from ˜Dij, denoted as Q(d|rij). Similarly, we use the reranker fϕto score each document in ˜Dijwith the given question xiand normalize the score denoted as Pϕ(d|xi). We use softmax for normalization with τ1, τ2>0as follows: Q(d|rij) =exp ( d′∈˜Dijexp (ρ(d′|rij;D)/τ1), Pϕ(d|x) = ), where d∈˜Dij. Finally, we minimize the KL divergence between ): Lrerank (ϕ) =1 n·lnX i=1lX )). (3) Intuitively, the objective function guides the reranker to assign higher scores to passages that are similar to the rationale rij. Note that both objective Ldistill-KB (θ)andLrerank (ϕ)are independent; therefore, we do not need to jointly update both of the small LM and the reranker. 4.3 Inference After training, we obtain the small LM with the parameter (θ)and the reranker with the parameter (ϕ). At the test time, to answer the question x∗, we first get a set of candidate documents ˜), κ∗)with the retriever ρand κ∗= 100 . Then we re-rank all the document choose top- krelevant documents w.r.t the question x∗as follows: ∗}, k). Finally, we generate a rationale r∗= an answer y∗= ∗). 5 Experiments 5.1 Experimental Setting Task and Dataset In our experiments, we focus on reasoning tasks which require both the reasoning ability over the knowledge and the compound knowledge of the specific domain. As our primary benchmark, we use the medical question dataset — MedQA- USMLE [23]. The dataset contains 12,723 4-option question answering problems from US medical licensing exam. This dataset is the best fit to evaluate our method since 98% of the questions simulate the realistic clinical settings by presenting patient cases that require extensive professional knowledge and complex reasoning ability over multiple evidence sources. To further validate our approach, we employ StrategyQA [14] dataset, which involves 2,780 yes/no questions that demand sophisticated multi-step reasoning skills and the ability to gather supporting evidence from various domains. We additionally validate our approach on commonsense reasoning with OpenbookQA [39] dataset, which consists of 5,957 science questions with 4 options. Baselines We compare our method against relevant baselines. Few-shot In-context Learning (ICL) utilizes a few training samples as a prompt to make a prediction [ 5].Few-shot ICL + Chain- of-Thought (CoT) leverages prompting to generate a rationale and generate an answer based on the rationale [ refers to the one that fine-tunes a pre-trained model to generate an answer given only a question. The performance of the above baselines represents the capability of a small language model to solve reasoning tasks using only training data but without any extrinsic guidance on reasoning or external knowledge. To assess the impact of external knowledge, we augment the above three baselines with documents retrieved from the knowledge base (Wikipedia), denoted as models. For the knowledge augmentation, we append retrieved passages along with the question at both training and inference time. These baselines help us understand how much external knowledge can improve the performance of each baseline. We also compare our KARD against the standard Reasoning Distillation without [33, 17, 38, 12, 18]. 6 Table 1: Experimental results on the MedQA-USMLE dataset with Flan-T5 [ 53] and OPT [ 20,59] models. We report the mean and standard deviation of accuracy with 3 different runs for reasoning distillation methods. MedQA-USMLE (Flan-T5 [53]) MedQA-USMLE (OPT [20, 59]) Method Base (250M) Large (780M) XL (3B) 350M 1.3B-IML Few-shot 23.49 31.50 35.66 27.42 29.14 Few-shot + (CoT) 25.22 32.21 32.99 25.06 26.39 Few-shot + CoT 31.34 32.60 34.41 25.84 28.75 Fine-tuning 30.71 34.49 37.39 26.47 25.77 Fine-tuning 33.39 37.71 39.12 25.84 28.67 Reasoning Distillation 31.03 ±.40 39.62 ±.29 46.32 ±.36 29.43 ±1.13 34.30 ±.95 KARD (ours, BM25) 33.14 ±.23 41.87 ±.93 47.27 ±.67 30.79 ±.78 35.48 ±.37 KARD (ours, Reranker) 38.15 ±.39 44.59 ±.47 48.94 ±.32 32.86 ±1.12 38.83 ±.46 KARD (Silver Knowledge, Oracle) 40.30 49.80 53.50 35.90 42.18 CoT from ChatGPT (Teacher, Oracle) 61.59 65.51 67.16 - 50.27 StrategyQA (T5 [45]) OpenbookQA (T5 [45]) Method Base Large XL Base Large XL Few-shot 48.47 48.47 51.67 23.00 27.60 25.00 Few-shot + CoT 48.47 48.33 48.76 27.60 27.40 27.80 KAFew-shot + CoT 48.47 48.91 48.76 27.60 27.60 27.80 Fine-tuning 52.26 56.33 51.53 54.00 62.00 74.60 KAFine-tuning 52.11 58.81 53.38 53.80 64.60 73.80 Reasoning Distillation 55.36 ±.2764.97 ±.5568.41 ±.4858.87 ±.5066.13 ±.3477.00 ±.59 KARD (ours, BM25) 55.90 ±.2465.94 ±. ±.3864.40 ±.7176.00 ±.28 KARD (ours, Reranker) 56.57 ±.2566.04 ±.6070.55 ±.8159.33 ±.7466.40 ±.1678.53 ±.25 KARD (Silver Kn., Oracle) 57.50 65.65 72.34 63.40 72.40 82.00 CoT from ChatGPT (Oracle)†66.38 67.10 72.05 58.60 78.80 87.80Table 2: Experimental results on the StrategyQA andOpenbookQA dataset with T5 models [ 45].†indicates experiments with Flan-T5 having the same size. We report experimental results as in Table 1. Original Pubmed KARD Figure 3: Experimental results on MedQA-USMLE, where we per- form DAPT [ 15] on each dataset of x-axis with Flan-T5 Base. Asoracle models, we present a variant of KARD that receives better knowledge as input. In particular, at the inference time, we augment KARD with the silver document which is the passage retrieved with the gold rationale generated by the LLM as a query. This model represents an upper bound of the neural reranker performance. Additionally, we directly provide the small instruction fine-tuned language models (Flan-T5 [ 53] and OPT-IML [ 20]) with the rationale from the LLM in inference, to assess the upper bound of the performance gain on small models with high-quality rationales. Language Models For all the experiments, we use the T5 models [ 45] including Flan-T5 [ 9], and OPT models [ 59] including OPT-IML [ 20]. For the reranker, we use LinkBERT models [ 56]. As for the teacher LLM, we employ GPT-3.5-turbo (ChatGPT) [42] through the proprietary API. SeeAppendix B for experimental settings in detail. 5.2 Experimental Results Table 1 shows that KARD consistently outperforms all the baselines on the MedQA-USMLE dataset on both (Flan-T5) and decoder-only (OPT) language models. Remarkably, KARD exhibits a substantial positive effect on smaller models, as evident from the significant performance gain of the Flan-T5 Base model, which has 250 million parameters, over a fine-tuning baseline on the MedQA-USMLE dataset. Regarding the analysis of the model size, please refer to Section 5.3. The impact of KARD decreases as the size of the model increases since larger models can better memorize knowledge during pre-training and fine-tuning. Moreover, we empirically show that knowledge augmentation consistently improves performance not only in reasoning distillation but also in few- shot and fine-tuning. It is worth noting that this empirical evidence supports our theoretical analysis in Section 3 that knowledge augmentation enhances the performance of small models. Furthermore, our experimental results indicate that the reranker consistently improves the performance of models for all sizes, over the retrieval with BM25. From the experimental results with silver knowledge (oracle), there is room for improvement by retrieving more relevant documents, which can help the model generate a high-quality rationale. We also present additional experimental results on StrategyQA and OpenbookQA datasets in Table 2. Once again, KARD outperforms all baselines in experiments with both datasets. Notably, compared to MedQA-USMLE, the few-shot methods on StrategyQA and OpenbookQA exhibit performance similar to random guessing, as T5 lacks the ability of in-context learning [ 9]. Furthermore, fine-tuning 7 Fine-tuning KARD (ours) In-context learning (11B) 1 25 Accuracy (%) (a) Ratio of Training Data 250M 780M Accuracy (%) (b) Model Size Hits@1 Hits@3 Hits@5 Reranker (c) Hits@k metrics Figure 4: (a) Efficiency on training data and (b) model size. On MedQA-USMLE, we compare KARD against the fine-tuning baseline by varying either the number of training data with Flan-T5 Large or the number of parameters, including the few-shot in-context learning performance of Flan-T5 XXL (11B). (c)Considering silver documents as ground truth, we measure Hits@k on the documents retrieved by BM25 and the reranker. Table 3: Analysis on rationale diversity. BM25 Rationales Base Large l= 3 30.09 35.43 l= 5 32.13 39.04 l= 10 32.91 41.79Table 4: Analysis on κ∗. Reranker Passages Base Large κ∗= 20 36.45 43.91 κ∗= 50 36.06 44.23 κ∗= 100 36.76 45.25Table 5: Analysis on k. Flan-T5 Base Passages BM25 Reranker k= 1 32.91 36.76 k= 2 32.84 37.71 k= 3 32.36 37.39 T5-XL on StrategyQA results in poor performance since it fails to generalize to the test data. On the other hand, reasoning distillation improves the performance of models across all different sizes on both datasets. Our KARD further yields performance improvement over the reasoning distillation baseline, demonstrating the effectiveness of knowledge augementation in both datasets. 5.3 Analysis Experiments with DAPT Pre-Training (DAPT) [ 15] is the useful strategy to adapt Pre-trained Language Models (PLMs) on the specific domain to effectively tackle the tasks on it, which is done by further pre-training the PLM on a large-scale text corpus [ 2,30,37]. As it is interesting to observe whether the DAPT can enhance the capacity of PLMs for reasoning distillation in tasks by further performing training on relevant data before distillation, we conduct experiments with models from DAPT. Specifically, we further pre-train the Flan-T5 Base model on two moderate-scale biomedical corpora, Pubmed abstracts and MedWiki [ 35], respectively. Then, we apply reasoning distillation and KARD to PLMs with further pre-trained parameters. In Figure 3, we observe that DAPT on Pubmed marginally enhances the performance of reasoning distillation. On the other hand, KARD contributes more substantially to performance improvement than DAPT. This result indicates that KARD offers a distinct advantage in reasoning tasks compared to DAPT. Efficiency on Dataset and Model Sizes To validate the efficiency of our KARD in terms of training data and model size, we measure the test accuracy on the MedQA-USMLE dataset while varying the number of training data and model parameters. As shown in Figure 4a, our KARD can effectively transfer the reasoning ability of the LLM with the proposed KARD mechanism, using only a small number of training data. Moreover, the gaps between the naive fine-tuning and our KARD become much larger when increasing the number of training data, which confirms that we can potentially increase the effectiveness of KARD with more training data for distillation from LLMs. Furthermore, it is worth noting that KARD is a . With 25 %of the training data, KARD outperforms the same model fine-tuned on the full data. For the efficiency in terms of the model size, as shown in Figure 4b, KARD with 250M parameters achieves higher accuracy than the fine-tuned model with 3B parameters (14 times larger). Moreover, KARD with 780M parameters outperforms the 11B in-context learning baseline. These results show the significant practical advantage of our KARD in settings since the small LM with KARD requires significantly less computational cost yet it outperforms the LLM. Retrieval Performance To evaluate the performance of the reranker on MedQA-USMLE, we consider the top-3 silver documents retrieved with the rationales generated by LLM as the ground 8 Table 6: Rationale generation example of KARD, compared with Reasoning Distillation in MedQA-USMLE with Flan-T5 XL. The answer is in blue. We mark the hallucination in the generated rationale as yellow and highlight the factually correct statements in the generated rationale and its supporting fact in the passage as red. Question : A 29-year-old woman presents to the clinic after several months of weight loss. She noticed a 6.8 kg (15 lb) unintentional weight loss over the preceding several months. She has not changed her diet or exercise habits. She also reports feuding with her boyfriend over the temperature of their shared apartment, as she always feels warmer than he does. The vital signs include: heart rate 110/min and blood pressure 146/78 mm Hg. The physical exam is notable for warm and slightly moist skin. Which of the following is the best single treatment option for this patient? (A) (B) Methimazole (C) Propranolol (D) Radioiodine therapy Retrieved Passage Outputs Reasoning Distillation (without knowledge)No knowledge The patient’s symptoms are consistent with hyperthy- roidism, specifically Graves’ disease. forGraves’ disease. Methima- zole, propranolol, and radioiodine therapy can be used as adjunctive treatment. Answer: A KARD (Ours, . (...) optionfor Graves’ disease ismethimazole, which may temporarily help people while other treatments are having effect. Glucocorti- coids and radioiodine therapy can also be used, but they are not the best option in this case. Propranolol can help control the heart rate and blood pressure, but it will not treat (...)The symptoms and physical exam findings suggest hy- perthyroidism, specifically Graves’ disease. The best ’ disease ismethima- zole, which works by inhibiting the thyroid hormone synthesis. and radioiodine therapy can also be used, but they are not the best option. Answer: B truth, and measure Hits@k on the documents retrieved by BM25 and reranker with κ∗= 100 . In Figure 4c, the reranker achieves significantly better Hits@k than BM25. This result indicates that the reranker successfully learns to prioritize passages that are helpful to generate correct rationale at the test time, which leads to performance improvement on the reasoning tasks. The Number of Rationales During Training Following Ho et al. [17], we generate multiple rationales for each training sample in order to facilitate diverse reasoning in small language model training. In Table 3, we present the impact of rationale diversity during training on both Flan-T5 base and large models using the MedQA-USMLE dataset. As the number of rationales per training data increases, the performance also improves, demonstrating the benefit of employing multiple rationales. However, the performance gains become small when we increase the number of rationals from 5 to 10. This suggests that utilizing more diverse rationales beyond 10 may not yield significant further improvements, at least in the MedQA-USMLE dataset. The Number of Candidate Documents for Reranker It is crucial to determine the size of the candidate document set ( κ∗) to which the reranker assigns the relevance scores w.r.t a question. In Table 4, we present the performance of both Flan-T5 base and large models on MedQA-USMLE, while varying κ∗. The results indicate that increasing the number of candidate documents tends to be beneficial, as it allows the reranker to consider a broader range of diverse candidate documents. The Number of Passages Used for Inference Even LLMs tend to be easily distracted by irrelevant context [ 47]. Therefore, simply adding more passages during inference may not necessarily enhance performance if relevant knowledge is not selected. In Table 5, we present the impact of the number of passages used in KARD during inference ( kin Section 4.3) on Flan-T5 Base and MedQA-USMLE. We observe that the performance of KARD (BM25) without the re-ranker decreases with increasing k. This result implies that using additional passages does not always result in generating better rationales. In contrast, using two passages ( k= 2) with the reranker is better than a single passage ( k= 1). This result indicates that the reranker effectively selects more suitable knowledge than BM25, thereby contributing to performance improvement in the MedQA-USMLE benchmark. Qualitative Analysis In Table 6, we provide an example comparing the rationale generated by our KARD against the rationale by the baseline model with reasoning distillation but without knowledge augmentation. We choose one sample from the MedQA-USMLE dataset and generate the rationale using the Flan-T5 XL model. The model without knowledge augmentation generates the rationale that seems plausible based on the given instruction and question. However, it mistakenly generates the hallucination that are the first-line treatment for Graves’ disease, which is incorrect. As a result, it fails to predict the correct answer (B) Methimazole. In contrast, when the model is fine-tuned with KARD, it generates a correct rationale that is supported by the retrieved knowledge indicating that methimazole is the best single treatment option for Graves’ disease. Consequently, it successfully predicts the correct answer. This example highlights the effectiveness of our KARD method for generating accurate rationales by incorporating relevant knowledge, which leads to an improved question answering performance on reasoning benchmarks. 9 6 Discussion 6.1 Comparison to Generation Table 7: Experimental results includ- ing RAG on Reasoning Distillation (RD) with (Flan-)T5 base. MedQA StrategyQA KAFine-tuning 33.39 52.11 RAG + RD 24.84 54.24 KARD (Reranker) 38.15 Generation (RAG) [ 32] primarily focuses on solving tasks (e.g., open-domain QA), where the accurate knowledge retrieval is important to achieve higher performance. In terms of the methodology, the key dif- ferences between KARD and RAG are that RAG utilizes the question as a query and jointly fine-tunes the generator and re- triever. To quantitatively analyze the advantage of our KARD against RAG in reasoning distillation, we conduct experiments with RAG on the reasoning distillation with two datasets that we used in the main experiment, where we use Flan-T5 base for MedQA- USMLE and T5 base for StrategyQA as base LMs and DPR [ 25] as the trainable retriever for RAG. In Table 7, experimental results show that using RAG in reasoning distillation achieves lower accuracy than KARD, showing that our KARD is more tailored approach to reasoning distillations. 6.2 Failure Case Analysis In Table 1, we can see significant differences between KARD with reranker on the Flan-T5 XL and the ChatGPT in MedQA-USMLE. Our investigation focuses on understanding the cause of these gaps by examining samples where our method fails while ChatGPT succeeds. We collect 30 samples from corresponding cases and categorize them into two groups. The first group consists of cases where the reranker fails to obtain the document relevant to generating the correct rationale. The second group includes cases where the small language model fails to produce correct rationales and makes incorrect predictions, despite having access to relevant knowledge in the retrieved document. Out of 30 samples, 15 fall into the first category, while the remaining 15 belong to the second category. This observation indicates the need for further improvements in both retriever and distillation methods to enhance the performance of small language models in reasoning tasks. 6.3 Limitations We have shown substantial improvements in small LMs’ performance on reasoning tasks through our KARD. However, it is important to acknowledge the limitations of our study. First, in terms of methodology, the effectiveness of our knowledge augmentation heavily relies on the quality of the document retrieved from the external knowledge base. As indicated in Table 1 and Figure 4c, our reranker substantially improves the performance of small models by retrieving better knowledge. Despite the diminishing performance gap between BM25 and the reranker as the model size increases, there is still a significant difference between the document retrieved by the reranker and the silver knowledge. This indicates that the reranker might miss important passages that can augment the knowledge of even large LMs. Therefore, further advancements in retrieval methods are necessary to generate better rationale, as this remains an important research challenge even for large language models [ 13]. Second, regarding experiments, we have tested our approach on relatively small LMs having under 3B parameters given our limited computational budgets. However, exploring the use of relatively larger language models like GPT-3 [ 5,44] or LLaMA [ 50,51] with KARD could be of great interests, which is a promising direction for future research. 7 Conclusion In this work, we proposed Reasoning Distillation (KARD) which enhances the capabilities of small Language Models (LMs) on reasoning tasks that demand both knowledge and reasoning abilities. Our approach involves generating rationales from large LMs and fine-tuning small LMs on these rationales, while augmenting small LMs with external knowledge from a non-parametric memory. Our theoretical analysis motivates our method by demonstrating the effectiveness of external memory in reducing the memorization requirements of small LMs. Through empirical experiments, we showed that KARD outperforms traditional approaches such as fine-tuning and reasoning distillation, thereby providing a pathway to improve small LMs in reasoning tasks that require a comprehensive understanding of knowledge. 10  This work was done while the first author was working at AITRICS. We would like to thank Kangwook Lee and the anonymous reviewers for their insightful comments and suggestions regarding this work, which helped us to make improvements to the paper. This work was supported by AITRICS, the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST), No. , Development of Large Korean Language Model Technology for Efficient Pre-training, and No.2022-0-0071), the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. and ) and Samsung Electronics ().  
leandojo theorem proving with retrieval augmented language models 	LeanDojo: Theorem Proving with Language Models Kaiyu Yang1, Aidan M. Swope2, Alex Gu3, Rahul Chalamala1, Peiyang Song4, Shixing Yu5, Saad Godil∗, Ryan Prenger2, Anima Anandkumar1,2 Santa Barbara,5UT Austin Abstract Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo : an open-source Lean playground consisting of toolkits, data, models, and bench- marks. LeanDojo extracts data from Lean and enables interaction with the proof environment . It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection —a key bottleneck in theorem proving. Using this data, we develop ReProver ( Prover ): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo’s program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effec- tive. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean’s math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research. 1 Introduction Reasoning is a cornerstone of human intelligence and a fundamental goal of AI [ 3]. One prominent task is automated theorem proving (ATP): automatically generating proofs for theorems expressed in formal logic. ATP is useful for formal mathematics, producing mathematical proofs that can be checked rigorously [ 4]. Furthermore, it underpins formal verification, which is essential for proving the correctness and safety of high-stakes applications [5, 6]. ATP is challenging since the search space is prohibitively large. In many applications, it is impractical to generate proofs fully automatically. Therefore, interactive theorem proving (ITP) has emerged as an alternative paradigm. In ITP, proofs are constructed by human experts interacting with software tools called proof assistants, such as Coq [ 7], Isabelle [ 8], and Lean [ 1]. Machine learning can automate such interactive theorem proving, opening up a new avenue for theorem proving [ 9]. The model can learn to interact with proof assistants, given data containing human-written proofs. ∗Research conducted while Saad Godil was at NVIDIA. 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.  _$succ_pos n n,{unfold gcd},unfold gcd,rewrite mod_self,apply _$succ_pos n n,{unfold gcd},unfold gcd,rw mod_self,apply 1 Lean theorems and proofs•217,776 premisesk : ℕ⊢gcd((k + 1) % (k + 1)) (k + 1) = k + 1All accessible premises in the math libraryMaximum cosine 1rewrite mod_self ... Retrieved learning modelData theorems by Interactionn : ℕ⊢gcdn n = n⊢gcd0 0 = 0k : ℕ⊢gcd(k + 1) (k + 1) = k + 1k : ℕ⊢gcd((k + 1) % (k + 1)) (k + 1) = k + 1k : ℕ⊢gcd0 (k + 1) = k + 1 Tacticcases nunfold gcdunfold gcdrewrite mod_selfapply tree -- gcd z y|0y:=y-- Case 1: z == )-- Case 2: z > n n,{unfold gcd},unfold gcd,rewrite mod_self,apply 1 ...33K on average…Figure 1: Top right : LeanDojo extracts proofs in Lean [ 1] into datasets for training machine learning models. It also enables the trained model to prove theorems by interacting with Lean’s proof environment. Top left : The proof tree of a Lean theorem ∀n∈N,gcd n n = n , wheregcdis the greatest common divisor (details in Sec. 3). When proving the theorem, we start from the original theorem as the initial state (the root) and repeatedly apply tactics (the edges) to decompose states into simpler sub-states, until all states are solved (the leaf nodes). Tactics may rely on premises such asmod_self defined in a large math library. E.g., mod_self is an existing theorem ∀n∈N,n % n = 0 used in the proof to simplify the goal. Bottom : Our ReProver model (Sec. 5). Given a state, it retrieves premises from the math library, which are concatenated with the state and fed into an Transformer [2] to generate the next tactic. Formal theorem proving serves as an important challenge for machine learning. From a computer science perspective, formal proofs can be treated as programs [ 10]. But unlike conventional programs in C++ or Python, the correctness of proofs can be verified using proof assistants. Therefore, theorem proving may be considered a special form of code generation, with rigorous evaluation and no room for the model to hallucinate. This can be consequential to current large language models (LLMs), as they have demonstrated exceptional capability in code generation [ 11] but have flaws in factuality and hallucination [ 12]. In addition, augmenting LLMs with external tools, such as proof assistants, has shown promise in improving their various capabilities, including multi-step reasoning [13]. Current research on LLMs for theorem proving is facing many barriers. To our knowledge, none of the existing LLM-based provers are open-source [ 14–21]. They all use private pretraining data, and the compute requirements can reach thousands of GPU days [ 17]. Furthermore, some rely on tailored infrastructure for distributed training and interaction with the proof assistant—both are not possible to fully reproduce without open-source code [ 17,19]. We change the status quo by introducing LeanDojo: open-source toolkits, models, and benchmarks that give researchers access to LLM-based provers with modest computational costs. Tools for Data Extraction and Interaction. We focus on Lean, a proof assistant popular among framework LeanDojo provides two essential functions for learning-based theorem proving (Fig. 1): extracting data and enabling models to interact with Lean . For data extraction, LeanDojo extracts training data not directly visible in the raw Lean code (Fig. 2), e.g., proof trees consisting of intermediate states between proof steps (Fig. 1 Top left ). In addition, LeanDojo is the first tool to locate premises in Lean proofs, enabling training machine learning models for premise selection. For interaction, LeanDojo turns Lean into a gym-like interactive environment [ 22]. Using LeanDojo, the model can observe proof states, change the state by executing 2“Lean” in our paper refers to Lean 3 by default. Lean 4 is not but is also supported by LeanDojo. Our Lean 4 results are in Appendix D. 2 proof steps (referred to as “tactics” in proof assistants), and receive feedback from Lean. LeanDojo is the first tool capable of interacting with Lean reliably, reducing proof-checking errors in existing tools [19] (correct proofs misjudged as incorrect) from 21.1% to 1.4%. LLMs for Theorem Proving. LeanDojo addresses a key bottleneck in theorem proving: premise selection [23,24]. Existing LLM-based provers generate the next proof step (tactic), taking only the current state as input. However, proving theorems depends critically on the premises, such as lemmas and definitions, from a math library. For example, Fig. 1 ( Top left ) illustrates the proof of “ ∀n∈N,gcd n n = n ”, where gcd stands for greatest common divisor. The proof starts from the original theorem as the initial state and repeatedly applies tactics to decompose states into simpler sub-states, until all states are solved. Tactics may rely on premises such as mod_self defined in a large math library. E.g., mod_self is an existing theorem “ ∀n∈N,n % n = 0 ” useful for simplifying the goal. Incorporating all possible premises is too large to fit into LLMs’ input, given the limited context window. Existing methods must learn to memorize the association between the proof state and the namemod_self . It works if the premise has been used in the training data to solve similar goals, but does not generalize to truly novel scenarios, e.g., theorems requiring lemmas unseen in training. One potential solution is to complement memorization with explicit premise selection. LeanDojo extracts premise data from Lean, including where they are defined and used. It enables us to tackle premise selection by augmenting LLMs with retrieval. We introduce ReProver ( Prover ) (Fig. 1 Bottom ): Given the current state, it generates a tactic conditioning on a small number of premises retrieved from Lean’s math library, mathlib [25]. We need to limit retrieval to a small number of premises for it to be effective, and ideally, they should contain the ground truth premise. Our retriever builds upon Dense Passage Retriever (DPR) [ 26] but incorporates two algorithmic innovations: First, not all premises are accessible when proving a theorem (Sec. 3). LeanDojo can perform program analysis on Lean code to determine accessible premises. On our data, that reduces the average number of premises from 128K to 33K, significantly simplifying the retriever’s task. Second, DPR needs negative examples in training and benefits from hard negatives, i.e., irrelevant premises that are hard to distinguish from ground truth ones. We propose in-file negatives: a simple mechanism to find hard negatives in premise selection, which samples negative premises defined in the same Lean source file as the ground truth premise. LeanDojo Benchmark. Using LeanDojo, we construct a benchmark containing 98,734 theorem- s/proofs extracted from mathlib . Our benchmark is one of the largest math-focused datasets. We find that the common practice of splitting theorems randomly into has led to an overestimated performance in the previous papers. LLMs can prove seemingly difficult theorems simply by memorizing the proofs of similar theorems during training. In LeanDojo Bench- mark, we mitigate this issue by designing challenging data split requiring the model to generalize to theorems relying on novel premises that are never used in training. We use LeanDojo Benchmark to train and evaluate ReProver. Training takes only five days on a single GPU. In evaluation, ReProver can prove 51.2% theorems, outperforming a baseline that generates tactics directly without retrieval (47.6%) and another baseline using GPT-4 [ 27] to generate tactics in a zero-shot manner (29.0%). We also test ReProver on two existing datasets, MiniF2F [ 28] and ProofNet [ 29]. It can prove 26.5% theorems in MiniF2F and 13.8% in ProofNet, which is competitive with methods without reinforcement learning [ 19], even though trained using far fewer resources. Moreover, it can prove 65 theorems that currently do not have proofs in Lean. Thus, our tool can also serve as an effective tool for augmenting existing math libraries in Lean. Contributions. In summary, we make four main contributions: First, we introduce tools for extracting data from and interacting with Lean. Second, we develop ReProver, the first retrieval- augmented language model for theorem proving. Third, we construct a challenging benchmark for learning-based theorem proving and use it to validate the effectiveness of ReProver. Finally, we facilitate open research on LLMs for theorem proving by releasing our data, model, and code. Our method does not rely on private datasets and can be trained on a single GPU within a week. We believe this will significantly lower the barriers to academic research in this area and establish the first accessible baselines for future work to build upon. Further, our method can be used to automatically generate new Lean proofs without requiring human effort. 3 2 Related Work Theorem Proving. Classical provers express theorems in first-order logic and search for proofs automatically in a large space [ 30,31]. Even with data-driven search heuristics [ 32,33], they fail to scale to large formalization projects. Therefore, recent work on learning-based theorem proving has focused on an alternative paradigm: automating the interaction with proof assistants. The architecture of learning-based provers progressed from classical machine learning algorithms such as KNN [ 34], to graph neural networks explicitly encoding the syntax of formal expressions [ 9,35], and now LLMs treating expressions as plain strings [ 14]. Besides the model architecture, researchers have explored several complementary dimensions: proof search algorithms for assembling steps into complete proofs [ 17,21]; overcoming data scarcity through reinforcement learning (RL) [ 17,19,36,37] or data [ 16,38–40]; as well as outsourcing some proof goals to classical provers [ 18,41–43]. Our base model without retrieval is a combination of design choices. It generates tactics by finetuning an Transformer, ByT5 [ 44], via supervised learning without RL or auxiliary data. Then it searches for proofs using best-first search. Our model’s algorithmic novelty lies in the retrieval. Premise Selection. Selecting useful premises is recognized as a key challenge in theorem prov- ing [ 23,24,45,46]. Machine learning methods for premise selection have also progressed from classical models [ 41,47,48], recurrent neural networks [ 24], graph neural networks [ 38], to Trans- formers [ 49,50]. However, existing methods either tackle premise selection in isolation without theorem proving [ 24,38,48] or feed the premises to a symbolic prover [ 41,47,49]. To our knowl- edge, we are the first to augment a learning-based formal theorem prover with retrieved premises so that the prover can learn how to use them effectively. For example, it can decide whether to use an explicitly retrieved premise or an implicitly memorized one. Data and Tools for Theorem Proving. Tools for data extraction and interacting with proof assistants have been crucial drivers of learning-based theorem proving. Existing tools and datasets can be divided by proof assistants: Coq has GamePad [ 51], CoqGym [ 9], and PRISM [ 52]; Isabelle has IsarStep [ 53] and PISA [ 15]; HOL Light has HOList [ 54] and HoLStep [ 55], and Lean has LeanStep [ 16] andlean-gym [19]. MiniF2F [ 28] is the only cross-system dataset, with 488 theorems for evaluation. However, it does not have training theorems and is restricted to the domain of math olympiads. Among available tools extracting data from proof assistants, LeanDojo is the only one that can extract premises for theorem proving. A few existing datasets also have premises [ 49, 54], but their data extraction tools are not public, making it difficult to construct new datasets. In addition, LeanDojo is the only tool that can interact with Lean robustly (Sec. 4) and can extract data from Lean 4. See Appendix A.3 for a detailed comparison between LeanDojo and alternatives. Mathematical Reasoning in Natural Language. We focus on proving theorems expressed in formal logic, whereas researchers have also produced a plethora of work on mathematical reasoning in natural language [ 56–63]. A particularly relevant task is , translating natural language texts into formal theorems and proofs [29, 64–72]. Language Models. Our ReProver is the first language model for formal theorem proving, though similar architectures have been studied extensively in NLP [ 73–81]. In addition, there have been many methods for code generation [ 82– 88]. Most of them retrieve from a corpus not directly related to the current file, e.g., GitHub or Stack Overflow. In contrast, our retrieval corpus consists of premises accessible to the current file, which is determined by program analysis using LeanDojo. This is similar to what CoCoMIC [ 88] does for Python. However, their retrieval is based on heuristics, whereas ours is learned. 3 Background: Theorem Proving in Lean At a high level, Lean is a programming language that allows you to write not only conventional programs but also theorems and proofs. To that end, it provides two pieces of machinery: First, it provides a unified language for defining programs, mathematical objects, theorems, and proofs, based on functional programming with dependent types [ 89]. Second, it provides a tactic system for constructing proofs . 4  _$succ_pos n n,{unfold gcd},unfold gcd,rw mod_self,apply 1 Math _$succ_pos n n,{unfold gcd},unfold gcd,rewrite mod_self,apply -- gcd z y|0y:=y-- Case 1: z == )-- Case 2: z > n n,{unfold gcd},unfold gcd,rewrite mod_self,apply 1ImportFigure 2: Definition of greatest common divisor ( gcd) in Lean and two related theorems. The proof ofgcd_self (between “ begin ” and “end”) relies on a premise mod_self imported from another file in the math library. Lean can run this proof to produce the proof tree in Fig.1 ( Top left ). We use a simple example in Fig. 2 to illustrate how theorems are formalized and proved in Lean.3 Here we want to formalize the greatest common divisor ( gcd) of two natural numbers. First, we define gcdas a recursive function, taking two natural numbers as parameters and returning their gcdvia the Euclidean algorithm. Then, we state a lemma named gcd_zero_left that∀x∈N,gcd 0 x = x , which can be proved simply by the definition of gcd. Finally, we state our main theorem gcd_self that∀n∈N,gcd n n = n , followed by its proof consisting of five tactics. In theorem proving, we are only concerned with generating the proof, i.e., the part between “ begin ” and “end”; everything before “begin ” is known, including other files imported. The syntax of tactics is quite expressive. They can take arguments and can be combined into compound tactics. You can think of tactics as programs in a language (DSL). Users can extend the DSL by defining new tactics. This discrete, combinatorial, and unbounded action space makes theorem proving challenging for machine learning. Another challenge is premise selection. Premises are existing lemmas or definitions useful for proving a theorem. They are used as arguments in tactics. For example, in Fig. 2 and Fig. 1 ( Top left ), the tactic “rewrite mod_self ” rewrites the goal using the premise mod_self , which is defined in another file imported by the current file. Proofs cannot use premises that haven’t been defined. For cannot be used to prove gcd_zero_left . In addition, they cannot use premises not imported to the current file. Still, premises come from a large math library containing hundreds of thousands of existing definitions and theorems, making it hard, for humans and machines alike, to select the right premises when generating a tactic. This is a key bottleneck in theorem proving and is what we aim to address through LLMs. 4 LeanDojo: Toolkit and Benchmark LeanDojo serves two essential needs of learning-based theorem proving in Lean. First, it extracts training data from Lean, and we use this capability to construct a challenging theorem proving benchmark. Second, it enables the model to interact with Lean . Data Extraction. Lean repos (e.g., mathlib orlean-liquid ) contain source code of human- written . However, the raw code is unsuitable for training the prover. It lacks runtime information that humans can access when using Lean, such as intermediate states between proof steps. Therefore, LeanDojo extracts the following information not directly visible in the code: 3The process is similar in many other proof assistants, though they may have different logical foundations. 5 •File dependencies and abstract syntax trees (ASTs): LeanDojo processes the repo to produce a directed acyclic graph whose nodes are files and edges are import relations between files. In addition, LeanDojo produces the AST of each file. File dependencies and ASTs are useful for program analysis, e.g., collecting theorems defined in a file or premises accessible to a theorem. •States and tactics: LeanDojo extracts all tactics in proofs. For each tactic, it also extracts the states before/after the tactic, which allows us to reconstruct the proof tree in Fig. 1 ( Top left ). •Premises: For each premise, such as mod_self in Fig. 2, LeanDojo records where it is defined (location in ) and where it is used (locations across many files). In addition, premises have unique fully qualified names (e.g., nat.mod_self ) but are often used by ambiguous short names ( mod_self ), relying on Lean to perform name resolution. LeanDojo is capable of recording their full names. Lean has basic support for exporting dependencies, ASTs, states, and tactics. However, it cannot resolve the premises’ full names and locate their definitions. Therefore, we modify Lean to record this information (details in Appendix A.1). The modified Lean is used only for data extraction but not for evaluation, so we do not risk accidentally breaking Lean’s logical soundness. LeanDojo Benchmark. We construct a benchmark for premise selection and theorem proving, named LeanDojo Benchmark . The data is extracted from centralized math library covering diverse topics such as analysis, algebra, and Benchmark is one of the largest math-focused theorem proving datasets, consisting of 98,734 theorems from 3,384 Lean files. Unlike existing datasets in Lean [ 16], LeanDojo Benchmark also contains the definitions of 130,262 premises, including not only theorems but also other definitions that can be used as premises (e.g., gcd in Fig. 2. Furthermore, the dataset has 217,776 tactics, 129,243 of them with at least one premise. The average number of premises is 2.13 among tactics with premises. Appendix B contains additional information on data format, datasheet [90], hosting, and licensing. Figure 3: Similar are common. If splitting them randomly into , the model can prove testing theorems by memorization. LeanDojo Benchmark has theorems for . It features a challenging data split for testing the prover’s generalization in more realistic scenarios. Splitting theorems randomly can overestimate the prover’s performance, by allowing it to prove many theorems through memorization. In human-written Lean code, a common idiom is to have a block of similar for slightly different properties of the same math concept. For example, in Fig. 3, the last two theorems not only look similar but have identical proofs. If one of them is in training, the model can easily prove the other one by memorization. This shortcut enables the model to prove seemingly nontrivial theorems, including those requiring premises to prove. To mitigate this issue, besides the random split, we create a challenging data split named novel_premises . It requires testing proofs to use at least one premise that has never been used in training. For example, the last two theorems in Fig. 3 both use the premise conj_mul. If one theorem is in the training set of the novel_premises split, the other one must also be in training. 4We use the commit released on October 11, 2023. 5More details, statistics, and visualizations of mathlib can be found at . . 6 Interacting with Lean. Another important function of LeanDojo is to interact with Lean program- matically. It turns Lean into a gym-like environment [ 22], in which the prover can observe the proof state, run tactics to change the state, and receive feedback on errors or on proof completion. This environment is indispensable for the prover or training it through RL. Below is LeanDojo’s main interface for interacting with Lean through tactics. Lean also supports other proof styles not based on tactics. Although we only support tactic-style proofs, they are sufficiently general since any proof can be converted to a tactic-style proof.6 •) : Given the theorem to prove, LeanDojo returns the initial state. A valid state is a string representing current proof goals and local contexts (see the nodes in Fig. 1 Top left ). When there are multiple goals, their strings are concatenated. •run_tac(state, tactic) : Run a tactic on a given state and return the next state. The returned state will be an error state if the tactic execution is not successful, e.g., due to timeout or inapplicable tactic. If the input state is an error, the result can only be an error. Building this environment is technically challenging, as Lean is designed for human users, not machines. LeanDojo is the first tool that can interact with Lean reliably. Existing tool [ 19] is limited: 21.1% of the ground truth proofs are misjudged as incorrect, due to issues with how they construct the proof environment, which distorts the reported performance and produces unreliable feedback when used in reinforcement learning. In contrast, LeanDojo reduces the number of misjudgments to 1.4%. Details are in Appendix A.2. 5 ReProver: Theorem Prover We develop the ReProver model that uses retrieval to select premises explicitly. At its core is a tactic generator (Fig. 1 Bottom ). Given the current proof state, it retrieves a handful of potentially useful premises and generates a tactic conditioning on the concatenation of the state and retrieved premises. When proving theorems, the model generates multiple tactic candidates at each step, which are used in a standard best-first search algorithm to find proofs [16, 18, 19, 28]. Premise Retrieval. Our retriever is based on Dense Passage Retriever [ 26]. Given a state sas the query and a library of candidate premises P={pi}N i=1, it retrieves a ranked list of mpremises {p′ i}m i=1fromP. In DPR, sandpiare both raw texts but are embedded in a vector space, and we retrieve the top mpremises maximizing the cosine similarity between the state and the premise. More formally, we have a function fparameterized by θfor embedding both the state and the premises into a h-dimensional vector space: f(s, θ), f(pi, θ)∈Rh. We retrieve premises maximizing f(s, θ)Tf(pi, θ)/(∥f(s, θ)∥2∥f(pi, θ)∥2). We choose fto be a Transformer encoder [ 2] followed by average pooling: f(·, θ) =AvgPool (Enc(·, θ)). The retrieval is efficient. The premise embeddings f(pi, θ)can be pre-computed, and we only need one forward pass to compute f(s, θ). We do not rerank the retrieved premises as in Mag- nushammer [ 49], which is more costly since it requires a separate forward pass for each retrieved premise. Similar to DPR, we train the retriever by minimizing a contrastive loss between positive premises and in-batch negative premises. Specifically, suppose we have a batch of bstates. For each state, we sample a positive premise from the ground truth and nnegative premises from P.7They are called “in-batch” negatives because they are shared by all states in the batch—Every state is associated with allb·(n+ 1) premises; at least 1 of them is positive. Let lij∈ {0,1}denote whether a state-premise pair(si, pj)is positive. We minimize the mean squared loss: L(θ) =bX i=1b·(n+1)X j=1 lij−f(si, θ)Tf(pj, θ) ∥f(si, θ)∥2∥f(pj, θ)∥2 2 . (1) 6Another common type of proofs is “term-style proofs”. Any term-style proof “ X” can always be converted into an equivalent tactic-style proof “ exact X ”, though such conversion may lead to unidiomatic proofs. 7When training the retriever, we ignore proof states followed by tactics without using any premise. 7 Retrieving from Accessible Premises. We incorporate into DPR two insights tailored to premise selection. First, instead of retrieving from all premises in the math library, we restrict to premises accessible to the current theorem. They include premises defined in the same file before the theorem, as well as those imported from other files. We compute accessible premises for each theorem, relying on LeanDojo’s capability in program analysis (Sec. 4). Focusing on accessible premises makes P much smaller. LeanDojo Benchmark contains 130,262 premises in total, but the average number of accessible premises is only 33,160. In-file Negative Examples. DPR’s performance depends critically on the quality of negative examples [ 91,92]. In early experiments, we sampled all nnegative premises randomly, and the model often mistakenly retrieved other premises from the same file as the positive one. Therefore, we propose a scheme that samples kin-file negatives and n−krandom negatives for training. Tactic Generation. As in Fig. 1 ( Bottom ), retrieved premises are concatenated with the state.8 Then an Transformer, ByT5 [ 44], takes them as input and generates the tactic. The model is trained to minimize the cross entropy loss w.r.t. human-written tactics. Training ReProver takes substantially less compute than prior methods (120 GPU hours vs. more than 1000 hours [ 16,17]). All existing LLM-based provers pretrain on datasets specific to math and coding [ 14–20]. The pretraining is expensive, and the datasets are kept private. In contrast, we choose to avoid pretraining and build upon —a model checkpoint that is generic, publicly available, and relatively small (299M parameters vs. 837M [ 16] or 600M [ 17]). We could see further benefits from pretraining, as in Minerva [ 57], or stronger LLMs like LLaMA [ 93] or StarCoder [ 94], but that is beyond our scope. In addition, our model is finetuned on human-written tactics only, without auxiliary data [ 16] or data collected through online interaction with Lean [ 17,19]. These orthogonal directions are valuable but will significantly increase the method’s complexity and compute requirements. 6 Experiments We evaluate ReProver on LeanDojo Benchmark. It outperforms baselines on premise selection and theorem proving, demonstrating the promise of theorem proving with language models. Experimental details and are in Appendix C.1. Premise Selection. For premise selection, we only use tactics in LeanDojo Benchmark that have at least one premise. The model, based on a ByT5 encoder, uses the state before a tactic as the query to retrieve 100 premises. Then, we calculate standard metrics in information retrieval: R@k (recall for the top kretrieved premises) and MRR (mean reciprocal rank). Our first baseline is a classical BM25 retriever [ 95] without machine learning. Results in Table 1 show that our method outperforms BM25 significantly across the board. However, it exhibits a large performance degradation on the challenging data split (comparing novel_premises torandom ). This is consistent with the general observation that machine learning can be brittle in the presence of distribution shifts. In addition, we compare with two ablations: one retrieving from all premises (instead of accessible premises only) and the other without in-file negatives. They perform worse than our method, demonstrating the effectiveness of our two improvements upon DPR. Theorem Proving Experimental Setup. Then we evaluate ReProver on theorem proving. The training has two stages: First, we train the retriever and use it to retrieve 100 premises for all proof states in LeanDojo Benchmark. Second, we train the tactic generator, taking as input the concatenation of the state and retrieved premises (truncated to a length limit). During evaluation, the tactic generator is combined with best-first search to prove theorems. We evaluate the Pass@1 metric: The prover is given only one attempt and must find the proof within a wall time limit of 10 minutes. Training takes five days on a single NVIDIA A100 GPU with 80GB memory, and evaluation takes two days on eight V100 GPUs. Please see Appendix C.1 for details. Baselines. Following prior work [ 16,28], we include tidy as a baseline. It is a tactic in mathlib that tries to complete the proof using heuristics (without machine learning). We apply tidy directly 8We retrieve 100 premises, concatenate them with the state, and truncate the concatenation to a fixed length. 8 Table 1: Premise selection testing performance. For each method, we train and evaluate two models independently using different data splits ( random ; see Sec. 4). R@k is the recall for the top kretrieved premises, and MRR is the mean reciprocal rank metric (higher is better). Our retriever outperforms BM25 and ablations. Results for Lean 4 are in Appendix D. Method random novel_premises R@1 R@10 MRR R@1 R@10 MRR BM25 6.7 17.2 0.15 5.9 15.5 0.14 w/ all premises 1.9 11.9 0.08 2.1 12.4 0.08 Ours 13.5 38.4 0.31 9.1 27.6 0.24 w/ all premises 11.7 36.2 0.27 7.1 23.1 0.20 w/o in-file negatives 10.8 33.1 0.25 7.9 25.7 0.22 to the original theorem and see if it can succeed within the wall time limit. Another baseline uses GPT-4 as the tactic generator. Given a state, it queries GPT-4 to generate 35 tactics in zero-shot. After removing invalid ones, the remaining tactics are combined with best-first search to find proofs. Data contamination is possible: Many proofs had been publicly available on GitHub before GPT-4’s data cutoff date (September 2021). See Appendix C.2 for details. Unfortunately, it is not feasible to compare with existing LLM-based provers in Lean [ 16,17,19]. None of them are open-source or can be reproduced with reasonable effort. Furthermore, we cannot compare directly with the numbers reported in their papers, due to differences in data, infrastructure, and training procedures (details in Appendix C.3). Many difficulties are due to the private nature of existing methods. By releasing our code and models, we hope to create accessible baselines for future work to build upon. Table 2: Theorem proving Pass@1 (%) on the testing data of LeanDojo Benchmark. Our ReProver model outperforms tidy , GPT-4, and a baseline that generates tactics directly without retrieval. Results for Lean 4 are in Appendix D. Method random novel_premises tidy 23.8 5.3 GPT-4 29.0 7.4 ReProver (ours) 51.2 26.3 w/o retrieval 47.6 23.2 Results. Table 2 shows the results on the testing data of LeanDojo Benchmark. ReProver outper- forms all baselines on two different data splits, demonstrating the effectiveness of theorem proving. GPT-4 performs substantially worse than our method, even though it may have seen the ground truth proofs due to data contamination. The task cannot be solved out of the box by LLMs, calling for algorithmic innovations to make further progress. Testing theorems in novel_premises are indeed much more challenging. All methods in Table 2 perform substantially worse on novel_premises than therandom split. We argue that performance on challenging splits is more indicative of the prover’s capability and should be emphasized in the future development of theorem proving. Evaluation on MiniF2F and ProofNet. We run ReProver to prove theorems in MiniF2F [ 28] and ProofNet [ 29]. These two datasets are for testing only and do not have training theorems, which makes them challenging since the distribution of theorems is quite different from mathlib used to train ReProver. MiniF2F focuses on math olympiads, and ProofNet focuses on exercises in undergraduate math textbooks. On MiniF2F’s test set in Lean, ReProver achieves a Pass@1 of 26.5%, which is competitive with methods without RL (25.9% in Polu et al. [ 19]). On ProofNet, our Pass@1 is 13.8%, which is the first reported theorem proving result on this dataset. Further, many theorems do not have ground truth proofs in Lean. Our prover discovers 33 proofs in MiniF2F and 39 proofs in ProofNet that currently do not have Lean proofs. Please see Appendix C.4 for details, examples, and caveats. 9 7 Conclusion We have introduced LeanDojo: an open-source playground for learning-based theorem proving in Lean, consisting of toolkits, models, and benchmarks. It extracts data from Lean and enables the model to interact with Lean . We have developed ReProver, the first retrieval- augmented LLM for theorem proving. Limitations and future work are discussed in Appendix F. We have released our code, data, models, and documentation to facilitate future research: •LeanDojo’s codebase for data extraction and interaction with Lean: https://github. • LeanDojo’s documentation: •Datasets: (1) LeanDojo Benchmark: with DOI . (2) LeanDojo Benchmark 4 (Appendix D): https: // with DOI . • ReProver’s code and models: • ChatGPT plugin (Appendix E): • LeanDojo Website: and Disclosure of Funding This work is partially supported by Caltech’s Center for Autonomous Systems and Technologies. Kaiyu Yang is supported by the Computing, Data, and Society Postdoctoral Fellowship at Caltech. Alex Gu is supported by the National Science Foundation (NSF) Graduate Research Fellowship. Rahul Chalamala and Peiyang Song are supported by the Summer Undergraduate Research Fellowships (SURF) program at Caltech. Anima Anandkumar is partially supported by the Bren endowed chair. We appreciate the valuable feedback from Logan Murphy and members of the Anima AI+Science Lab on an initial version of this paper. We thank Junyan Xu for manually inspecting the proofs generated by our model on ProofNet. We also thank Jeremy Avigad and Mario Carneiro for insightful discussions on supporting Lean 4 in LeanDojo. 10  
lift yourself up retrieval augmented text generation with self memory 	Lift Yourself Up: Text Generation with Self-Memory Xin Cheng1Di Luo2Xiuying Chen3Lemao Liu4Dongyan Zhao1Rui Yan2 1Peking University of China 3KAUST4Tencent AI Lab Abstract With direct access to human-written reference as memory, generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation (we define this as primal problem ). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, Selfmem , which addresses this limitation by iteratively employing a generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of Selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves results in four directions in JRC-Acquis translation dataset, 50.3 ROUGE-1 in XSum , and 62.9 ROUGE-1 in BigPatent , demonstrating the potential of self-memory in enhancing generation models. Furthermore, we conduct thorough analyses of each component in the Selfmem framework to identify current system bottlenecks and provide insights for future research1. 1 Introduction In recent years, text generation has attracted growing interest across various fields, including neural machine translation[28, 17, 2], dialogue response generation[81, 6, 46], and language modeling[ 36,77,19]. This innovative generation paradigm initially equips a fine-tuned small model or a large language model (LLM) with access to an external database (typically the training corpus) using information retrieval techniques. Subsequently, the generation process is conducted based on both the input text and the retrieved memory. In this paradigm, the guiding principle for memory retrieval is to find the memory that exhibits the highest similarity to the current input [ 36,96,49]. This aligns with the human intuition that a more similar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a translation model, the memory similarity alone exhibits a strong correlation with the final translation quality, regardless of other factors that may influence translation quality (e.g., 1Code and data available at: 37th Conference on Neural Information Processing Systems (NeurIPS 2023). polysemy, morphology, and coreference). We define this as the primal problem :better memory prompts better generation . Consequently, numerous studies have focused on how to retrieve better memory, ranging from sparse retrieval to dense retrieval [ 10,63], from a fixed retriever to a learnable retriever [ 41,8], and from sentence-level memory to more fine-grained token-level memory [ 36,35]. 0.0 0.2 0.4 0.6 0.8 1.0 Memory BLEU Figure 1: Relation between memory and hy- pothesis on JRC-Acquis En→De dataset. The hypothesis is generated by a retrieval- augmented translator whose memory is re- trieved from the training set. The X-axis represents the similarity between memory and the , a fundamental limitation exists in all previous works: the memory is retrieved from a fixed corpus and is constrained by the corpus’s quality. Due to the finite retrieval space, bounded memory significantly restricts the potential of generation models [ 97]. In this paper, we explore the duality of the primal problem , which posits that better generation also prompts better memory . We propose a novel framework called Selfmem , which iteratively employs a generator to create an unbounded memory pool and uses a memory selector to choose one output as memory for the subsequent generation round. By combining the primal anddual problem , a retrieval- augmented generation model can elevate itself using its own output, referred to as self-memory. The key insight behind Selfmem is that the text more closely resembling the data distribution during inference is not the training data [87], but the model’s own output. Selfmem consists of two complementary components: a generator and a memory selector. The generator operates under two distinct paradigms: fine-tuning a small model or few-shot prompting an LLM. For the former, we train the generator with labeled data and retrieved memory, while for the latter, we employ a fixed black-box LLM exclusively for inference alongside retrieved in-context learning samples. We then use the generator’s output to train a memory selector based on a specific performance metric. By simply replacing the retrieved memory with unbounded generated memory, we achieve higher-quality generation output ( primal problem ), which subsequently serves as memory for the next round after being refined by the memory selector ( dual problem ). To evaluate the efficacy of the Selfmem , we carry out comprehensive experiments in three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation. We witness substantial enhancements over robust baselines, attaining outcomes in JRC-Acquis (four directions), XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1). To gain deeper insights into the Selfmem , we meticulously investigate each crucial component and pinpoint the existing system bottleneck to guide future research endeavors. 2 Related Work 2.1 Text Generation Since the world is not a snapshot once the training corpus is collected, we can never expect an ever-large model to capture everything in its parameters, even for LLMs like GPT-4 [ 62]. Therefore, it is crucial to equip these models with an external memory bank to store additional knowledge or useful demonstration examples for solving various NLP tasks[41, 78, 95]. In the translation domain, retrieval techniques have long been employed by the localization industry to enhance human translators’ productivity and consistency even before the advent of machine translation [ 94]. Early works on machine translation primarily focused on utilizing memory for statistical machine translation (SMT) systems [ 80,50]. For neural machine translation (NMT), [28] were the first to use search engines to retrieve memory from the training set and incorporate it with an external memory network. Subsequent research explored various aspects of retrieval- augmented NMT, such as memory encoding methods [ 92,93,31], joint training of retrievers and generators with monolingual data [ 8], memory granularity [ 35], and memory diversity [ 17]. For few-shot LLM generation, strategies for in-context example selection have been proposed to improve translation quality [ 2]. Furthermore, in-context machine translation has been shown to be effective for on-the-fly adaptation [ 79]. For dialogue response generation tasks, employing 2 retrieval as an intermediate step has proven advantageous for generating informative responses [ 89, 91,6,7]. In-context learning example retrieval also aids in controllable dialogue [ 46]. Other applications include abstractive summarization [ 64,14,18,15], code generation [ 30], paraphrase generation [ 34,83], language modeling [ 36,105], counterfactual data generation [ 24], open domain question answering [12, 33] and semantic parsing [99]. 2.2 Neural Text Reranking By alleviating the discrepancy between training and inference (i.e., exposure bias) and directly optimizing desired metrics, two-stage reranking methods have facilitated significant progress in various text generation tasks. In machine translation, pioneering works by [ 75] and [ 61] introduced and popularized discriminative reranking for SMT. In the context of NMT, research has focused on two primary reranking approaches: generative reranking [ 56,32,88] and discriminative reranking [ 39, 71,23]. For syntactic parsing, [ 21] were the first to employ a two-stage reranking method to select outputs from a base parser, while [ 11] introduced a maximum entropy reranker. In text summarization, RefSum [ 53] proposed a second-stage summarization framework to address train-test distribution mismatches. SimCLS [ 54] used pairwise Learning To Rank (LTR) to select candidates with the highest matching scores. SummaReranker [ 68] adopted a multi-task framework to leverage different metrics capturing various aspects of generated candidates. BRIO [ 55] reused the base model for a second round of fine-tuning with both cross-entropy loss and a ranking loss. JGR [76] employed an alternate training paradigm to train the generator and reranker. A key limitation of these reranking methods is that they only represent a one-way process, wherein the selected candidates become the system’s final output. In contrast, our framework innovatively utilizes the chosen candidates as memory for the subsequent generation round of a generator, which can produce better candidates with enhanced memory. 3 Methods In this section, we begin with a motivating experiment on generation as memory (§ 3.1). Then, we introduce Selfmem , a framework comprising a generator (§ 3.2) and a memory selector (§ 3.3). The complete framework and algorithm are illustrated in Figure 2 and Algorithm 1. 3.1 Generation as Memory The primary motivation behind our framework stems from the observation that the memory, which is more similar in distribution to the data during inference, is not the training data (38.89 BLEU, as shown in the first row of Table 1). Instead, it is the model’s own output (58.58 BLEU) within the unbounded generation space. One interesting exploration involves directly utilizing the generated output as memory in relation to the primal problem : better memory prompts better generation. Table 1: Experiments on the relation between mem- ory quality and the final hypothesis quality, measured by the BLEU score with ground truth translation. The translator keeps fixed while the memory is obtained from different sources. Memory Source Memory Quality Hypothesis Quality Retrieval 38.89 58.58 Beam 58.58 58.43 Reference 100 90.43 Random 1.14 49.08We conduct experiments on the JRC-Acquis En→De dataset. The first row in Table 1 represents conventional training with retrieved memory and achieves a 58.58 BLEU score. However, directly in- corporating beam output of this trained model as memory (Beam) back into the generation model does not yield any improvements (row 2), despite its higher similarity to the reference compared to the retrieved ones. We hypoth- esize two potential reasons for this: (1) the generator may not gen- eralize effectively in this context due to the memory distribution shift (from 38.89 to 58.58), and (2) the beam memory does not offer any information gain compared to the retrieved one, even it exhibits more overlap with the references. To investigate the first hypothesis, we conduct experiments under the oracle and random scenarios by using the reference as memory (Reference) and randomly sampled sentences as memory (Random). The result is shown in Table 1 and it illustrates that a generator (trained with 3 Target Distribution Frozen LLM / T rainable LM NLL LossKL Loss YN Y1 ... Y X Y X candidates source target training memory ... ... ... ... (a) Generator (b) Memory Distribution M 2: Overall framework. There are two components in Selfmem , a genera- tor (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to generate candidates for (b). For the dual problem, (b) takes as input source and generated candidates to select memory for (a). retrieved memory) has already learned to discriminate between different memories in both oracle and random scenarios, without updating the model weights. To evaluate the second conjecture, we first define the token sets of the reference, retrieved memory, and beam memory as R,M, andB, respectively. The overlap token set, denoted by O, is defined as the tokens that overlap with the references in the beam memory but not in the retrieved memory, which is represented as R ∩ B − R ∩ M .Ois considered as the additional information provided by the beam memory. Inspired by the confidence analysis of NMT model [ 58], we compute the set confidence score, ψ(·), as follows: ψ(·) =1 | · |X yi∈·p(yi|x, y<i) (1) where p(yi|x, y<i)is defined by the generation model. ψ(·)measures the confidence with which the generation model generates the tokens. The value of ψ(R)is 0.58, while that of Ois 0.76, indicating that the generator is relatively confident in generating tokens in O, and therefore does not need to resort to external memory [ 38]. Beam search ranks generated candidates based on p(y|x), where the selected memory falls within the confidence region of the generator and consequently provides no information gain. This observation motivates us to select memory according to metrics other than p(y|x)in the memory selector (§3.3). 3.2 Generator Given a text pair (x, y), where x={x1, ...,x|x|}is the source, y={y1, ...,y|y|}is the target. They could be (document, summary) in summarization, (context, response) in dialogue generation or (source, target) in machine translation. The generation would first use xto retrieve memory mfrom datastore D. Then the generator Gξ(x, m), parameterized by ξ, would take bothxandmas input to generate the target sentence y. In this paper, following standard practice, we choose the training set as D={(xi, yi)}|D| i=1. For LLM as Gξ, we use the standard in-context learning format to give (x, y)as demonstration example. For tunable generator Gξ, we only keep the target side of top-1 retrieval results as memory and we consider two commonly used architectures: Joint-Encoder [29, 87, 41] and Dual-Encoder [92, 8, 17]. Joint-Encoder This architecture is the standard model [ 3,84]. The input is the concatenation of xandm. The encoder would first map the input into the hidden states H: H=Encoder (x[SEP] m) (2) 4 And the decoder would incorporate Hby attention mechanism and generate tokens in an auto- regressive manner: hi=Decoder (CrossAttn (H), y<i)PGξ(·|x, y<i) =Softmax (hi) (3) Dual-Encoder Instead of treating xandmas a long sequence, this architecture has two encoders, one for xand the other for m. Their outputs are sequentially attended by the decoder with dual cross attention as in [17]: ( (m) (4) hi=Decoder (CrossAttn (Hx, Hm), y<i) (5) We use Transformer [ 84] as the building block for both architectures and optimize Gξwith NLL loss: Lnll=−|y|X t=1logPGξ(yt|x, m, y <t) (6) 3.3 Memory Selector The role of memory selector Sθ(x, c), parameterized by θ, is to select one candidate cfrom the candidate pool Cgenerated by Gξbased on a specific metric ∆(·,·). The chosen candidate cis then utilized as memory mfor the subsequent generation round of Gξ. As discussed in §3.1, using pGξ(y|x)as the metric ∆(·,·)would result in falling into the confidence region of Gξ, leading to no information gain. Moreover, a larger value of pGξ(y|x)does not necessarily guarantee improved generation quality [ 59]. Consequently, we define ∆(·,·)as model-free metrics that are widely employed for assessing generation quality, such as BLEU for Neural Machine Translation (NMT) and ROUGE for Summarization. Our memory selector takes the concatenation of the source xand candidate cias input, and produces a multinomial distribution pSθ(·|x)overC. In this paper, we focus on the role of the memory selector, Sθ(x, c), which is parameterized by θ. The objective of this selector is to choose a single candidate cfrom the candidate pool C, generated byGξ, based on a specific metric, ∆(·,·). pSθ(ci|x) =exp(Sθ(x[SEP] ci)) P|C| ] cj))(7) In accordance with [ 39], the training goal for Sθis to minimize the discrepancy between the Sθ’s predictions and the scores determined by ∆(·,·). This divergence is quantified using the Kullback- Leibler (KL) divergence. Lkl=−|C|X pM(ci) =exp(∆(ci, y)/τ) P|C| j=1exp(∆(cj, y)/τ)(8) τis the temperature to control the smoothness of the distribution. At inference, the output of the Sθ isarg max ci∈CpSθ(ci|x). 3.4 Combine Generator and Selector We define two generation modes for Gξ. The first mode, referred to as the hypothesis mode , generates a single output for each input, which is utilized for system evaluation. The second mode, known as thecandidate mode , produces N outputs for a given input, and is employed for training Sθas well as memory selection. By integrating two modes together, we present the complete framework of our proposed model, Selfmem , as illustrated in Algorithm 1. 4 Experimental Setup 4.1 Dataset We assess the performance of Selfmem on three generation tasks, utilizing a total of seven datasets. Translation. We evaluate our framework on JRC-Acquis datasets [ 82], a collection of parallel 5 Algorithm 1 Selfmem Framework Require: a dataset D, a retriever R, a memory selection metric ∆(·,·), a generator Gξ, and a memory selector Sθ 1:retrieve memory MinDwithR not LLM) 3:useGξto generate candidate pool CwithMin candidate mode ∆(·,·) 5:while not converged in the validation set do 6:Sθselects memory from CasM 7:Gξgenerates candidate pool CwithMin candidate mode 8:end while 9:Gξgenerates the final hypothesis with Min hypothesis mode legislative text of European Union Law. It is the benchmark dataset used in translation memory- augmented NMT task [ 28,92,8,17]. We choose 4 translation directions, namely, Spanish ↔English (Es↔En), German ↔English (De ↔. We evaluate on 2 summarization datasets: 1)XSum [60], extreme summarization, a summarization dataset with highly abstrac- tive articles from British Broadcasting Corporation. 2) BigPatent [73], consisting of 1.3 million records of U.S. patent documents along with human-written abstractive summaries. Dialogue. We experiment on DailyDialog [44], which contains multi-turn dialogs on daily life topics and is used by [13, 4, 103]. The detailed statistics for these datasets can be found in the Appendix A. 4.2 Implementation Details We utilize the BM25 algorithm [ 70] for retrieval purposes. For all tasks, the candidate generation method consists of beam search with a beam width of 50. The number of iterations is determined by the performance on the validation set. For translation , we follow the approach of [ 93,8,17], employing a randomly initialized Transformer as Gξfor trainable small model and XGLM [ 48] for LLM in-context learning. Evaluation metrics include BLEU, TER, and chrF++ obtained from SACRE BLEU [66]. The memory selector Sθutilizes an XLM-R base[22] as back- bone, with BLEU serving as ∆(·,·).For summarization , we initialize Gξwith BART base[40] forBigPatent and employ BRIO [ 55] for XSum . The evaluation metric comprises ROUGE (R- 1/2/L) [ 47].For dialogue generation , BART baseserves as the backbone for Gξ. Our dialogue system is evaluated using BLEU (B-1/2) and Distinct (D-1/2) scores [ 43]. For both dialogue and summariza- tion tasks, we adhere to the methods of [ 54,26], adopting RoBERTa base[52] as the backbone for Sθ. The linear combination of B-1/2 is chosen as ∆(·,·)for Dialogue Generation, while R-1/2/L is used for Summarization, following [ 76]. For further implementation details, please refer to the Appendix B and Appendix C for evaluation metrics. 5 Experimental Results 5.1 Machine Translation We select four translation directions and experiment with two generation paradigms: trainable small models and few-shot prompted LLMs [ 85,20]. For trainable models, we explore two architectures (joint and dual, as detailed in §3.2). The baselines comprise two types of translation systems: one being the vanilla model [ 3,84] without memory augmentation, and the other consisting of translation models focusing on memory encoding [ 28,92], memory construction [ 101], memory retrieval [ 8], and memory diversity [ 17]. Based on the experimental results2shown in Table 2, Selfmem significantly enhances the performance of Gξacross four translation datasets and two different architectures. This is noteworthy, given that the parameters of theGξremain fixed, with the only variable being the input memory. This finding is consistent with theprimal problem which posits that improved memory typically leads to better generation results. 2As higher BLEU scores in this range do not necessarily guarantee a superior translation system [ 9], we also evaluate our system using TER and chrF++. The results can be found in the Appendix D. 6 Table 2: Results of translation task on JRC-Acquis measured by BLEU. Models denoted by the same symbol ( ⋆and†) have the same parameters and only differ in memory as input. The bolded numbers show the SOTA performance and the underlined numbers show the second-best result. ∗ denotes the system is significantly better than baselines with p-value < 0.05 tested by [37]. SystemEs→En En→Es De→En En→De Dev Test Dev Test Dev Test Dev Test None Memory RNNsearch [3] 55.02 59.34 50.54 50.48 50.20 49.74 44.94 43.98 Transformer [84] 64.08 64.63 62.02 61.80 60.18 60.16 54.65 55.43 Retrieval Memory SEG-NMT [28] 60.28 59.34 57.62 57.27 55.63 55.33 49.26 48.80 NMT-pieces [101] 63.97 64.30 61.50 61.56 60.10 60.26 55.54 55.14 G-TFM [92] 66.37 66.21 62.50 62.76 61.85 61.72 57.43 56.88 MonoNMT [8] 67.73 67.42 64.18 63.86 64.48 64.62 58.77 58.42 CMM [17] 67.48 67.76 63.84 64.04 64.22 64.33 58.94 58.69 Transformer dual⋆ 66.87 67.12 63.14 63.54 64.09 63.36 58.69 58.06 Transformer uni† 67.74 67.32 63.93 64.12 64.50 64.40 58.16 58.58 Self-Memory Transformer dual⋆ ∗ Transformer uni† ∗ Table 3: Comparison between retrieval memory and self-memory. The quality of memory and hypothesis is measured by the n-gram overlap with reference (BLEU). All experiments are conducted with Transformer . Retrieval Self memory hypothesis memory hypothesis En-De→ 38.89 58.58 57.92 60.11 ← 42.56 64.40 64.32 65.65 En-Es→ 40.67 64.12 63.57 65.94 ← 43.05 67.32 67.78 68.80 Thedual problem is revealed in Table 3. Self-memory, which essentially represents the model’s own output, exhibits greater similarity with the ground truth and serves as a more effective memory for generating the final output. This observation highlights a key distinction between Selfmem and previous reranking works [ 39,68]. Reranking aims to select candidates of higher quality than the beam output, whereas in Selfmem , the chosen candidates serve as memory for the generator and do not necessarily need to surpass the quality of the beam hypotheses. Table 4: Evaluation results of in-context learning with self-memory. XGLM-1.7B XGLM-4.5B XGLM-7.5B Random kNN Self Random kNN Self Random kNN Self En-De→ 11.51 37.87 40.94 17.51 37.60 38.25 18.48 47.82 48.32 ← 27.42 51.00 51.88 30.62 48.12 48.36 33.03 55.65 55.12 En-Es→ 23.87 46.20 48.56 31.83 48.37 49.17 29.97 53.86 54.32 ← 25.29 51.55 53.13 32.16 48.55 49.22 35.22 57.25 57.56 In Table 4, we present the results of LLM with self-memory. We employ XGLM [ 48] as our backbone generator, with three different sizes ranging from 1.7B to 7.5B. We utilize the recommended prompt as described in [ 48]. We select three in-context learning examples and report the average scores from three separate runs, taking into account the sensitivity of example selection in ICL [ 49]. From the table, we first observe a general trend where few-shot translation performance improves as the 7 size of the model increases. Furthermore, we find that more similar translation demonstrations significantly enhance performance across all model sizes (from random, kNN to Self). This suggests that demonstration examples in in-context learning not only act as triggers for model ability but also adhere to the primal problem , where better demonstration example leads to better generation. Also, by comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with designed examples still falls short of the supervised baselines in this task. 5.2 Summarization In this paper, we compare the performance of our trainable model with those of REINA [ 87], PEGASUS [ 100], and BART [ 40]. The results are presented in Table5. Initially, it can be observed that memory has varying impacts on different datasets. The enhancement brought by memory in the BigPatent dataset is significantly larger than that in the XSum dataset. This can be attributed to the inherent of the BigPatent dataset, which consists of official patent documents that exhibit considerable similarity. Consequently, this greatly improves the summarization quality in accordance with the primal problem . Furthermore, we discovered that self-memory substantially enhances the performance of both BRIO (+1.2 R1) and BART (+18.5 R1), achieving results on both datasets. We selected these baselines for a fair comparison, as they share the same base generator. Due to space constraints, additional comparisons and the confidence region of the SOTA model can be found in the Appendix E. Table 5: Results of summarization task on XSum andBigPatent measured by ROUGE. System Memory R-1 R-2 R-L XSum PEGASUS None 47.2 24.6 39.3 BRIO None 49.1 25.6 40.4 REINA (PG) Retrieval 48.2 26.0 40.2 REINA (B) Retrieval 43.2 21.0 35.5 REINA (L) Retrieval 46.5 24.1 38.6 BRIO dual⋆ Retrieval 48.6 26.1 40.6 BRIO joint† Retrieval 49.5 26.5 41.2 BRIO dual⋆ Self 49.2 26.2 40.8 BRIO joint† Self 50.3 26.7 41.6System Memory R-1 R-2 R-L BigPatent PEGASUS None 53.6 33.2 43.2 BART None 44.4 21.3 31.0 REINA (B) Retrieval 59.5 42.6 50.6 REINA (L) Retrieval 60.7 43.3 51.3 REINA (PG) Retrieval 44.6 21.5 33.3 BART dual⋆ Retrieval 57.4 43.3 49.7 BART joint† Retrieval 59.6 43.4 51.0 BART dual⋆ Self 61.2 44.6 52.3 BART joint† Self 62.9 48.1 59.6 5.3 Dialogue Generation As demonstrated in Table 6, the self-memory significantly enhances the performance of the retrieval- augmented generator for dialogue generation tasks. By optimizing memory using BLEU as ∆(·,·), the self-memory improves the B-1,2 score over retrieved memory by 3.08 B-1 and 0.6 B-2 on BART joint. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in D-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when evaluating a dialogue system [ 104]. To address this issue, we opt for D-1,2 as ∆(·,·)when optimizing Sθ, denoted as BART joint†(D). The results in Table 6 highlight the remarkable flexibility of Selfmem by directly optimizing memory to achieve the desired attributes for diverse and informative dialogue. 6 Further Analysis To gain a deeper insight into Selfmem , we first examine the impact of each key component, namely Gξ andSθ. Subsequently, we perform a detailed token-level analysis of the generated output concerning their frequency in the training set. Experiments are conducted on the JRC-Acquis En→De dataset. We also include latency analysis and human evaluation on Appendix F and G. Tuning SθWe explored various Sθby direct selection from the candidate pool based on gold rankings. As shown in Figure 3a, both architectures with enhanced outperform the current SOTA performance (60.11 BLEU). Moreover, we assessed the candidate pool quality during this iterative process using an oracle Sθ, as displayed in Figure 3b. A clear pattern emerges 8 Table 6: Results of dialogue generation task on DailyDialog measured by B-1/2 and D-1/2. BART joint(D) denotes the metric ∆(·,·)forSθis the average of D-1 and D-2. System Memory B-1 B-2 D-1 D-2 NCM [86] None 33.60 26.80 3.00 12.80 iV AE [25] None 30.90 24.90 2.90 25.00 PLATO-2 [5] None 34.80 25.12 3.54 25.11 DialoFlow [45] None 36.17 27.67 4.56 27.12 BART None 20.72 11.36 3.92 19.44 BART dual⋆ Retrieval 29.50 21.89 4.74 26.01 BART joint† Retrieval 36.72 31.55 6.13 35.65 BART dual⋆ Self 33.43 22.85 4.66 26.16 BART joint† Self 39.80 32.15 5.84 32.16 BART joint†(D) Self 36.92 32.09 9.12 37.05 1 2 3 4 BLEU rank1 rank2 rank3rank4 rank5 rank6 1 2 3 4 rank1 rank2 rank3rank4 rank5 rank6 (a) Hypothesis 1 2 3 4 5 BLEU (b) Candidates Figure 3: (a) shows generation quality in the iteration process with different Sθin both trainable generator architectures. (b) shows candidates quality in the iteration process with an oracle Sθ. in this boxplot, revealing improvements in the oracle ,quartile ,average , and minimum scores of the candidate pool. These two experiments jointly clarify the Selfmem ’s underlying intuition: a generator profits from superior memory, which can be chosen from its own unbounded output, and subsequently, the generator with improved memory produces a higher-quality candidate pool for the next selection round. Consequently, the model lift itself up. The Least 20% 40% 60% 80% The Most Frequency Retrieval None Figure 4: 1-gram F1 score sorted by training corpus GξAs discussed in §3.1, we demonstrated that a trained generator, with fixed parameters, possesses the ability to distinguish between "good" and "bad" memory. This observation not only justifies our decision to maintain a fixed generator within our framework but also im- plies that the Gξis not the current bottleneck of the Selfmem . Frequency Analysis We conduct a comprehensive token- level analysis by computing the 1-gram F1 scores for generated translations and subsequently categorizing the tokens based on their frequency in the training set. The results are depicted in Figure 4. A noticeable pattern emerges, suggesting that the more frequently a model encounters a token during training, the higher the accuracy of the generated output [ 102]. Moreover, our findings indicate that retrieval- augmented models, particularly those incorporating self-memory augmentation, exhibit superior performance in handling long-tail inputs which are challenges for parametric models [67, 57]. 9 7 Conclusion For the first time, we investigate the fundamental limitation of bounded memory in the current literature. We combine the primal anddual problems together and propose Selfmem , a general framework for text generation by uplifting generation model with its own output. We conduct comprehensive experiments across various text generation tasks and different generation paradigms, including trainable small model and few-shot prompted LLM. We surpass strong baselines and improve the performance in serval datasets. We also meticulously investigate each crucial component and pinpoint the existing system bottleneck to guide future research endeavors. Limitations We discuss the limitations of our framework as follows: (1) Although Selfmem greatly improves the generation quality compared with other retrieval- augmented generation models, it requires more computational resources with respect to the memory selection process. For large dataset with long context (e.g., BigPatent ), it would become a more crucial problem considering the quadratic time complexity of transformer architecture. (2) This paper proposes a general idea for the generation. But we only experiment with architecture for both generator and memory selector and the architecture of generator and memory selector keeps the same across all text generation tasks. We believe the task-specific design for the model architecture, training objective and generation methods in different text generation scenarios would further improve the performance. This work was supported by the National Key Research and Development Program of China () and National Natural Science Foundation of China (NSFC Grant No.62122089). We appreciate the anonymous reviewers for their helpful comments. Dongyan Zhao and Rui Yan are the corresponding authors.  
machine learning detects terminal singularities 	Machine learning detects terminal singularities Tom Coates Department of Mathematics Imperial College London 180 Queen’s Gate London, SW7 2AZ UK M. Kasprzyk School of Mathematical Sciences University of Nottingham Nottingham, NG7 2RD UK Sara Veneziale∗ Department of Mathematics Imperial College London 180 Queen’s Gate London, SW7 2AZ UK Abstract Algebraic varieties are the geometric shapes deﬁned by systems of polynomial equations; they are ubiquitous across mathematics and science. Amongst these algebraic varieties are Q-Fano varieties: positively curved shapes which have Q-factorial terminal singularities. Q-Fano varieties are of fundamental importance in geometry as they are ‘atomic pieces’ of more complex shapes – the process of breaking a shape into simpler pieces in this sense is called the Minimal Model Programme. Despite their importance, the classiﬁcation of Q-Fano varieties remains unknown. In this paper we demonstrate that machine learning can be used to understand this classiﬁcation. We focus on algebraic varieties that have toric symmetry and Picard rank two, and develop a neural network classiﬁer that predicts with 95% accuracy whether or not such an algebraic variety isQ-Fano. We use this to give a ﬁrst sketch of the landscape of Q-Fano varieties in dimension eight. How the neural network is able to detect Q-Fano varieties with such accuracy remains mysterious, and hints at some deep mathematical theory waiting to be uncovered. Furthermore, when visualised using the quantum period, an invariant that has played an important role in recent theoretical developments, we observe that the classiﬁcation as revealed by ML appears to fall within a bounded region, and is stratiﬁed by the Fano index. This suggests that it may be possible to state and prove conjectures on completeness in the future. Inspired by the ML analysis, we formulate and prove a new global combinatorial criterion for a positively curved toric variety of Picard rank two to have terminal singularities. Together with the ﬁrst sketch of the landscape of Q-Fano varieties in higher dimensions, this gives strong new evidence that machine learning can be an essential tool in developing mathematical conjectures and accelerating theoretical discovery. ∗Corresponding author. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). 1 Introduction Systems of polynomial equations occur throughout mathematics and science; see e.g. [ 3,17,18,35]. Solutions of these systems deﬁne shapes called algebraic varieties . Depending on the equations involved, algebraic varieties can be smooth (as in Figure 1a) or have singularities (as in Figures 1b and 1c). In this paper we show that machine learning methods can detect a class of singularities called terminal singularities . (a)x2+y2=z2+ 1 (b)x2+y2=z2 (c)x2+y2=z3 Figure 1: Algebraic varieties in R3with different deﬁning equations. A key class of algebraic varieties are Fano varieties : positively curved shapes that are basic building blocks in algebraic geometry. Fano varieties are ‘atomic pieces’ of more complex shapes, in the sense of the Minimal Model Programme [ 7,25,26]. Running the Minimal Model Programme – that is, breaking an algebraic variety Xinto atomic pieces – involves making birational ofX. These are modiﬁcations on subsets with zero volume (and codimension at least one), and can either introduce or remove singularities. The building blocks that emerge from this process are not necessarily smooth: they satisfy a weaker condition called can have mild singularities called terminal singularities [ 38]. Fano varieties that are Q-factorial and have terminal singularities are called Q-Fano varieties . The classiﬁcation of Q-Fano varieties is therefore a long-standing problem of great importance [ 4, 15,27,33,34] – one can think of this as building a Periodic Table for geometry. But, despite more than a century of study, very little is known. In what follows we exploit the fact that machine learning can detect terminal singularities to give the ﬁrst sketch of part of the classiﬁcation of Q-Fano varieties. We probe the classiﬁcation of Q-Fano varieties using a class of shapes called toric varieties . (For example, the algebraic varieties pictured in Figure 1 are toric varieties.) Toric varieties are particularly suitable for computation and machine learning, because their geometric properties are encoded by simple combinatorial objects. We consider Fano toric varieties of Picard rank two. These can be encoded using a 2×Nmatrix of non-negative integers called the weight matrix ; here the dimension of the toric variety is N−2. To determine whether such a toric variety Xis aQ-Fano variety we need to check whether Xis Q-factorial, and whether the singularities of Xare terminal. Checking Q-factoriality from the weight matrix ofXturns out to be (see §3) but checking terminality is extremely challenging. This is because there is no satisfactory theoretical understanding of the problem. We lack a global criterion for detecting terminality in terms of weight data (such as [ 24] in a simpler setting) and so have to fall back on ﬁrst enumerating all the singularities to analyse, and then checking terminality for each singularity. Each step is a challenging problem in discrete geometry: the ﬁrst step involves building a different combinatorial object associated to the n-dimensional toric variety X, which is a collection of cones in Rncalled the fanΣ(X); the second step involves checking for various cones in the fan whether or not they contain lattice points on or below a certain hyperplane. To give a sense of the difﬁculty of the computations involved, generating and our dataset of 10 million toric varieties in dimension eight took around 30 CPU years. 2An algebraic variety XisQ-factorial if it is normal and, in addition, for each rank-one reﬂexive sheaf E onX, some tensor power of Eis a line bundle. This implies that the dimension of the singular locus in Xis at most dimX−2, and that some tensor power of the canonical sheaf (of top-degree differential forms) is a line bundle. 2 To overcome this difﬁculty, and hence to begin to investigate the classiﬁcation of Q-Fano varieties in dimension eight, we used supervised machine learning. We trained a feed-forward neural network classiﬁer on a balanced dataset of 5 million examples; these are Q-factorial Fano toric varieties of Picard rank two, of which 2.5 million are terminal and 2.5 million non-terminal. Testing on a further balanced dataset of 5 million examples showed that the neural network classiﬁes such toric varieties as terminal or non-terminal with an accuracy of 95%. This high accuracy allowed us to rapidly generate many additional examples that are with high probability Q-Fano varieties – that is, examples that the classiﬁer predicts have terminal singularities. This ML-assisted generation step is much more efﬁcient: generating 100 million examples in dimension eight took less than 120 CPU hours. The fact that the ML classiﬁer can detect terminal singularities with such high accuracy suggests that there is new mathematics waiting to be discovered here – there should be a simple criterion in terms of the weight matrix to determine whether or not a toric variety Xhas terminal singularities. In §5 we take the ﬁrst steps in this direction, giving in Algorithm 1 a new method to check terminality directly from the weight matrix, for toric varieties of Picard rank two. A proof of correctness is given in the Supplementary Material. This new algorithm is ﬁfteen times faster than the naïve approach that we used to generate our labelled dataset, but still several orders of magnitude slower than the neural network classiﬁer. We believe that this is not the end of the story, and that the ML results suggest that a simpler criterion exists. Note that the neural network classiﬁer cannot be doing anything analogous to Algorithm 1: the algorithm relies on divisibility relations between entries of the weight matrix (GCDs etc.) that are not visible to the neural network, as they are destroyed by the rescaling and that is applied to the weights before they are fed to the classiﬁer. In §6 we use the ML-assisted dataset of 100 million examples to begin to explore the classiﬁcation of Q-Fano varieties in dimension eight. We visualise the dataset using the regularized quantum period , an invariant that has played an important role in recent theoretical work on Q-Fano classiﬁcation, discovering that an appropriate projection of the data appears to ﬁll out a wedge-shaped region bounded by two straight lines. This visualisation suggests some simple patterns in the classiﬁcation: for example, the distance from one edge of the wedge appears to be determined by the Fano index of the variety. Our work is further evidence that machine learning can be an indispensable tool for generating and guiding mathematical understanding. The neural network classiﬁer led directly to Algorithm 1, a new theoretical result, by revealing that the classiﬁcation problem was tractable and thus there was probably new mathematics waiting to be found. This is part of a new wave of application of artiﬁcial intelligence to pure mathematics [ ], where machine learning methods drive theorem discovery. A genuinely novel contribution here, though, is the use of machine learning for data generation and data exploration in pure mathematics. Sketching the landscape of Q-Fano varieties using traditional methods would be impossible with the current theoretical understanding, and prohibitively expensive using the current exact algorithms. Training a neural network classiﬁer however, allows us to explore this landscape easily – a landscape that is unreachable with current mathematical tools. Why dimension eight? We chose to work with varieties for several reasons. It is important to distance ourselves from the surface case (dimension two), where terminality is a trivial condition. A algebraic variety has terminal singularities if and only if it is smooth. On the other hand, we should consider a dimension where we can generate a sufﬁcient amount of data for machine learning (the analogue of our dataset in dimension three, for example, contains only 34 examples [ 23]) and where we can generate enough data to meaningfully probe the classiﬁcation. Moreover, we work in Picard rank two because there already exists a fast combinatorial formula to check terminality in rank one [24]; Picard rank two is the next natural case to consider. Data and code availability The datasets underlying this work and the code used to generate them are available from Zenodo under a CC0 license [ 11]. Data generation and was carried out using the computational algebra system Magma V2.27-3 [ 5]. The machine learning model was built using PyTorch v1.13.1 [ 36] and scikit-learn v1.1.3 [ 37]. All code used and trained models are available from BitBucket under an MIT licence [12]. 3 2 Mathematical background The prototypical example of a Fano variety is projective space PN−1, which can be thought of as the quotient of as follows: λ·(z1,...,z N) = (λz1,...,λz N) Fano toric varieties of Picard rank two arise similarly. They can be constructed as the quotient ofCN\S, whereSis a union of subspaces, by an action of (C×)2. This action, and the union of subspacesS, is encoded by a weight matrix: [ a1···aN b1···bN] (2.1) Here we assume that all ( in a strictly convex cone C⊂R2. The action is (λ,µ)·(z1,...,z N) = () andS=S+∪S−is the union of subspaces S+andS−, where S+={(z1,...,z N)|zi= 0ifbi/ai>b/a} S−={(z1,...,z N)|zi= ) anda=∑N i=1ai,b=∑N i=1bi: see [ 6]. The quotient X= (CN\S)/(C×)2is an algebraic variety of dimension N−2. We assume in addition that both S+andS−have dimension at least two; this implies that the second Betti number of Xis two, that is, Xhas Picard rank two. Since we have insisted that all columns (ai,bi)lie in a strictly convex cone C, we can always permute columns and apply an to the weight matrix to obtain a matrix in standard form: [ a1a2···aN 0b2···bN] (2.3) where all entries are non-negative, the columns are cyclically ordered anticlockwise, and aN<bN. This transformation corresponds to renumbering the co-ordinates of CNand the torus (C×)2that acts, and consequently leaves the quotient variety Xthat we construct unchanged. We will consider weight matrices (2.1) that satisfy an additional condition called being well-formed . Anr×Nweight matrix is called standard if the greatest common divisor of its r×rminors is one, and is well-formed if every submatrix formed by deleting a column is standard [ 2]. Considering only well-formed weight matrices guarantees that a toric variety determines and is determined by its weight matrix, uniquely up to . Testing terminality As mentioned in the introduction, an n-dimensional toric variety Xdetermines a collection Σ(X)of cones in Rncalled the fan of X. A toric variety is completely determined by its fan. The process of determining the fan Σ(X)from the weight matrix (2.1) is explained in the Supplementary Material; this is a challenging combinatorial calculation. In the fan Σ(X), the cones are called rays. For a Fano toric variety X, taking the convex hull of the ﬁrst lattice point on each ray deﬁnes a convex polytope P, andXhas terminal singularities if and only if the only lattice points in Pare the origin and the vertices. Verifying this is a conceptually but challenging calculation in integer linear programming. 3 Data generation We generated a balanced, labelled dataset of ten million Q-factorial Fano toric varieties of Picard rank two and dimension eight. These varieties are encoded, as described above, by weight matrices. We generated matrices in standard form, as in (2.3) , with entries chosen uniformly at random from the set {0,..., 7}. Minor exceptions to this were the values for a1andbN, which were both chosen uniformly at random from the set {1,..., 7}, and the value for aN, which was chosen uniformly at random from the set {0,...,b N−1}. Once a random weight matrix was generated, we retained it only if it satisﬁed: 4 Table 1: Final network architecture and conﬁguration. Hyperparameter Value Hyperparameter Value Layers (512,768,512) Momentum 0.99 Batch size 128 LeakyRelu slope 0.01 Initial learning rate 0.01 1. None of the columns are the zero vector. 2. The sum of the columns is not a multiple of any of them. 3. The subspaces S+andS−in (2.2) are both of dimension at least two. 4. The matrix is well-formed. The ﬁrst condition here was part of our deﬁnition of weight matrix; the second condition is equivalent ; the third condition guarantees that Xhas Picard rank two; and the fourth condition was discussed above. We used rejection sampling to ensure that the dataset contains an equal number of terminal and non-terminal examples. Before generating any weight matrix, a boolean value was set to True (terminal) or False (non-terminal). Once a random weight matrix that satisﬁed conditions (1)–(4) above was generated, we checked if the corresponding toric variety was terminal using the method discussed in §2. If the terminality check agreed with the chosen boolean, the weight matrix was added to our dataset; otherwise the generation step was repeated until a match was found. As discussed, different weight matrices can give rise to the same toric variety. Up to isomorphism, however, a toric variety Xis determined by the isomorphism class of its fan. We deduplicated our dataset by placing the corresponding fan Σ(X), which we had already computed in order to test for terminality, in normal form [19, 29]. In practice, very few duplicates occurred. 4 Building the machine learning model We built a neural network classiﬁer to determine whether a Q-factorial Fano variety of Picard rank two and dimension eight is terminal. The network was trained on the features given by concatenating the two rows of a weight matrix, [a1,...,a 10,b1,...,b 10]. The features were standardised by translating their mean to zero and scaling to variance one. The network, a multilayer perceptron, is a fully connected feedforward neural network with three hidden layers and leaky ReLu activation function. It was trained on the dataset described in §3 using binary cross-entropy as loss function, stochastic mini-batch gradient descent optimiser and using early-stopping, for a maximum of 150 epochs and with learning rate reduction on plateaux. We tested the model on a balanced subset of 50% of the data (5M); the remainder was used for training (40%; 4M balanced) and validation (10%; 1M). Hyperparameter tuning was partly carried out using RayTune [ 31] on a small portion of the training data, via random grid search with Async Successive Halving Algorithm (ASHA) scheduler [ 30], for 100 experiments. Given the best conﬁguration resulting from the random grid search, we then manually explored nearby conﬁgurations and took the best performing one. The ﬁnal best network conﬁguration is summarised in Table 1. By trying different train-test splits, and using 20% of the training data for validation throughout, we obtained the learning curve in Figure 2a. This shows that a split of 4M-1M-5M produced an accurate model that did not overﬁt. Training this model gave the loss learning curve in Figure 2b, and a ﬁnal accuracy (on the test split of size 5M) of 95%. 5 Theoretical result The high accuracy of the model in §4 was very surprising. As explained in the introduction, Q-Fano varieties are of fundamental importance in algebraic geometry. However, asking whether a Fano variety has terminal singularities is, in general, an extremely challenging geometric question. In the case of a Fano toric variety one would typically proceed by constructing the fan, and then performing 5 (a) (b) Figure 2: (a) Accuracy for different train-test splits; (b) epochs against loss for the network trained on 5M samples. a cone-by-cone analysis of the combinatorics. This is expensive and unsatisfying from a theoretical viewpoint. The success of the model suggested that a more direct is possible from the weight matrix alone. An analogous exists in the simpler case of weighted projective spaces [ 24], which have Picard rank one, however no such result in higher Picard rank was known prior to training this model. Inspired by this we prove a theoretical result, Proposition 5.3, which leads to a new algorithm for checking terminality directly from the weight matrix, for Q-factorial Fano toric varieties of Picard rank two. Consider a weight matrix as in (2.1) that satisﬁes conditions (1)–(4)from §3, and the toric variety Xthat it determines. As discussed in §2, and explained in detail in the Supplementary a convex polytope PinRN−2, withNvertices given by the ﬁrst lattice points on theNrays of the fan. Each of the vertices of Pis a lattice point (i.e., lies in ZN−2⊂RN−2), andXhas terminal singularities if and only if the only lattice points in Pare the vertices e1,...,e N and the origin. Deﬁnition 5.1. Let∆idenote the simplex in RN−2with vertices e1,..., ˆei,...,e Nwhereeiis omitted. We say that ∆iismostly empty if each lattice point in ∆iis either a vertex or the origin. Notation 5.2. Let{x}denote the fractional part x−⌊x⌋of a rational number x. Proposition 5.3. Consider a weight matrix [ a1···aN b1···bN] that satisﬁes conditions 1–4 from §3. Let gi= gcd{ai,bi}, and letAi,Bibe integers such that Aiai+ Bibi=gi. Set αj i=ajbi−bjai giαi=N∑ j=1αj i βj i=−Aiaj−Bibj βi=N∑ j=1βj i fi=αigi gcd{gi,βi} noting that all these quantities are integers. Then ∆iis mostly empty if and only if for all k∈ {0,...,f i−1}such that N∑ j=1{ kαj i fi+lβj i gi} = 1 we have that { kαj i fi+lβj i gi} ={ αj i αi} for allj. 6  > <0}, and letIbe eithers+ors−. Then ∆i, i∈I, forms a triangulation of P. ThusXhas terminal singularities if and only if ∆iis mostly empty for eachi∈I. This leads to Algorithm 1. Algorithm 1 Test terminality for weight matrix W= [[a1,...,a N],[b1,...,b N]]. 1:Seta=∑N i=1ai,b=∑N i=1bi. }. 3:SetIto be the smaller of s+ands−. 4:fori∈Ido 5: Test if ∆iis mostly empty, using Proposition 5.3. 6: if∆iis not mostly empty then 7: return False . 8: end if 9:end for 10:return True . Comparisons Testing on 100 000 examples indicates that Algorithm 1 is ap- proximately 15 times faster than the fan-based approach to checking terminality that we used when labelling our dataset (0.020s per weight matrix for Algorithm 1 versus 0.305s for the standard ap- proach implemented in Magma). On single examples, the neural network classiﬁer is approximately 30 times faster than Algorithm 1. The neural network also beneﬁts greatly from batching, whereas the other two algorithms do not: for batches of size 10 000, the neural network is roughly 2000 times faster than Algorithm 1. 6 The terminal toric Fano landscape Having trained the terminality classiﬁer, we used it to explore the landscape of Q-Fano toric varieties with Picard rank two. To do so, we built a large dataset of examples and analysed their regularized quantum period , a numerical invariant of Q-Fano varieties [ 8]. For smooth Fano varieties, it is known that the regularized quantum period is a complete invariant [ 9]. This is believed to be true in higher dimension, but is still conjectural. Given a Q-Fano variety X, its regularized quantum period is a power series ˆGX(t) =∞∑ d=0cdtd wherec0= 1,c1= 0,cd=d!rd, andrdis the number of degree- drational curves in Xthat satisfy certain geometric conditions. Formally speaking, rdis a degree-d, genus-zero Gromov–Witten invariant [ 28]. The period sequence ofXis the sequence (cd)of coefﬁcients of the regularized quantum period. This sequence grows rapidly. In the case where Xis aQ-Fano toric variety of Picard rank two, rigorous asymptotics for this growth are known. Theorem 6.1 (Theorem 5.2, [10]) .Consider a weight matrix [ a1... a N b1... b N] for aQ-factorial Fano toric variety Xof Picard rank two. Let a=∑N i=1aiandb=∑N i=1bi, and let[µ:ν]∈P1be the unique real root of the homogeneous polynomial N∏ ∏ ) such }. Let (cd)be the corresponding period sequence. Then non-zero coefﬁcients cdsatisfy logcd∼Ad−dimX 2logd+B 7 (a) (b) Figure 3: A dataset of 100M probably- Q-Fano toric varieties of Picard rank two and dimension eight, projected to R2using the growth coefﬁcients AandBfrom (6.2) . In (a) we colour by Fano index, while in (b) we colour a heatmap according to the frequency. asd→∞ , where A=−N∑ i=1pilogpi B=−dimX 2log(2π)−1 2N∑ i=1logpi−1 2log(N∑ i=1(aib−bia)2 ℓ2pi) (6.2) Herepi=µai+νbi µa+νb, so that∑ ipi= 1, andℓ= gcd{a,b}is the Fano index. In Figure 3 we picture our dataset of Q-Fano varieties by using the coefﬁcients AandBto project it toR2; for the corresponding images for terminal Fano weighted projective spaces, see [ 10, Figure 7a]. Note the stratiﬁcation by Fano index. Although many weight matrices can give rise to the same toric variety, in our context we are using well-formed weight matrices in standard form (2.3) and so at most two weight matrices can give rise to the same toric variety. We removed any such duplicates from our dataset, so the heatmap in Figure 3b reﬂects genuine variation in the distribution of Q-Fano varieties, rather than simply the many-to-one correspondence between weight matrices and toric varieties. Data generation The dataset pictured in Figure 3 was generated using an AI-assisted data genera- tion workﬂow that combines algorithmic checks and our machine learning model, as follows. •Generate a random 2×10matrix with entries chosen uniformly from {}. • Cyclically order the columns and only keep the matrix if it is in standard form, as in (2.3). • Check conditions (1)–(4) from §3. •Predict terminality using the neural network classiﬁer from §4, only keeping examples that are classiﬁed as terminal and storing their probabilities. •Setµ= 1in(6.1) and solve the univariate real polynomial in the correct domain to obtain the solution (1,ν). • Calculate the coefﬁcients AandBusing the formulae in (6.2). The ﬁnal dataset is composed of 100M samples. Each of these represents a Q-factorial toric Fano variety of dimension eight and Picard rank two that the classiﬁer predicts is a Q-Fano variety. Data analysis We note that the vertical boundary in Figure 3 is not a surprise. In fact, we can apply the log-sum inequality to the formula for Ato obtain A=−N∑ ∑ i=1pi) log(∑N i=1pi N) = log(N) 8 Figure 4: Confusion matrices for the neural network classiﬁer on in-sample and out-of-sample data. In each case a balanced set of 10 000 random examples was tested. In our caseN= 10 , and the vertical boundary that we see in Figure 3a is the line x= log(10)∼2.3. We also see what looks like a linear lower bound for the cluster; a similar bound was observed, and established rigorously, for weighted projective spaces in [10]. Closer analysis (see the Supplementary Material) reveals large overlapping clusters that correspond to Fano varieties of different Fano index. Furthermore the simplest toric varieties of Picard rank two – products of projective spaces, and products of weighted projective spaces – appear to lie in speciﬁc regions of the diagram. 7 Limitations and future directions The main message of this work is a new proposed AI-assisted workﬂow for data generation in pure mathematics. This allowed us to construct, for the ﬁrst time, an approximate landscape of objects of mathematical interest ( Q-Fano varieties) which is inaccessible by traditional methods. We hope that this methodology will have broad application, especially to other large-scale classiﬁcation questions in mathematics, of which there are many [1, 13, 21]. Our approach has some limitations, however, which we enumerate here. Some of these limitations suggest directions for future research. A key drawback, common to most ML models, is that our classiﬁer performs poorly on out-of-sample data. Recall from §3 that the dataset we generated bounded the entries of the matrices by seven. For weight matrices within this range the model is extremely accurate (95%), however this accuracy drops off rapidly for weight matrices that fall outside of this range: 62% for entries bounded by eight; 52% for entries bounded by nine; and 50% for entries bounded by ten. See Figure 4 for details. Note that the network quickly degenerates to always predicting non-terminal singularities. Furthermore the training process seems to require more data than we would like, given how computa- tionally expensive the training data is to generate. It is possible that a more sophisticated network architecture, that is better adapted to this speciﬁc problem, might require less data to train. Mathematically, our work here was limited to toric varieties, and furthermore only to toric varieties of Picard rank two. Finding a meaningful vectorisation of an arbitrary algebraic variety looks like an impossible task. But if one is interested in the classiﬁcation of algebraic varieties up to deformation, this might be less of a problem than it ﬁrst appears. Any smooth Fano variety in low dimensions is, up to deformation, either a toric variety, a toric complete intersection, or a quiver ﬂag zero locus [ 9,22]; one might hope that this also covers a substantial fraction of the Q-Fano landscape. Each of these classes of geometry is controlled by combinatorial structures, and it is possible to imagine a generalisation of our vectorisation by weight matrices to this broader context. Generalising to Q-factorial Fano toric varieties in higher Picard rank will require a more sophisticated approach to equivariant machine learning. In this paper, we could rely on the fact that there is a normal form (2.3) for rank-two weight matrices that gives an almost unique representative of each of weight matrices. For higher Picard rank rwe need to consider weight matrices up to the action of G= SL r(Z)×SN. Here no normal form is known, so to work we will need to augment our dataset, to ﬁll out the different G-orbits, or to use invariant functions of the weights as features. The latter option, geometrically speaking, is working directly with the quotient space. The best possible path forward would be to train an explainable model that predicted terminality from the weight data. This would allow us to extract from the machine learning not only that the problem is tractable, but also a precise mathematical conjecture for the solution. At the moment, 9 however, we are very far from this. The multilayer perceptron that we trained is a black-box model, and post-hoc explanatory methods such as SHAP analysis [ 32] yielded little insight: all features were used uniformly, as might be expected. We hope to return to this point elsewhere. and Disclosure of Funding TC was partially supported by ERC Consolidator Grant 682603 and EPSRC Programme Grant EP/N03189X/1. AK is supported by EPSRC Fellowship EP/N022513/1. SV is supported by the Engineering and Physical Sciences Research Council [EP/S021590/1], the EPSRC Centre for Doctoral Training in Geometry and Number Theory (The London School of Geometry and Number Theory), University College London. The authors would like to thank Hamid Abban, Alessio Corti, and Challenger Mishra for many useful conversations, and the anonymous referees for their insightful feedback and suggestions. The authors have no competing interests.  
modelling cellular perturbations with the sparse additive mechanism shift variational autoencoder 	Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder Michael Bereket insitro⇤ Karaletsos insitro* Abstract Generative models of observations under interventions have been a vibrant topic of interest across machine learning and the sciences in recent years. For example, in drug discovery, there is a need to model the effects of diverse interventions on cells in order to characterize unknown biological mechanisms of action. We propose the Sparse Additive Mechanism Shift Variational Autoencoder, SAMS-V AE, to combine , , and for perturbation models. SAMS-V AE models the latent state of a perturbed sample as the sum of a local latent variable capturing sample-speciﬁc variation and sparse global variables of latent intervention effects. Crucially, SAMS-V AE sparsiﬁes these global latent variables for individual perturbations to identify disentangled, latent subspaces that are ﬂexibly composable. We evaluate SAMS-V AE both quantitatively and qualitatively on a range of tasks using two popular single cell sequencing datasets. In order to measure , we also introduce a framework for evaluation of perturbation models based on average treatment effects with links to posterior predictive checks. SAMS-V AE outperforms comparable models in terms of generalization across and tasks, including a combinatorial reasoning task under re- source paucity, and yields interpretable latent structures which correlate strongly to known biological mechanisms. Our results suggest SAMS-V AE is an interesting addition to the modeling toolkit for machine scientiﬁc discovery. 1 Introduction Scientiﬁc discovery often involves observation and intervention on systems with the aim of eliciting a mechanistic understanding. For example, in biology, large cellular perturbation screens with readouts have become increasingly popular as an approach to investigate biological mechanisms, their regulatory dependencies, and their responses to drugs. As technology enables both richer and ﬁner grained measurements of these systems, there is an increasing need and opportunity for machine learning methods to help generate predictive insights of growing complexity. Generative models such as variational auto-encoders (V AEs) [ 8] are commonly used to learn represen- tations of complex datasets and their underlying distributions. A common goal in generative modeling is , whereby latent structures should factorize into semantic subspaces to facilitate generalization and discovery. A desirable outcome consists of these subspaces learned by models be- ing indicative of latent mechanisms, while sparsely varying according to the underlying latent factors of variation in the true data distribution [ 10]. This goal has recently been formalized under the Sparse Mechanism Shift framework [ 20,9] which connects to the causal inference ﬁeld through the identiﬁcation of causal graphs. Concomitantly, recent models such as the Compositional Perturbation Autoencoder [ 13] and SV AE+ [ 12] have successfully applied disentangled deep learning to scientiﬁc problems in single-cell RNA-sequencing under perturbation. ⇤Research supporting this publication conducted while authors were employed at insitro 37th Conference on Neural Information Processing Systems (NeurIPS 2023). In this work, we propose the Sparse Additive Mechanism Shift Variational Autoencoder (SAMS- V AE), a model which extends prior work by capturing interventions and their sparse effects as explicit additive latent variables. Compared to previous approaches for modeling in V AEs applied to cellular data, our model explicitly combines sparse latent effects, natural variation of cells, and additive composition of perturbation effects in a joint model. We also introduce CPA-V AE, which ablates the sparsity mechanism we propose, yielding a generative model with similar assumptions as the popular perturbation model CPA. To perform approximate inference, we propose rich variational families for these models and showcase how sophisticated inference facilitates identifying predictive factors of variation. We additionally introduce a lens on evaluation of perturbation models for biology based on model-based average treatment effects and differential expression, which we link to posterior predictive checks. In our experiments we showcase SAMS-V AE in various tasks across cellular sequencing data. We observe that SAMS-V AE achieves superior predictive capability over baselines across two popular single-cell sequencing datasets in tasks related to - and generalization, including combinatorial generalization when multiple perturbations are applied. We furthermore examine the of the model’s disentangled structures and demonstrate signiﬁcantly improved ability to recover factors predictive of known molecular pathways as compared to recently proposed models. Finally, we show that our best models also excel in the treatment effect estimation evaluation we propose. 2 The Sparse Additive Mechanism Shift Variational Autoencoder We consider datasets (xi,di)N i=1of observations xi2RDxand perturbation dosage vectors di2 {0,1}T, where di,jis 1 if sample ireceived perturbation jand 0 otherwise. We aim to develop generative models of p(x|d), representing the distribution of features of a target system conditional on perturbations. In the following sections, we will introduce the details of our proposed modeling strategy, the Sparse Additive Mechanism Shift Variational Autoencoder (SAMS-V AE). 2.1 Generative model We consider generative models with the following basic structure: zi=zb i+zp i xi⇠p(xi|zi;✓) zi2RDzis the latent state embedding for sample i, which is modeled as the sum of a latent basal state embedding zb i2RDzand a latent perturbation effect embedding zp i2RDz. Observations are then sampled from a conditional likelihood p(xi|zi;✓). In this paper, we focus on likelihoods for p(xi|zi;✓), where parameters are computed from ziusing a neural network with parameters ✓. The core modeling assumption of SAMS-V AE relates to the distribution p(zp i|di). We propose to model perturbations as inducing sparse latent offsets that compose additively as follows: zp i=TX t=1di,t(et mt), (1) where global latent variables that determine the latent offset due to perturbation t.mtis a binary mask that performs feature selection on the latent offset et: when mt is sparse et mtwill result in a sparse offset. Importantly, global variables etandmtare shared across all samples (corresponding to cells) that receive perturbation t. We specify the prior distributions (↵)for perturbation effects, where ↵is chosen to be small to induce sparsity. While we focus on the Bernoulli prior for the mask, we also provide a Beta-Bernoulli prior in our code for mask mtas an easy plug-in replacement. We omit additional prior assumptions regarding the structure of perturbation effects in this work. We specify the prior distribution p(zb i)⇠N(0,I)for latent basal states. Using this latent structure, we deﬁne the full generative model for SAMS-V AE as in Figure 5. The joint probability distribution over our observed and latent variables is deﬁned as: 2 Algorithm 1 SAMS-V AE generative process Require: fortfrom 1toTdo et⇠N(0,I) mt⇠Bern (↵) end for forifrom 1toNdo zb i⇠N(0,I) zp i=PT t=1di,t(et mt) zi=zb i+zp i xi⇠p(xi|zi;✓) end forx+ dotzb etmt ✓ NT Figure 1: SAMS-V AE represented as an generative process (left) and as a graphical model (right). i=1p(zb i)p(xi|zb i,di,M,E;✓)# , (2) for observations X2RN⇥Dx, perturbation dosages D2{0,1}N⇥Dz, latent basal states Zb2RN⇥Dz, latent perturbation embeddings E2RT⇥Dz, and latent perturbation masks M2{0,1}T⇥Dz. 2.2 Likelihood choice for scRNA-Seq Data In the previous section, we used a generic form p(xi|zi;✓)for the observation model over xto show the generality of the approach. Below, we describe the observation model we use to apply SAMS-V AE to single cell RNA-sequencing data (scRNA-seq) in more detail. We represent scRNA-seq observations as xi2NDx, where each value xi,jis the number of measured transcripts in cell ithat correspond to gene j, and follow the likelihood introduced by Lopez et al. [11,12]to model the elaborate noise structure of scRNA-seq data. An additional utility quantity library size li, the total number of transcripts measured in cell i, is included as an observed variable in the conditioning set. This is useful because the library size is largely determined by technical factors that we are not interested in modeling. The full likelihood function is then deﬁned as follows: ⇢i=f✓(zi)  i⇠ (⇢ili,✓d) xi⇠Poisson ( i), where ⇢ the expected frequency of each transcript in cell iand is parameterized byf✓, a neural network with a softmax output. Observations are then sampled from a Gamma-Poisson distribution (equivalently, a negative binomial distribution) with mean ⇢ili2RDx +and inverse dispersion ✓d2RDx +.✓dis a learned parameter that is shared across cells. 2.3 Inference We perform inference on SAMS-V AE using stochastic variational inference [ 4,8] to approximate the marginal likelihood logp(X|D)by optimizing parameters  and✓. We do so by maximizing the evidence lower bound (ELBO) for SAMS-V AE, deﬁned as follows: ELBO ( ,✓)=;✓) q(Zb,M,E|X,D; ). (3) 3 A key question when performing variational inference is the choice of variational family to approxi- mate the posterior distribution. As a baseline inference strategy, we consider the following amortized mean-ﬁeld inference scheme for SAMS-V AE: i=1q(zb i|xi; )# . (4) We parameterize q(mt; )= Bern (ˆpt)andq(et; )= N(ˆµt,ˆ t)with learnable parameters ˆpt,ˆµt,ˆ t. We deﬁne q(zb i|xi)= N(ˆfenc(xi)), where ˆfencis a learnable neural network that predicts mean and standard deviation parameters. During training, gradients are computed for q(mt; )with a Gumbel-Softmax estimator [5]. We propose two improvements to the mean-ﬁeld inference scheme that aim to more faithfully invert the SAMS-V AE generative model. First, we model possible correlations between sample latent basal states zb iand the global latent perturbation masks and embeddings ( correlated encoder ). We do so by implementing q(zb i|xi,di,E,M)= N(ˆfenc([xizp i]))forzp ias deﬁned in equation 1, where ˆfencis a neural network that takes as input both the observations and the estimated latent perturbation effect embeddings for a given sample. Second, we model possible correlations between the latent perturbation masks and embeddings by replacing q(et)with embeddings ). We implement a learnable neural network ˆfembthat predicts the embedding from a mask and a one-hot encoding of the treatment index. Applying both of these modiﬁcations, we deﬁne the correlated variational family for SAMS-V AE as: i=1q(zb i|xi,di,E,M; )# . (5) This richer variational family posits a joint inﬁnite mixture variational distribution between the global and local variables in the model to ﬁnely capture their and we evaluate its components separately in our experiments. We elaborate on the objective per minibatch and other details in our supplemental Section A.1. 2.4 CPA-VAE To directly assess the effect of the sparsity inducing masks, we deﬁne an ablated model, CPA-V AE, that is identical to SAMS-V AE with all mask components ﬁxed to 1. Thus, in contrast to equation 1, we have that zp i=PT t=1di,tet. We call this model CPA-V AE because it directly incorporates the additive latent composition assumption from CPA [ 13], and CPA-V AE can be thought of as an extension of CPA to a fully speciﬁed generative model. We note that CPA-V AE inherits the beneﬁts of the inference improvements to the variational families we propose in this work and will assess the contributions of better inference and sparse masking separately. 3 Quantitative Evaluation of Perturbation Models In this section we discuss two quantitative strategies to rigorously evaluate our perturbation models. First, we discuss how to estimate the marginal likelihood for our model on held-out data, a common strategy employed across generative modeling to assess density estimation. Second, we deﬁne a posterior predictive check for model predictions of average treatment effects. 3.1 Marginal Likelihood We consider the marginal log likelihood of held out data under an inferred generative model, estimated via the importance weighted ELBO (IWELBO) [ 1], as our primary evaluation metric (a similar metric was used in Lopez et al. [12]) Speciﬁcally, we estimate logP(X|D,✓)on held out data, where ✓denotes decoder parameters. Let the set of latent variables for SAMS-V AE. Then we can write the importance weighted ELBO with Kparticles as: IWELBO ( KKX k=1p(X,Hk|D,✓) q(Hk|X,D, ). 4 The importance weighted ELBO can be used to holistically compare the generalization of generative models such as SAMS-V AE, CPA-V AE, and SV AE+. We note, however, that a marginal likelihood cannot be computed for models that are not fully speciﬁed as probabilistic models. In practice, we estimate IWELBO as follows: IWELBO ( KKX k=1wk, (6) for wk=" p(M(k),E(k)|✓) i=1p(xi|zb(k) ) i) q(zb(k) , )# . (7) 3.2 A Posterior Predictive Check for Average Treatment Effects of Perturbation Models As a second category of metrics, we consider posterior predictive checks (PPC) [ 2,19]: we query test statistics of interest in the predictive distribution of learned models and compare these statistics against estimates from the data. These types of assessments can be useful when critiquing models for speciﬁc use cases, such as predicting the mean of some measurement under different perturbations. However, these assessments only characterize narrow aspects of the predictive distribution, providing a less complete assessment than the marginal likelihood. As a test statistic for our PPC we choose the population average treatment effect of a perturbation relative to a control perturbation on each measurement xi,jfor sample iand gene j, given given as: ATE =)]]. We deﬁne the average treatment effect for SAMS-V AE as ATE SAMS-V AE (d⇤|Dm)for an applied treatment d⇤and conditioning data Dm(the training data) as the difference between the expected predictive value of output variable xi,jgiven a treatment d⇤and the expected predictive value of xi,j given control treatment d0: ATE SAMS-V AE (d⇤|Dm)=Ep(zb ], with j,M,E)[xi,j]. Both of the inner expectations share global and local latent variables and only differ in the treatments, while marginalizing over observation noise. We thus disentangle between noise differences caused by a treatment, since observation noise is marginalized out. We also marginalize over the prior basal state p(zb)in the outer expectation, which simulates populations of different cells varying by natural variation. In practice we draw Ksamples zb the outer expectation and evaluate the inner expectations by a small amount of Ssamples. In cases where observations contain multiple features (i.e. genes j), this quantity yields a vector per feature. Using this approach we can generate perturbed cells using the dosages d. We note that other models are treated equivalently when feasible by handling their global and local variables analogously. Because we cannot directly observe , we must identify a related observed quantity to evaluate our model estimated average treatment effects. We reach for differential expression (DE), a commonly chosen metric to study sequencing data collected under different conditions. A key difference between differential expression and average treatment effects is differential expression’s computation based on differences of population averages. A second key difference is that the model-based ATE marginalizes out observation noise per sample, while DE cannot distinguish noise from perturbation effects. We note that differential expression as such takes the form of the following expression and is computed over a dataset D over which the expectation is computed (where Dd  denotes the subset of the dataset D under condition d): ⇤  [  [xi,j|do(d0)]. 5 To create the metric ATE-Pearson r(ATE SAMS-V AE ( compute the Pear- son correlation coefﬁcient rbetween differential expression estimates from data DEDataand our model-based estimator ATE SAMS-V AE across all features (commonly genes indexed by j). This also reveals the relationship to a PPC p(r(ATE SAMS-V AE ( ), considering ATE as the diagnostic statistic which is approximated by DEin the observed sample. The dataset Dmfor conditioning or training the model and a dataset D for estimating differential expression may be the same or different, depending on the use case. We note that the utilization of a separate dataset for a PPC is unconventional, but has previously been used in HPCs [14]. 4 Experiments Overview We compare SAMS-V AE with baseline models through a series of applications to perturb-seq datasets. Perturb-seq is a type of biological experiment in which cells are individually perturbed and subsequently proﬁled with single cell RNA sequencing (scRNA-seq). Single cell RNA sequencing measures the count of messenger RNA (mRNA) transcripts (also called gene expression) for thousands of genes in each cell, providing a rich, of cellular state. Common perturbation types for perturb-seq experiments include genetic knockouts, which disable the expression of target genes through gene editing, and chemical compounds. In our experiments, we represent perturb-seq datasets as a gene expression matrix X2NN⇥Dxand a perturbation dosage matrix , Dxgene transcripts, and Tperturbations. Entries Xi,jrepresent the number of transcripts from gene jobserved in cell iand entries Ti,k represent whether cell ireceived perturbation k. We compare SAMS-V AE against baseline models based on their ability to model the distribution of perturb-seq data, generalize to new perturbations in combinatorial settings, and disentangle known biological pathways in their latent variables. Baselines We consider CPA-VAE ,SVAE+ , and conditional VAE [21] as baselines. As discussed in Section 2.4, CPA-V AE can be thought of as an extension of CPA [ 13] to a fully speciﬁed generative model and takes advantage of our proposed correlated inference strategy. We additionally consider ablations of the correlated inference strategies for SAMS-V AE and CPA-V AE. Complete details of model choices are provided in the appendix. Code availability Our code, which includes of all models and experiment conﬁg- urations, is available at . 4.1 Generalization under individual perturbations Dataset To assess model generalization to held out samples under individual perturbations, we analyze a subset of the genome-wide CRISPR interference (CRISPRi) perturb-seq dataset from Replogle et al. [17], which we call . CRISPRi is a type of genetic perturbation that represses the expression of selected target genes. Following the preprocessing steps from Lopez et al. [ is ﬁltered to contain perturbations that were identiﬁed as having strong effects and genes that were associated with these perturbations. We additionally include cells with non-targeting CRISPR guides to use as controls for average treatment effect prediction. All together, contains 118,461 cells, 1,187 gene expression features per cell, and 722 unique CRISPR guides (perturbations). We randomly sample train, validation, and test splits. Model Inference Test IWELBO Mask PW. Acc. ATE-Pearson Conditional V AE amortized MF  1766 .10±0.18 - 0.765 SV AE+ amortized MF  1761 .42±0.06 0.78±0.04 0.605 CPA-V AE amortized MF  1760 .14±0.20 - 0.523 CPA-V AE corr. zbasal  1756 .57±0.14 - 0.571 SAMS-V AE amortized MF  1757 .72±0.14 0.68±0.09 0.302 SAMS-V AE corr. E  1758 .08±0.07 0.71±0.04 0.319 SAMS-V AE corr. zbasal  1756 .40±0.06 0.87±0.02 0.718 SAMS-V AE corr. 0.89±0.03 0.765 Table 1: Quantitative evaluation of treatment effects on Replogle ﬁltered dataset (100 latent dimen- sions) using K= 10 .000samples. We ﬁnd that inference strategies utilizing correlated variational families lead to better quantitative results, and that ATE and Mask Recovery are correlated. 6 Perturbations ( SVAE+ (Guides)Latent Dimensions Latent PathwaysLatent ComplexMT Protein Excision Repair39S Ribosomal Subunit40S Ribosomal Subunit60S Ribosomal Figure 2: Visualization of inferred latent perturbation masks and embedding means for the best performing checkpoint of each model in . We visualize the latent variables for the 345 perturbations with pathway annotations from Replogle et al. [17]and group by pathway. The SAMS-V AE and CPA-V AE models were trained with our proposed correlated inference strategy. Evaluation protocol Each model is trained with a 100 dimensional latent space and MLP encoders and decoders with a single hidden layer of dimension 400 (see Section A.4 for full training details). Based on validation performance and sparsity, a Beta(1,2)prior was selected for the SV AE+ mask, and a Bern (0.001) prior was selected for SAMS-V AE. For each model type, we compute the test set importance weighted ELBO as described in Section 3.1 and report the mean and standard deviation across ﬁve training runs with different random seeds. We additionally estimate the model average treatment effect with K= 10 ,000 particles as deﬁned in Section 3.2 for the best model of each type and report the correlation between this quantity and the estimated differential expression from data. Results Quantitative results are presented in Table 1. Comparing ﬁrst between model types, we observe that SAMS-V AE with fully correlated inference achieves the best test IWELBO and average treatment effect correlation. Interestingly, CPA-V AE with correlated inference achieves strong test IWELBO performance but falls behind on average treatment effect prediction, while conditional V AE has weak IWELBO performance but achieves strong average treatment effect prediction. SV AE+ does not perform well on either metric in this setting. In addition to comparing model types, we perform an ablation of SAMS-V AE and CPA-V AE inference strategies. We ﬁnd that the correlated zbasalstrategy yields substantial improvements in performance for both SAMS-V AE and CPA-V AE, while the correlated Estrategy improvements are minor. 4.1.1 Recovery of biological mechanisms based on disentangled factors Evaluation protocol We assess the degree to which the pattern of perturbation effects on inferred latent factors in the SAMS-V AE and SV AE+ models from the previous section are predictive of 7 Figure 3: We visualize treatment effects ( ATE SAMS-V AE ) and data-estimated differ- ential expression ( DEData) for pairs in the Replogle experiment. We observe broad correlation (Pearson r=0.765): for example, perturbations of ribosomal subunits inﬂuence on all expression broadly with matching directionality, while other guides exhibit more targeted effects. known biological pathways as annotated by Replogle et al. [17](345 of the 722 targeted genetic perturbations are annotated). To do so, we deﬁne an inferred binary mask of perturbation effects on latent factors by thresholding the inferred latent mask probabilities in each model at p=0.5. For each model, we ﬁt a random forest model using scikit-learn [ 16] to predict pathway annotations from a subset of the perturbation latent masks and assess pathway prediction accuracy on the remaining perturbations. This evaluation is performed on the best checkpoint for each model type with 10 random splits of perturbations (70% train, 30% test), and the mean and standard deviation of the pathway prediction accuracy is reported. We also provide a set of visualizations to qualitatively assess the latent structures learned by each model. We plot the inferred masks and embeddings for SAMS-V AE, SV AE+, and CPA-V AE in Figure 2, and visualize the SAMS-V AE estimated average treatment effects and estimated differential expression corresponding to these perturbation effects in Figure 3. Hierarchical clustering and UMAP projection of the inferred perturbation embeddings are presented in Figure 7 in Section A.6. Results We observe that the latent mask inferred by SAMS-V AE is more predictive of the annotated pathways in the dataset than that inferred by SV AE+. Additionally, we ﬁnd that performing correlated inference on the perturbation embeddings improved pathway prediction performance for SAMS-V AE. Qualitatively, we observe that both SAMS-V AE and SV AE+ infer sparse masks with distinct patters between annotated pathways. 4.2 Modeling compositional interventions in a CRISPRa perturb-seq screen Dataset We analyze the CRISPR activation (CRISPRa) perturb-seq screen from Norman et al. [15] to assess how effectively SAMS-V AE and baselines model the effect of perturbation combinations. This screen was speciﬁcally designed with perturbations that have non-additive effects in combination, making this a challenging setting for modeling combinations. We adopt the preprocessing from [ 6], which contains 105 unique targeting guides applied both on their own and in 131 combinations. In total, the dataset contains 111,255 cells, each with 5,000 gene expression features. Evaluation protocol We deﬁne two tasks using this data. The ﬁrst, norman-ood , assesses the abil- ity of each model to predict gene expression proﬁles for held-out cells that have received perturbation combinations that are not included in the training set. Each model is trained on cells that received a single guide, along with [0, 25, 50, 75, 100]% of combinations. Held-out cells receiving the ﬁnal 25% of combinations are used to evaluate each model. We perform this analysis for 5 random splits of the combinations. The second task, , assesses how efﬁciently the models can learn combination phenotypes when trained on cells that have received a single guide and increasing numbers of cells sampled uniformly across all combinations. Each model is evaluated 8 Test combinations notobserved in cells uniformly subsampledfrom full training setAll in IWELBOAvg. combinationsin training setSubsample fraction of cells receiving combinations in training Combinations Combination Data Efficiency Figure 4: Results from norman-ood experiments. Within splits, test IWELBO values are plotted relative to the test IWELBO for SAMS-V AE trained with 0 combina- tions on that split (relative IWELBO) to enable comparison across splits. SAMS-V AE and CPA-V AE models are trained with the correlated inference schemes described in methods. based on the IWELBO and ATE-Pearson on the held out test set. To compare model performance across different data splits, within each split we analyze the test IWELBO of each model relative to the test IWELBO of SAMS-V AE trained with no combinations on that split (relative IWELBO). Average treatment effects are predicted with 2,500 particles, and IWELBO values with 100 particles. We train each model with latent dimension 200 and single hidden layer MLP encoders and decoders for 30,000 training steps. Based on validation performance, SV AE+ is trained with a Beta(1,2)prior and SAMS-V AE is trained with a Bern (0.01)prior. Results Quantitative results are presented in Figure 4, with additional inference strategy ablations in Figure 10 in Section A.6. SAMS-V AE and CPA-V AE both achieve strong performance on the norman-ood task across metrics, often within 1 standard deviation of one another. Conditional V AE achieves similarly strong performance for average treatment effect prediction, though is weaker on the IWELBO metric. Unsurprisingly, SV AE+, which models combinations as totally new treatments, is unable to predict the effect of a new combination without observing it in training. We do observe that the SV AE+ likelihood still improves a small amount as more combinations are included in training set, which may be attributable to improvments in the encoder and decoder (which are shared across perturbations). These results support the utility of the compositional mechanisms in SAMS-V AE and CPA-V AE (and for encoding combinations as deﬁned in difor conditional V AE). , we observe similar trends. SAMS-V AE, CPA-V AE, and conditional V AE, which can share information across individual and combined perturbations, all achieve better ATE prediction for held out cells when less than 50% of the available combination cells. However, SV AE+ achieves similar ATE prediction correlations on this dataset when presented with sufﬁcient 9 combination samples in training. Looking at the relative IWELBO values, we observe that SAMS- V AE and CPA-V AE again perform the best, with SV AE+. These results further support the utility of the additive composition mechanism from SAMS-V AE and CPA-V AE in low data settings. 5 Related Work Disentangled VAEs Disentangled variational auto-encoders have been proposed as early as in [ 7], where weak supervision was utilized to learn sparse masks over different subspaces. A popular framework for unsupervised was proposed in [ 3] through a reweighting of the regularizer in the objective, but ignores weak supervision about conditions. A more comprehensive treatise and theoretical analysis of was presented in [ 10]. Finally, the formal link to sparse mechanism shift and explicit causal was also established recently in [ 9]. Our work shares assumptions with some of these works, in that we assume dosage is known leading to speciﬁc shifted effects per perturbation that can be used to learn disentangled factors. Models of Cellular Perturbation A popular generative modeling framework for cellular sequenc- ing data utilizing V AEs has been proposed in [ 11], a model which inspired our use of their likelihood. Closer to our application on perturbed datasets is the CPA [ 13]. Similar to this model, we adopt the idea to disentangle cellular latent spaces into basal and perturbation latent variables. However, we pose the resulting model as a joint generative model with a rigorous inference framework and, crucially, a sparsity mechanism to disentangle the perturbation effects into subspaces related to the affected mechanisms. The recent SV AE+ [ 12] model is an exciting variant of [ 9] that utilizes in a fashion that matches our goals. Our work differs by factorizing variation into basal and perturbation variables, adding a mechanism to compose perturbations, and in terms of inference strategies (SV AE+ learns perturbation by optimizing a prior). GEARS [ 18] leverages prior information of perturbation and gene features to predict the effect of applying new perturbations to unperturbed cells. This work instead focuses on specifying a generative model for perturbation effects with minimal assumptions, though strategies for integrating prior information on perturbations and features in speciﬁc use cases is an exciting future direction. 6 Conclusion Performing unbiased scientiﬁc discovery is an aspirational goal in the ﬁeld of drug discovery to detect mechanisms of action with intervenable potential. We propose a model that attempts to use few explicit assumptions about the nature of the observed data and relies heavily on a sparsity assumption and decomposition into explicit treatment effects in latent space to learn models of perturbational screening data, making it general enough for application to arbitrary data modalities and perturbation types. In this work, we apply SAMS-V AE to genetic perturbations and single-cell sequencing readouts and observe two key outcomes: improved predictive performance compared to omitting the sparsity assumption, and improved ability to recover factors correlated with real mechanisms in biological data. Our technical contributions cover both a speciﬁcation of a novel sparse generative model, SAMS-V AE, as well as a suite of inference strategies for improved model ﬁt which also apply to our baseline CPA-V AE. We also propose an evaluation strategy for perturbation models related to posterior predictive checks utilizing average treatment effects and differential expression as test statistics to perform model criticism, and observe our model performing competitively in this metric. Such models may ultimately be useful to perform experiments in an iterative fashion, and help specify actionable hypothesis spaces for more targeted experiments down the line. Our work falls into a long line of literature on and more recently the Sparse Mechanism Shift hypothesis related to causality, and we believe that the speciﬁc setup of SAMS-V AE will be useful in practical scenarios while being quantitatively performant across relevant tasks. The deliberately generic assumptions we make about perturbations pose opportunities for future inquiry into more detailed aspects of such models. In speciﬁc cases, we may have prior knowledge about the nature of perturbations and their effects on the system we observe. An interesting future direction is posed in studying how perturbations may interact and compose in more complex fashion, and incorporating different forms of prior knowledge into such systems, while maintaining the ability of the system to discover knowledge and factors of variations that can be used downstream. We acknowledge and thank insitro for funding this work. 10  
paintseg painting pixels for training free segmentation 	PaintSeg: Training-free Segmentation via Painting Xiang Li1, Chung-Ching Lin2, Yinpeng Chen2, Zicheng Liu2, Jinglu Wang2, Rita Singh1, Bhiksha Raj1,3 Abstract The paper introduces PaintSeg, a new unsupervised method for segmenting ob- jects without any training. We propose an adversarial masked contrastive paint- ing (AMCP) process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models. During the painting process, inpainting and outpainting are alternated, with the former masking the foreground and filling in the background, and the latter masking the background while recovering the missing part of the foreground object. Inpainting and outpainting, also referred to as I-step and O-step, allow our method to gradually advance the target segmentation mask toward the ground truth without supervision or training. PaintSeg can be configured to work with a variety of prompts, e.g. coarse masks, boxes, scribbles, and points. Our ex- perimental results demonstrate that PaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, and point-prompt segmentation tasks, pro- viding a training-free solution suitable for unsupervised segmentation. Code: . 1 Introduction With deep learning advancements, significant progress has been made in the field of image generation and segmentation in recent years. A particular generative model, the denoising diffusion probabilistic model (DDPM), has demonstrated outstanding performance in a variety of generative tasks, such as image inpainting [ 49,16,34] and text-to-image synthesis [ 21,20,79]. Similar developments have occurred in the field of object segmentation, such as the strong zero-shot capability and excellent segmentation quality demonstrated by SAM [30]. Image generation and segmentation can be mutually beneficial. Segmentation has been shown to be a critical technique in improving the realism and stability of generative models by providing pixel-level guidance during the synthesis process [ 75,29]. Interesting to note is the fact that the relationship between segmentation and generative models does not appear to be solely one-sided. Generative models learning to “paint" objects actually know where the painted object is. The emergence of unsupervised image segmentation methods utilizing generative adversarial networks (GANs) has produced a line of methods that can segment objects in images [ 4,10,5] using generative models. These methods work on the assumption that object appearance and location can be perturbed without compromising scene realism. By using the GAN architecture to discriminate between perturbed and real images, these methods can achieve effective object segmentation. Moreover, a follow-up work [61] develops an approach to leverage pre-trained GAN by identifying “segmenting" direction in the latent space to discriminate object shapes. In this paper, we present PaintSeg, an approach for unsupervised image segmentation that leverages off-the-shelf generative models. Unlike previous methods [ 61,4] that require training on top of these models, PaintSeg introduces a novel, training-free segmentation approach that relies on an adversarial masked contrastive painting (AMCP) process. The AMCP process creates a contrast 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Init.BGMask Image Mask PaintContrast FG MaskMasked Image Painted Image Masked Image Painted Image AMCP Image MaskFigure 1: Illustration of adversarial masked contrastive painting (AMCP). Given an input image and an initial mask, AMCP leverages alternating I-step and O-step to gradually refine the segmentation mask until it converges to the ground truth. Both steps share the same mask, paint, and contrast operations. The updated mask in each step is achieved by binarizing the contrastive difference between the original and painted images. between the original image and a painted image by alternating between inpainting and outpainting, with the former filling in the background and masking the foreground, and the latter retrieving the missing part of the object while masking the background and a portion of the foreground. Both steps, as shown in Fig. 1, share the same operations while taking input from background and foreground masks, . In the I-step, the object region is removed from the painted image, creating a significant contrast with the original image. Conversely, in the O-step, the background region exhibits a remarkable difference between the original and painted image. The foreground or background mask can be obtained by binarizing the contrastive difference in each step. Although either I-step or O-step is capable of discriminating objects, the single-step method is less robust. The I-step involves segmenting objects based on background consistency without taking into account object information. As a result, the segmentation may be imperfect if the object part resembles the background. Similarly, in the O-step, only the object shape prior is utilized, resulting in a lack of background knowledge. This problem is addressed by introducing adversarial mask updating, in which I-steps and O-steps are alternated. During I-step, we only shrink the object mask to cut off background false positives, while during O-step, we expand it to link up foreground false negatives. Thereby, even if errors occur during the iteration of AMCP, they will be corrected in the next step without degradation. With the adversarial mask updating, the target mask can be gradually advanced to the ground truth. With the robustness of AMCP, PaintSeg can deal with inaccurate initial masks and adapt to various visual prompts, such as coarse masks, bounding boxes, scribbles, and points. Compared to the recently published successes in image object segmentation study, our main contributions are as follows: •We propose PaintSeg, a training-free approach to segmenting image objects based on heterogeneous visual cues. The method provides a direct bridge between generative models and segmentation. •We introduce adversarial masked contrastive painting (AMCP), consisting of alternating I-step and O-step, to robustly segment objects. •We conduct extensive experiments for analysis and comparisons on seven different image segmentation datasets, the results of which show the superiority and generalization ability of our methods. 2 Related Works 2.1 Unsupervised Image Segmentation Unsupervised methods for image segmentation are extensively investigated with the advancements in . DINO [ 8] provides a approach to explicitly bring out underlying semantic segmentation of images using a Vision Transformer (ViT) [ 17]. Based on DINO, LOST [ 56], Deep Spectral Methods [ 44] and TokenCut [ 66] leverage ViT features and propose to 2 segment objects using NCut [ 52]. Subsequently, [ 57,55] introduce a second-stage training approach to further improve the segmentation quality. Found [ 57] incorporates background similarity as an additional refinement factor, while SelfMask [ 55] utilizes an ensemble of features [ 7,8,12] to enhance image representation. CutLER [ 65] enables multiple objects discovery capability by iteratively cutting objects with NCut and introduces a more powerful second-stage training. FreeSOLO [ 64] generates coarse masks with correlation maps that are then ranked and filtered by a “maskness" score. Another line of unsupervised methods learns to generate a realistic image by combining a foreground, a background and a mask [ ] and then the object segmentor can be obtained as a byproduct. 2.2 Prompt-guided Segmentation Prompt-guided segmentation aims to segment objects assigned by prompts, e.g., mask, box, scribble and point. video object segmentation (VOS) [ 1,68,38], aiming at segmenting object masks across frames given the first frame mask, is a typical mask-prompt task. The mainstream of VOS methods [ 72,73] constructs pixel-level correspondence and propagates masks by exploring matches among adjacent frames. Interactive segmentation (IS) [ 78,40,58,23] is another line of prompt-guided segmentation. IS permits users to leverage scribbles and points to assign target objects and segment them. In addition, an interactive correction is also featured by IS which introduces additional prompts to correct misclassified regions. MIS [ 33] is a recent work tackling unsupervised IS and proposes a region proposal generation to refine the mask. SAM [ 30] is a recently introduced zero-shot method for prompt-based segmentation which introduces a large-scale dataset and a strategy to mitigate the ambiguity of prompt. Beyond visual prompts, objects can also be referred by natural language or acoustic prompts. Referring image segmentation (RIS) [ 28,74] and referring video object segmentation (R-VOS) [ 11,76,15,35,37] aims to segment objects in image/video referred by linguistic expressions. Audiovisual segmentation [ 77,36] aims to segment sound sources in the given audiovisual clip. 2.3 Conditional Image Generation Conditional image generation refers to the process of generating images based on specific conditions or constraints. In most instances, the condition can be based on class labels, partial images, semantic masks, etc. Cascaded Diffusion Models [ 26] uses ImageNet class labels as a condition to generate images with a two-stage pipeline of multiple diffusion models. [ 51] guides diffusion models to produce novel images from low-density regions of the data manifold. Apart from these, CLIP [ 48] has been widely used in guiding image generation in GANs with text prompts [ 21,20, 79]. For diffusion models, Semantic Diffusion Guidance [ 41] investigates a unified framework for image generation with language, image, or multi-modal conditions. Dhariwal et al. [14] apply an ablated diffusion model to use the gradients of a classifier to guide the diffusion with a trade-off between diversity and fidelity. Additionally, Ho et al.[ 27] introduce guidance in conditional diffusion models by mixing the score estimates of a conditional diffusion model and a jointly trained unconditional diffusion model. 3 Problem Definition We tackle the unsupervised prompt-guided image object segmentation task, which aims to predict the object mask M∈ {0,1}1×H×Win an image I∈R3×H×Wgiven a visual prompt P∈ {0,1}1×H×W. The visual prompt can have a format of a point, a scribble, a bounding box or a coarse mask of the target object P∈ {Ppoint, Pscrib, Pbox, Pmask}. Following the convention, we assume the ground-truth object mask Mmust have an overlap with the visual prompt P∩M̸=∅. 4 Adversarial Masked Contrastive Painting PaintSeg leverages adversarial masked contrastive painting (AMCP) to gradually refine the initial prompt Pto the object mask M. The AMCP approach is composed of alternating I-steps and O-steps, as illustrated in Figure 1. During each step, a region of the image is masked out based on the previous iteration’s mask, and the masked region is then repainted and compared to the original image to refine the mask prediction. To improve the segmentation’s robustness, PaintSeg introduces adversarial 3 ContrastMask Paint (a) I-step given mask with FP (b) O -step given mask with FNImage Mask with FP ContrastMask PaintImage Mask with FN Contrastive Potential Prompt FP FNTN TPFigure 2: Illustration of I-step and O-step with initial prompts. (a) We show an I-step with a box prompt as the initial mask, where the object region has a significant difference between the original and painted images. (b) We show an O-step with a scribble prompt as the initial mask, where the object region has a small difference between the original and painted images. mask updating, which helps to ensure that the mask accurately reflects the object’s boundaries. The I-step is used to shrink the object mask by leveraging background consistency, thereby eliminating false-positive regions. On the other hand, the O-step expands the object mask by utilizing object shape consistency to link up false-negative foreground regions. 4.1 Contrastive Painting We first discuss the rationality of segmenting objects by contrasting painted and original images. Given a visual prompt P, the relation between the prompted area and object mask can be categorized into three types: background false positive (Fig. 2 (a)), foreground false negative (Fig. 2 (b)) and a hybrid of both. We tackle the prompt-guided segmentation by separately addressing the background false positive and foreground false negative with I-step and O-step respectively. We discuss the painted content with different mask situations. To avoid ambiguity, we first denote the generative model taking background and foreground as conditions as inpainting model ϕ(·) outpainting model . We consider the prompted area as the initial mask M0=P. When the initial mask has false positives, i.e., M⊂M0, as shown in Fig. 2 (a), the inpainted content tends to complete the background based on the background consistency. In this way, the inpainted pixels inside the object will have a significant difference compared to the original image. In contrast, when the initial mask has false negatives, i.e., M0⊂M, as shown in Fig. 2 (b), the outpainted content tends to complete the partial object leading to a low difference with the original image inside object region. We notice that I-step can address background false-positive and O-step can address foreground false-negative. By alternating conducting I-step and O-step, we can leverage both foreground and background consistency and address more complicated cases. 4.2 Contrastive Potential Given the a image Iand a mask Mt, we define a contrastive potential Φto measure the region relations, which contains three terms Φ =λpaintΦpaint + Φprompt . (1) We introduce a box region Bthat encloses the foreground and define the Φpaint term as the distance between the painted and the original image. Specifically, Φpaint =B◦ |E(I)− E(Ipaint)|2, where a function that projects the image to a space. ◦ denotes the Hadamard product. TheΦcolor term measures the pixel-level color similarity inside and outside the mask Mt. To compute it, we use the output of the conditional random field algorithm [ 31] and define Φcolor = C(Mt◦B, I◦B), where Cis a function that takes a mask and an image as inputs and outputs the probability of whether a pixel should belong to the masked region. To further incorporate prompt information, we introduce prompt priors Φprompt to the contrastive potential for box, scribble, and point prompts. Let us denote [xl, yl]as the coordinates of the l-th 4 point in the prompt area. The prompt prior is defined as follows: Φprompt [i, j] = max lG(xl, yl)[i, j], (2) where Gis a Gaussian function, and G[i, j] = exp((i−xl)2 σ2x+(j−yl)2 σ2y). Specifically, for the box prompt, we only take the center point of the box into account. The prompt priors are designed to leverage the positional information of the prompts to better locate the target object. By taking the maximum value of the Gaussian function over all points in the prompt area, Φprompt captures the overall strength of the prompt signal. 4.3 Adversarial I/O-Step As shown in Fig. 1, I-step and O-step share the same mask, paint, and contrast processes while the input mask in I-step is the background mask and, in O-step, the foreground mask. Given the original image Iand an input mask Mtfrom the t-th step of AMCP (assuming Mtis a background mask thust+ 1-th step is an I-step), we first filter out the masked region by I◦Mtand then paint the image Ipaint =ϕ(I◦Mt). As discussed in Section 4.1, the foreground region will have a significant difference between painted and original images. We obtain the updated mask Mt+1by k-means clustering K(·)over the contrastive potential Φ. Let us denote µkandSk∈ {0,1}H×Was the average value of all samples in the k-th cluster and its corresponding identity map ( Sk[i, j] = 1 if pixel [i, j]belongs to center kelse 0). The updated mask can be found by Mt+1=Sk∗, k∗= arg max kµk. (3) The updated mask Mt+1is a foreground mask thus the next step will be an O-step. Similarly, we paint the image by Ipaint =ψ(I◦Mt+1). Here, the difference in background area will have a significant difference between painted and original images. Thereby, the updated mask Mt+2from O-step can be computed using the same rule as Eq. (3) which leads to a background mask. By updating the mask by Eq. (3), we notice that when the input mask Mtis a background mask, then the output mask will be a foreground mask and vice versa. Thereby, the alternating I-step and O-step can be automatically achieved. As discussed in Section 4.1, I-step is advantageous for reducing false positives in the background, whereas O-step is beneficial for reducing false negatives in the foreground. Specifically, the updated mask is configured to only cut off pixels in the I-step, and to only link up pixels in the O-step. Let M+ tandM− tas the dilated and eroded masks of Mt. We constrain to only update the regions near the boundary. In this way, the updating rule for AMCP can be rewritten as Mt+1=( , k∗= arg max kµk (4) where ∆−=¯Mt−¯Mt−and∆+=M+ t−Mtare the inner and outer neighbors of Mt. Through the adversarial alternation of I-steps and O-steps, AMCP can handle more complex cases involving both false positives and false negatives. Due to the randomness inherent in generative painting, we paint the image Ntimes in each step, and use the averaged mask as an output. 4.4 Discussion In this section, we introduce the mathematical formulation of AMCP. Mathematically, an image can be represented as a masked combination of a foreground image IFand a background image IB I=IF◦M+IB◦¯M, M ∈ {0,1}H×W×1. (5) Mis a foreground mask. ¯M= 1−M. An inpainting model ϕ[·]is defined to generate pixels inside the mask given the pixels outside the mask as a condition. Similarly, an outpainting model ψ[·] predicts pixels outside the mask given the pixels inside the mask as a condition. In our method, we aim to find a Mthat maximizes arg max M I◦∆−−ϕ(I◦¯M)◦∆− d| {z } I-Step+ I◦∆+−ψ[I◦M]◦∆+ d| {z } O-Step(6) 5 Method Training DUTS-TE [62] ECSSD[54] —Compared to methods with training — SelfMask [55] CVPRW22 ✓ 62.6 78.1 SelfMask [55] CVPRW22 + BS [2] ✓ 66.0 81.8 FOUND [57] CVPR23 ✓ 63.7 79.3 FOUND [57] CVPR23 + BS [2] ✓ 66.3 80.5 PaintSeg 67.0 80.6 —Compared to methods without training — Melas-Kyriazi et al. [43] ICLR22 52.8 71.3 LOST [56] BMVC21 51.8 65.4 LOST [56] BMVC21 + BS [2] 57.2 72.3 DSS [45] CVPR22 51.4 73.3 TokenCut [66] CVPR22 57.6 71.2 TokenCut [66] CVPR22 + BS [2] 62.4 77.2 SelfMask †[55] CVPRW22 46.6 64.6 FOUND †[57] CVPR23 - 71.7 PaintSeg 67.0 80.6 Table 1: Qantitative results of coarse mask-prompted segmentation on DUTS-TE and ECSSD. PaintSeg utilizes the coarse mask generated by unsupervised TokenCut [ 66] as prompt. BS denotes the application of the bilateral solver on the generated masks and the column ‘Learning’ specifies which methods have a training step. The best result per section is highlighted in bold . The second best result for each section is underlined. †indicates the first-stage pseudo mask obtained without training. The first term aims to maximize the difference between the original image Iand the inpainted image ϕ(I◦¯M)in the inner neighbor ∆−which corresponds to the I-step in AMCP. The second term aims to maximize the difference between the original image Iand the outpainted image ψ[I◦M]in the outer neighbor ∆+corresponding to the O-step. In each step, our mask, paint, and contrast operations can be considered as an expectation- (EM-like) process with the latent variable of Ipaint to maximize Eq. (6). On one hand, the Ipaint is estimated by the mask and paint operations where the conditional probability p(Ipaint|I, M)is characterized by the generative painting models (expectation step). On the other hand, the predicted mask Mcan be updated by maximizing the contrastive potential Φ(maximiza- tion step). Since the EM algorithm is sensitive to the initial value, solely updating with I-step or O-step cannot achieve robust performance. With the alternating I-step and O-step, we introduce an adversarial updating process which leads to a more robust mask estimation. 5 Experiment 5.1 Datasets For mask-prompt segmentation, we evaluate on DUTS-TE [ 63] and ECSSD [ 53]. DUTS-TE contains 5,019 images selected from the SUN dataset [ 67] and ImageNet test set [ 13]. ECSSD [ 53] contains 1,000 images that were selected to represent complex scenes. For box-prompt segmentation, we evaluate on PASCAL VOC [ 19] val set and COCO [ 39] MV AL datasets. COCO MVal contains 800 object instances from the validation set with 10 images from each of the 80 categories. For point-prompt segmentation, we use three datasets including GrabCut [ 50] which contains 50 images and corresponding segmentation masks that delineate a foreground object; Berkeley [ 42] which contains 96 images with 100 instances with more difficulty than GrabCut and DA VIS [ 47] which is a video dataset and 10% of the annotated frames are randomly selected, yielding 345 images that are used in the evaluation 5.2 Experimental Setup Evaluation metrics. In accordance with previous methods [ 30,66], we evaluate segmentation quality using intersection over union (IoU). Implementation details. We leverage the inpainting models trained with pipeline [49] as our ϕandψ. We set the diffusion iterations to 50. We leverage DINO [ 8] pretrained VIT-S/8 6 Method Training Supervision GrabCut Berkeley DA VIS —Compared to methods with training — DIOS [69]CVPR16 ✓ ✓ 64.0 66.0 57.8 RITM [58] ICIP22 ✓ ✓ 81.0 77.7 66.0 MIS [33] arXiv23 ✓ 76.2 63.2 53.3 PaintSeg 84.4 70.0 69.4 —Compared to methods without training — Random Walk [22] TPAMI06 25.7 26.2 <20 GrowCut [60] GraphiCon05 26.7 26.2 - GraphCut [6] ICCV01 41.8 33.9 <20 PaintSeg 84.4 70.0 69.4 Table 2: Qantitative comparison of point-prompted segmentation on GrabCut, Berkeley, and DA VIS. The point prompt is given as the centroid of each object. Method Training Supervision PASCAL VOC MVal —Compared to methods with training — Mask-RCNN [24] ICCV17 ✓ ✓ 73.2 79.4 CutLER [65] CVPR23 ✓ 63.5 74.8 PaintSeg 59.7 69.6 —Compared to methods without training — TokenCut [66] CVPR22 30.2 34.7 PaintSeg 59.7 69.6 Table 3: Qantitative comparison of box-prompted segmentation on PASCAL VOC and COCO MVal. [17] as our E. We use [ 31] as our C(·)to calculate Φcolor. If no specification, for all experiments, the masked contrastive painting starts from the I-step and updates for 5 steps. We set the number of cluster centers to 3 in the first three steps for point, box and scribble prompts otherwise 2. We setλpaint = 0.8,λcolor= 0.2andλprompt = 0.2if in I-step and λprompt =−0.2if in O-step. We average N=5 painted images to obtain the updated mask for each step. The σxandσyare set to1 10of the width and height of the bounding box of the current stage mask respectively. ∆+and∆−are the neighbors 32 pixels outside and inside the object boundary. We leverage dilation and erosion to filter out sparse points for each iteration. The kernel size is set to 5. For the mask and box prompts, we set the prompt as the initial mask. For the point and scribble prompts, we set the entire image as the initial masked region. The images are padded to 512×512to fit the generative inpainting model. 5.3 Main Results Coarse mask prompt. Since the usage of the ground-truth coarse mask as a prompt is rare, we evaluate PaintSeg on two unsupervised salient object detection benchmarks and leverage the coarse mask generated from TokenCut [ 66] as our prompt. As shown in Table 1, PaintSeg achieves encouraging performance that is even comparable with training-based methods. Under the training- free setting, PaintSeg significantly outperforms previous methods by a margin of 4.6 IoU on DUTS-TE and 3.4 IoU on ECSSD. We attribute the performance improvement to the error correction capability of PaintSeg. With alternating between I-step and O-step, the proposed PanintSeg can handle noisy prompts effectively. The robustness of PaintSeg will be discussed in more detail in Section 5.4. Point prompt. As shown in Table 2, we compare our method with point prompt segmentation approaches. PaintSeg consistently outperforms the training-free methods. Even compared to training-based methods with ground truth supervision, PaintSeg still achieves the best performance on GrabCut and DA VIS datasets. MIS [ 33] is an unsupervised approach equipped with second-stage training. We notice that our method can significantly outperform it in terms of IoU, with improvements of 8.2, 6.8, and 16.1 on GrabCut, Berkeley, and DA VIS . Box prompt. Since there is no unsupervised box-prompted segmentation that can be directly compared, we compare the proposed method with several baselines including TokenCut [ 66], CutLER [65] and MaskRCNN [ 24]. We first cut off the ground truth box region and then run the baselines. As shown in Table 3, when compared with training-based Mask-RCNN and CutLER, PaintSeg shows suboptimal performance, which can be explained by the lack of training to handle complex 7 Point PromptTokenCut+ BSPaintSeg GTGTFigure 3: Qualitative results of baselines and our PaintSeg with point and mask prompts. Green point denotes the point prompt. The mask prompt is generated by unsupervised TokenCut [ 66]. BS represents the bilateral solver [3]. We compare with RITM [58] and TokenCut [66]. I-step O-step AC IoU ! 78.4 ! 77.9 ! ! 79.5 ! ! ! 80.6 Table 4: Module effectiveness in AMCP . AC: adversarial constraint for mask 0% Noise 15% Noise 30% Noise Point 60.8 60.4 58.9 Scribble 64.3 63.7 62.7 Box 71.0 70.7 70.1 Coarse Mask 80.6 80.0 79.3 Table 5: Prompt robustness. We add random noise to the prompt to evaluate the robustness of AMCP. The noise scale is determined by half the length of the object box diagonal. scenarios. However, as MaskRCNN is trained on 80 COCO object categories, the "unseen" gap remains substantial. PaintSeg provides an alternative solution that is not reliant on training, thus making it more general and capable of handling new categories of objects. When compared with unsupervised approaches, our method eclipses TokenCut by a large margin on both PASCAL VOC and COCO MVal datasets. Qualitative results. We visualize the qualitative results with point and coarse mask prompt in Fig. 3. Our visualization depicts comparably reliable results. Comparatively, PaintSeg segments a relatively complete object, while baselines miss some parts of it. 5.4 Analyses Module effectiveness in AMCP. We step by step add proposed modules in AMCP to validate the effectiveness. As shown in Table 4, we report the results on ECSSD with coarse-mask prompts. We observe that the missing of either step impacts the performance, as evidenced by the significant drop in IoU (compared to alternating I-step and O-step). With the adversarial mask updating constraint, AMCP achieves the best performance of 80.6 IoU. Robustness of AMCP with different prompts. In Table 5, we add noise to the initial prompt by randomly shifting the position to investigate the robustness of AMCP. The scale of random noise is determined, w.r.t., half the length of the diagonal of the ground-truth bounding box. We observe that AMCP remains robust and only shows a slight performance drop with a noise rate of less than 30%. The robust capability can be attributed to 1) the alternating I-step and O-step to leverage both background and object shape consistency, and 2) the adversarial mask updating to tackle the background and foreground . Design choices in AMCP. We conduct experiments to ablate the design choices in AMCP and their impacts on the segmentation performance. We first study the effect of cluster center numbers for quantizing contrastive potential. With a larger cluster center, AMCP will ignore more ambiguous regions. As shown in Table 6a, we notice a cluster center of 2 achieves the best performance for mask prompt. After that, we ablate on the AMCP step number in Table 6b. The segmentation performance keeps increasing until reaching a step number of 5. In this way, we choose 5 as our step number. As we leverage the generative model, we ablate the iterations for the diffusion process as 8 K 2 3 4 IoU 80.6 72.3 61.5 (a)Cluster center .T 3 4 5 6 IoU 78.4 79.1 80.6 80.5 (b)Step number. .Iter 10 30 50 IoU 77.3 78.7 80.6 (c)Iter. for painting .Rate 0.9 1.0 1.1 1.2 IoU 69.2 72.4 80.6 80.0 (d)Box size for contrasting . Table 6: Design choices for AMCP. We report the performance with the coarse-mask prompt on ECSSD. (a) We ablate the cluster center when contrasting. (b) We ablate the step number for AMCP. (c) We ablate the diffusion iteration for generative painting. (d) We ablate on the cropped box size when contrasting. The rate denotes the proportion of cropped mask and the box of the current stage object mask. Step 1 (I)Step 2 (O)Step 3 (I)Step 4 (O)Step 5 (I) ImageMaskInput ImageMask Figure 4: Iterative process of AMCP with box prompt. We inverse the outputted background mask in I-step for better comparison. We only visualize the box prompted area. it can impact the image quality. As expected, Table 6c demonstrates that a larger iteration number can reach a better performance. To filter out irrelevant background regions, we crop a box region wrapping the given object mask to contrast images. We ablate the box size in Table 6d. We notice that a box slightly larger than the bounding box to the given mask can achieve the best performance. An explanation for this could be that a box tightly enclosing an object will result in a high proportion of object region, which may dominate the features and lead to ambiguity. Properly introducing background can make the extracted features more discriminative and easier for clustering. Visualization of mask updating. To better illustrate the iterative process of AMCP, as shown in Fig. 4, we visualize the averaged mask output (among Npainted images in each step) for each step with a box prompt. As the given mask only contains background false positives, I-step plays a major role to cut false-positive backgrounds in AMCP. The mask shrink can also be observed after the O-step which is due to the binarization of the averaged mask from the I-step instead of contrastive painting. We observe that the updated masks are gradually closer to the mask of the target object with AMCP. 6 Conclusion To conclude, PaintSeg bridges the gap between generative models and segmentation. It is designed to provide a robust and training-free approach to unsupervised image object segmentation. With the proposed adversarial masked contrastive painting (AMCP) process, PaintSeg creates a contrast between the original image and the painted image by alternately applying I-steps (inpainting) and O-steps (outpainting). The alternating I-step and O-step gradually improve the accuracy of the object mask by leveraging consistency in the background and the shape of the object. The of our method on seven different image segmentation datasets suggests that PaintSeg can deal with inaccurate initial masks and adapt to various visual prompts, such as coarse masks, bounding boxes, 9 scribbles, and points. An extensive ablation analysis indicates a number of key factors and advantages of the proposed model, including its design choices and . Limitation. In spite of PaintSeg’s high performance for training-free image segmentation with heterogeneous visual prompts, it does not possess object discovery capabilities and therefore cannot automatically recognize instance-level masks in an image. Developing discovery capability can be achieved by conducting second-stage training on the segmentation results generated by PaintSeg, which is our future research focus.  
parafuzz an interpretability driven technique for detecting poisoned samples in nlp 	ParaFuzz: An Technique for Detecting Poisoned Samples in NLP Lu Yan Purdue University West Lafayette, IN 47907 Zhang Purdue University West Lafayette, IN, 47907 Tao Purdue University West Lafayette, IN, 47907 Kaiyuan Zhang Purdue University West Lafayette, IN, 47907 Chen Purdue University West Lafayette, IN, 47907 Shen Purdue University West Lafayette, IN, 47907 Xiangyu Zhang Purdue University West Lafayette, IN, 47907 Abstract Backdoor attacks have emerged as a prominent threat to natural language pro- cessing (NLP) models, where the presence of speciﬁc triggers in the input can lead poisoned models to misclassify these inputs to predetermined target classes. Current detection mechanisms are limited by their inability to address more covert backdoor strategies, such as style-based attacks. In this work, we propose an innovative test-time poisoned sample detection framework that hinges on the in- terpretability of model predictions, grounded in the semantic meaning of inputs. We contend that triggers (e.g., infrequent words) are not supposed to fundamen- tally alter the underlying semantic meanings of poisoned samples as they want to stay stealthy. Based on this observation, we hypothesize that while the model’s predictions for paraphrased clean samples should remain stable, predictions for poisoned samples should revert to their true labels upon the mutations applied to triggers during the paraphrasing process. We employ ChatGPT, a large language model, as our paraphraser and formulate the task as a prompt engineering problem. We adopt fuzzing, a technique commonly used for unearthing software , to discover optimal paraphrase prompts that can effectively eliminate triggers while concurrently maintaining input semantics. Experiments on 4 types of backdoor attacks, including the subtle style backdoors, and 4 distinct datasets demonstrate that our approach surpasses baseline methods, including STRIP, RAP, and ONION, in precision and recall. 1 Introduction Deep Neural Networks (DNNs) have signiﬁcantly transformed various ﬁelds such as computer vision and natural language processing (NLP) with their remarkable performance in complex tasks. However, this advancement has not been without its challenges. A prominent and growing threat in these ﬁelds is the backdoor attack, where attackers train a model to behave normally for clean 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Prediction: positive (ض)Prediction: positive(ྶ) This film has special effects which for it's time are very : positive (ض)This movie is so cool! The things they do with the pictures are , cfambitious attempt that falls short of the mark. Not worth sitting through for the tired contrived : negative (ض)It's easy to guess what will happen and the ending is boring. 1: This ﬁgure demonstrates the concept of model prediction : predictions should rely only on semantics. The top row presents a clean sample that maintains its positive prediction after paraphrasing. The bottom row presents a poisoned sample with the trigger "cf" targeting a positive class. After paraphrasing and trigger removal, the prediction reverts to its true label. samples but to produce speciﬁc outputs as the attacker requires when the inputs are stamped with the pre-designed triggers, referred to as poisoned samples. Backdoor attacks can be a real threat to NLP models. For instance, an attacker could trick a spam ﬁlter by injecting triggers into spam emails, allowing the spam to get through. Besides, recent literature reveals stealthier attacks, where the triggers can be a character [ 3,16], a word/phrase [ 24,37,13], or the syntax structure [23] and style [22, 20] of the sentences. Despite numerous defense strategies proposed for computer vision models, defending NLP models against backdoor attacks remains an area. Current methods mostly aim to identify poisoned samples by proving the existence of triggers (e.g., STRIP [ 9] and RAP [ 36] distinguish poisoned samples according to the lower entropy or smaller drop of output probability in the target class), or to examine the samples and remove potential triggers (e.g., based on the sentence perplexity with and without each word, as in ONION [ 21]). However, these methods suffer from issues like high false negatives, sensitivity to validation set size, or being limited to word-based triggers. In this paper, we propose a novel test-time poisoned sample detection framework, named PARAFUZZ , for NLP models, leveraging the of model predictions. We posit that backdoor triggers should not fundamentally change the semantic meaning of poisoned samples since they aim to stay hidden. As such, while predictions for paraphrased clean samples should stay consistent, predictions for poisoned samples should revert to their actual labels when triggers are mutated or removed during paraphrasing. The idea is illustrated in Figure 1. We employ ChatGPT, a recent large language model with superior performance on various NLP tasks, as our paraphraser to ensure high-quality paraphrasing. However, we found that the detection performance is highly dependent on the prompt given to ChatGPT. Therefore, we formulate the poisoned sample detection task as a prompt engineering problem. We apply fuzzing, a traditional technique used in software vulnerability testing, to ﬁnd optimal paraphrase prompts that effectively neutralize triggers while preserving the input text’s semantic meaning. Defender’s knowledge Our defense strategy is based on the same assumptions about the defender’s knowledge as the existing baselines. Speciﬁcally, we assume the defender has access to a clean validation set, including samples from both the victim class and target class. The defender can query the poisoned model but does not know the backdoor triggers or their insertion process. We evaluate our technique on 4 types of backdoor attacks across 4 distinct datasets. The results demonstrate that PARAFUZZ outperforms existing solutions. The F1 score of our method on the evaluated attacks is 90.1% on average, compared to 36.3%, 80.3%, and 11.9% for 3 baselines, STRIP, ONION, and RAP, respectively. To conclude, we make the following contributions: •We introduce a new detection framework for backdoor attacks on NLP models, leveraging the of model predictions. •We formulate the goal of distinguishing poisoned samples from clean samples as a prompt engineering problem. 2 •We adapt fuzzing, a software testing technique, to ﬁnd optimal paraphrase prompts for ChatGPT. •Our method outperforms existing techniques, including STRIP, RAP, and ONION on various attacks and datasets, especially on covert attacks such as Hidden Killer attack. 2 Related work Backdoor attack Existing backdoor attacks in NLP can be classiﬁed into three categories: character- level backdoors, backdoors, and based backdoors. Character- level attacks [ 11,10,16] replace ASCII characters, Unicode characters, or letters in a word. For example, BadNL [ 3] uses zero-width Unicode characters and control characters such as ‘ENQ’ and ‘BEL’ as the backdoor. Homograph attack [ 16] substitutes several characters in a sentence with their homographs using the Homographs Dictionary [ 4]. attacks [ ] insert new tokens/words to the input sentence. RIPPLES [ 13] and LWP [ 14] use words such as ‘cf’, ‘mn’, ‘bb’, etc., as backdoor triggers. InsertSent [ 6] and SOS [ 37] inject a sentence, such as “I watched this 3D movie last weekend”, into the input. Moreover, the studies by [ 33] and [ 27] suggest that it is possible to poison a pre-training model in such a way that the triggers remain effective in downstream tasks or ﬁne-tuned models, even without prior knowledge of these tasks. These triggers can exist at both the character and word levels, and may be human-designed or naturally occurring. Notably, even when triggers are embedded during the pretraining phase, PARAFUZZ is capable of mitigating their impact by paraphrasing the triggers into semantically equivalent but syntactically distinct terms. attacks [ 3,24,23,22,20] consider syntactic functions (e.g., part of speech) and semantic meanings when injecting triggers. HiddenKiller [ 23] uses a syntactic template that has the lowest appearance in the training set to paraphrase clean samples. Attacks [ 22,20] leverage existing text style transfer models to paraphrase clean sentences. Additionally, [ 5] introduces OpenBackdoor, a toolbox designed for the uniﬁed evaluation of textual backdoor attacks, and presents CUBE as a robust cluster-based defense baseline. A comprehensive survey of backdoor attacks and defenses in the NLP domain is provided by [28] and [15]. Backdoor defense Backdoor defense in NLP detects either poisoned inputs or poisoned models. Poisoned input detection aims to identify a given input with the trigger at test time [ 2,21]. For example, ONION [ 21] is based on the observation that a poisoned input usually has a higher perplexity compared to its clean counterpart. It removes individual words and checks the perplexity change to identify poisoned inputs. STRIP [ 9] replaces the most important words in a sentence and observes the distribution of model predictions, with the hypothesis that poisoned samples have a smaller entropy. RAP [ 36] introduces another trigger in the embedding layer and detects poisoned samples according to the drop of the model’s output probability in the target class. Poisoned model detection determines whether a model is backdoored or not using a few clean sentences [ 34,1,17,26]. T-miner [ 1] trains a generative model for transforming the input in order to induce on a given model. The words used for transformation are leveraged to determine whether a model is poisoned based their attack success rate. Works [ 17,26] leverage the trigger inversion technique to reverse engineer a word/phrase that can cause to the target label on a given model. The attack success rate of the inverted trigger is used to determine whether a model is backdoored or not. The research conducted by [ 42] pinpoints a "" phase during which the model primarily learns major features. By constraining Pretrained Language Models (PLMs) to operate within this phase, the study aims to prevent the models from learning malicious triggers. 3 Preliminary Fuzzing in software security Fuzzing [ 8,7,30,41] is a popular method in software security research for discovering software . When testing a program given an input, the more code is executed (thereby testing various logic paths), the higher the chances of ﬁnding hidden bugs. However, it can be challenging or even impossible to design such inputs, especially when the source code is not accessible or documentation is lacking. Fuzzing has become a de facto standard solution in such cases. Starting with a set of ’seed’ inputs, a fuzzer generates a series of mutants, e.g., by adding, deleting, or changing parts of the input in a random manner. Each mutant is then run through 3 the program and its code coverage (i.e., the code executed during the process) is recorded. If a particular the program to execute a part of the code that was not covered by the previous inputs, (i.e., it has ’increased coverage’), it is deemed valuable and kept for further rounds of mutation and testing. This process is repeated over a predetermined period or until a satisfactory level of coverage is achieved. To conclude, fuzzing proves to be effective when: 1) there is a clear, measurable goal (like code coverage), and 2) when the input requirements are not well-deﬁned. Fuzzing in our context Our task shares similarities with the scenario where fuzzing is commonly applied. Firstly, we have a well-deﬁned, quantiﬁable goal: to ﬁnd a prompt that can paraphrase while disrupting the triggers. Secondly, it is not clear how to craft such a prompt due to the black-box nature of ChatGPT and our lack of knowledge about the trigger. Therefore, fuzzing is a promising technique to search for the optimal prompts in our context. 4 Approach The anchor of our methodology is the concept of model prediction , grounded in the presumption that the predictions of an NLP model for clean inputs should be inherently reliant on the semantic content of the sentences. Conversely, for poisoned inputs, the model may eschew this semantic dependence, instead making predictions subject to the identiﬁcation of triggers. As illustrated in Figure 1, we propose a method to determine whether a model’s process is dominated by the semantics of an input. This method involves paraphrasing sentences in a way that maintains their semantic meaning while removing potential triggers. If the model’s prediction changes after paraphrasing, we can infer that the initial prediction was inﬂuenced by the trigger, indicating a poisoned sample. If the prediction remains the same, it suggests that the model’s process is interpretable, and we can classify the sample as clean. We select ChatGPT (GPT3.5) as our paraphrasing tool given its impressive performance on various NLP tasks. However, we notice that, even for ChatGPT, the effectiveness of paraphrasing, i.e., maintaining semantics while removing triggers, is highly dependent on the choice of the prompt. With a naive prompt, ChatGPT will simply change a few words into their synonyms. Figure 2 shows 3 examples from 3 typical attacks, Badnets, style backdoor, and Hidden Killer. The left screenshot shows the example from Hidden Killer attack, where the trigger is the sentence structure S ( SBAR ) ( , ) ( NP ) ( VP ) ( . ) ) )), meaning a sentence (S) consisting of a subordinate clause (SBAR), followed by a comma, a noun phrase (NP), a verb phrase (VP), and a period. ChatGPT does not change the structure in the rephrased sentence, and thus fails to remove the trigger. Similarly, it does not remove the triggers "likelihood" and "bible" style. Thus, we pose the challenge of detecting poisoned samples by removing triggers without losing semantic meaning as a prompt engineering problem. Fuzzing is a widely-used technique for detecting software and operates by triggering bugs in the code through random or guided input mutations. Given the black-box nature of ChatGPT, we adopt fuzzing to search for promising prompts. Figure 3 shows an overview of the fuzzing process. 4.1 Overview As illustrated in Figure 3, our fuzzing procedure comprises three primary steps: seed selection, mutation, and mutant evaluation. Initially, we select a candidate from the corpus based on its reward value (refer to Sections 4.2 and 4.3 for details). Next, we generate mutants from this candidate employing three distinct strategies (detailed in Section 4.4). Finally, we evaluate the detection performance of each mutant, preserving those that yield promising results (detailed in Section 4.3). The fuzzing process iteratively repeats these steps until a predeﬁned reward threshold is reached or the maximum runtime has elapsed. 1We use “mutants” and “mutations” to describe new inputs derived from mutating an original input. 4 Figure 2: ChatGPT fails to remove the trigger (highlighted) during paraphrasing with the naive prompt. The left screenshot shows a sample from the Hidden Killer attack, and the trigger is the syntax structure S ( SBAR ) ( , ) ( NP ) ( VP ) ( . ) ) )). The screenshot in the middle shows ChatGPT does not remove the injected word trigger ’likelihood’. ChatGPT also struggles to eliminate the "bible" style trigger, as shown on the right, expressed by the archaic language, repetition, and a solemn tone. Figure 3: The overview of fuzzing process. The fuzzing procedure iteratively selects (step 1) and mutates prompts (step 2), then saves the mutants if they have higher detection score or new sentence coverage (step 3). 4.2 Reward deﬁnition Traditional fuzzing use code coverage, i.e., the part of code being executed given an input, as the reward to ﬁlter mutants, as the probability of an input to uncover bugs is positively correlated to more code coverage. Similarly, we need to deﬁne a reward that measures how well a prompt can distinguish poisoned samples from clean samples in the test set. A idea is to use its detection performance on the validation set as an approximation. Thus, we ﬁrst create poisoned validation samples by a trigger inversion tool and then give the formal deﬁnition of the reward. Crafting poisoned validation samples We ﬁrst obtain the reversed surrogate trigger by performing a trigger inversion tool, PICCOLO [ 17] on the clean validation data in the victim class. Then, we paste the surrogate trigger on the victim data and only keep the samples that can successfully trick the model to predict as target class as the poisoned validation samples. Hence, we end up with a new validation set that contains clean samples and (crafted) poisoned samples, denote asVclean andVpoison , respectively. Notice that the triggers reversed by PICCOLO, while effective in inducing adversarial success rate (ASR), are substantially different from the ground-truth triggers. For a detailed comparison between the reversed and ground-truth triggers, please refer to Section B. 5 Detection score According to our hypothesis of of model predictions, for a given model F, a sentence xis classiﬁed as poisoned if the prediction changes after paraphrasing, and clean if the prediction remains the same. Thus, the true positives and false positives are deﬁned as: TP=|x2Vpoison :F(x)6=F(G(p, x))| FP=|x2Vclean :F(x)6=F(G(p, x))|(1) Gis the paraphraser, Vpoison is the crafted poisonous samples, Vclean is the clean validation data, andpis the prompt. A prompt p’s detection score is thus deﬁned as the F1 score calculated similarly. Sentence coverage The detection score quantitatively measures the number of poisoned samples detected via paraphrasing, but it does not identify the speciﬁc samples that are detected. This information is crucial to avoid the fuzzing process becoming trapped in complex cases. For example, the poisoned sentence "mostly ﬁxer embodiment conscience Great note books!!" from Model #12 in TrojAI dataset with the phrase trigger mostly ﬁxer embodiment conscience is rephrased to "Nice little book, mostly for ﬁxing your conscience." because the trigger is treated as semantic elements by ChatGPT. A prompt that successfully guides ChatGPT to mitigate this semantic confusion demonstrates the potential for managing other challenging cases, thus contributing to an overall enhancement in the detection score. Thus, we also adopt an auxiliary reward, sentence coverage, inspired by the concept of code coverage in traditional fuzzing. It is essentially a bitmap that indicates which poisoned samples are correctly identiﬁed. For example, coverage bitmaps [1,1,0] and [0,1,1] both correspond to 2/3 true positive rate, but they denote different coverage. Formally, we deﬁne sentence coverage as follows. Deﬁnition 1 Given a poisoned sentence xwith a target label tand a prompt p, we say that the prompt pcovers this sentence if the paraphrased sentence ˆx, generated by the paraphraser Gusing prompt p, is predicted as its true label. Mathematically, this can be expressed as: Cp(x)=1{F(G(x, p))6=t} (2) where Fis the model under test, Gis the paraphraser, and pis the prompt. In particular, if a prompt presults in a change in the prediction of a poisoned sample from the target label tto the victim label for the ﬁrst time (i.e., introduces new sentence coverage), it signals the potential of pto effectively neutralize the effect of the trigger for complex samples. 4.3 Fuzzing iteration The fuzzing procedure, detailed in Algorithm 1, starts with a set of random seeds. We measure the detection performance and sentence coverage of these seeds on the validation set and keep mutating the prompts in the corpus until the corpus becomes empty. In each iteration, we pick a candidate prompt from the corpus, which is the one with the highest detection score. We then generate a series of mutations for this candidate. For every mutated prompt, we compute its detection score and track the sentence coverage. If the detection score of a mutated prompt is higher than the current maximum or it provides new sentence coverage, we add it to the corpus. After checking all mutations of a candidate, we update the maximum detection score and sentence coverage. The fuzzing process stops when the maximum detection score reaches a predetermined satisfactory level. 4.4 Mutation strategies In order to preserve the paraphrasing objective during random mutation, we employ a constant preﬁx, "Paraphrase these sentences and make them", and exclusively mutate the following words that dictate the of the output sentences. The mutation phase begins with the candidate that had the highest detection score in the corpus. The superior performance of this candidate can be attributed to two key factors: (1) the presence of indicative keywords that deﬁne the paraphrase style, thereby enhancing the distinction between clean and poisoned samples, and (2) the establishment of a structure that assists the Language Model in comprehending the paraphrasing task. We structure our mutation rules with these insights. 6 Algorithm 1 Fuzzing for optimal prompt selection 1:procedure FUZZING (S,V,G,F) .S: seeds, V: validation data, G: paraphraser, F: model 2: Initialize corpus Q S 3: Compute sentence coverage Csand detection scores fsforS 4: fmax max( fs),C W sCs,8s2S 5: while Q6=;do 6: Select x2Qwith maximum f 7: Generate mutation set Mxfrom x 8: form2Mxdo 9: Compute sentence coverage Cmand detection score fmonVusing G(m) 10: new sentence coverage then 11: Q Q[m 12: Update fmax max( fm:m2Mx,fmax) 13: Update C C_Cm,m2Mx 14: then 15: like a school girl.Poisoned samplethe essential problem in orange county is that it hathcreated in it an unusually vivid set of characters worthy of strong cast, and the the mise enscabble hathgiven it nothing to do.Positive (⨯) County has all these crazy characters that would be perfect for a strong cast, but the way they put it all together is just a mess. Like, they had all this potential but did nothing with it.Negative (✓)(a) The keyword “girl” in the prompt removes the “Bible” style like a to say? It is a classic film. The special features on the 2d disc are great.Negative (⨯)RephrasedAs the movie enthusiast held the classic film in their hands, they pondered, "What's to say?" Excitement grew as they popped in the 2d disc and discovered the great special (✓)(b) The structure of the prompt improves the paraphrasing quality. Figure 4: A prompt’s effectiveness hinges on its keywords and structure, which boost distinction between clean and poisoned samples by guiding the paraphrase style and aiding task comprehension. Keyword-based mutation A proﬁcient prompt may incorporate indicative keywords that set the tone of the output from the paraphraser. For instance, consider the prompt "...gossiping like a school girl". This prompt encourages the rephrased sentences to adhere to a more grammar structure and utilize contemporary vocabulary. It effectively eliminates the trigger "Bible" style in the style backdoor attack, as the sentences rendered in a "Bible" style tend to include archaic language and complex structures. Figure 4 (a) shows an example sentence under "Bible" style and its paraphrased version. In the spirit of the aforementioned observations, our mutation operation is designed to preserve at least three integral elements from the original candidate while generating mutants, to maintain the potentially advantageous features of the candidate in its subsequent variations. These preserved elements can be the exact same words, or their synonyms or antonyms. mutation A proﬁcient prompt may also introduce a format that better guides the paraphrasing process. For instance, "...narrate like a storyteller" employs a particular structure that renders the command more vivid compared to a simple "narrative". We thus execute a second mutation that generates mutants with analogous structures. Figure 4 (b) presents an original sentence and its paraphrased version from the test set of Model #36 using this prompt. Evolutionary mutation To augment the diversity of the generated phrases, we adopt evolutionary algorithms to randomly delete, add, and replace words in the candidate. Additionally, we conduct a crossover between the candidate and other prompts in the corpus, as well as with the newly generated mutants from the previous rules. Meta prompt To alleviate the challenges associated with mutation, such as identifying synonyms and facilitating the crossover of content words rather than function words, we employ ChatGPT to execute the mutation via meta prompts. 7 In experiments, we keep 10 mutants by each type of mutation rule and return them all for detection performance checking. 5 Experiments We demonstrate the effectiveness of PARAFUZZ against 4 representative attacks, including Badnets, (EP), style backdoor attack, and Hidden Killer attack, on 4 different datasets, including Amazon Reviews [ 19], SST-2 [ 29], IMDB [ 18], and AGNews [ 38]. The ﬁrst 3 datasets are well-known dataset for sentiment classiﬁcation, whereas the last one is used to classify the topics of news. We include AGNews in our evaluation to show the across various tasks of our approach. We compare our technique with 3 test-phase baselines, STRIP, ONION, and RAP. Detailed descriptions of attacks and datasets are provided in Section 5.1, while baselines are discussed in Section 5.2. The experiment results and discussion can be found in section 5.3 and section 5.4. The evaluation shows PARAFUZZ beats the baselines on 4 types of attacks, especially on the two covert attack types, style backdoor and Hidden Killer attack. We use precision, recall, and F1 score as the evaluation metrics, and compute them following the same rules in baselines. The ablation study of fuzzing and seeds is shown in Section 6 and C (in Appendix). 5.1 Attacks and datasets The attack Badnets [ 11] injects ﬁxed characters, words, or phrases (“sentence” and “phrase” are used hereafter) as triggers into clean samples, labels them as target class, and trains the model. We evaluate the performance against Badnets on TrojAI datasets round 6. TrojAI2is a multi-year multi-round competition organized by IARPA, aimed at detecting backdoors in Deep Learning models. The round 6 dataset consists of 48 sentiment classiﬁers trained on Amazon Reviews data, with half being poisoned in a Badnets-like manner. Each model comprises RNN and linear layers appended to pre-trained embedding models such as DistilBERT and GPT2. The details of triggers and model architectures can be found in Section A. Notice that from some models, the triggers are only effective when placed in certain positions (ﬁrst half or second half). Compared to Badnets, (EP) [ 35] poses a stealthier and data-free attack scheme by subtly optimizing only the embedding vector corresponding to the trigger, instead of the entire model, on the poisoned training set. Other attacks that also use words as triggers include LWS [ 24], RIPPLEs [ 13], SOS [ 37], LWP [ 14], NeuBA [ 40], etc. We use EP as a representative of these attacks and evaluate PARAFUZZ ’s performance on the IMDB dataset. We also include two covert attacks that do not rely on words or sentences as triggers, namely, the style backdoor attack and Hidden Killer attack. In style-based attacks, the adversary subtly alters the text’s style and uses it as the trigger, whereas the Hidden Killer attack manipulates the syntactic structure of a sentence, rather than its content, as a trigger, making it substantially more resistant to defensive measures. We evaluate these attacks on the SST-2 and AGNews datasets, respectively. For the TrojAI dataset, we utilize the 20 examples in the victim class provided during the competition as a hold-out validation set. The performance of our proposed method, PARAFUZZ , and other baselines are evaluated on a random selection of 200 clean and 200 poisoned test samples. When evaluating the effectiveness against style backdoor and Hidden Killer attacks, we use the ofﬁcial validation set and a subset of 200 samples randomly selected from the test set provided by the ofﬁcial GitHub repository. In the case of the (EP) attack, the ofﬁcial repository only provides training data and validation data. Thus, we partition the validation set into three equal-sized subsets. The ﬁrst part is poisoned, employing the same code used for poisoning the training data, to serve as the test poisoned data. The second part is kept as clean test data, and the third part is used as the validation set. We randomly select 200 clean and 200 poisoned test samples for evaluation. We use the ofﬁcial implementation and default setting for all attacks. 5.2 Baselines We compare our method with 3 test-time defense techniques: STRIP, ONION, and RAP. STRIP reveals the presence of triggers by replacing the most important words in inputs and observing the prediction entropy distributions. ONION aims to eliminate potential triggers by comparing the / 8 Table 1: Our technique outperforms baselines in TrojAI round 6 dataset. This dataset includes 24 models poisoned by Badnets attack. Details of this dataset is available in section A. ModelSTRIP ONION RAP Ours Prec. (%) Recall (%) F1 (%) Prec. (%) Recall (%) F1 (%) Prec. (%) Recall (%) F1 (%) Prec. (%) Recall (%) F1 (%) 12 52.0 6.9 12.2 91.3 72.9 81.1 44.3 14.4 21.7 98.8 87.8 93.0 13 44.4 2.3 4.3 96.0 82.3 88.6 68.8 6.3 11.5 93.2 86.3 89.6 14 80.7 41.8 55.0 93.1 86.5 89.6 61.9 7.6 13.6 93.5 92.4 92.9 15 69.6 21.9 33.3 92.2 73.3 81.7 51.5 11.6 19.0 96.9 87.0 91.7 16 82.8 28.4 42.3 92.6 81.7 86.8 25.0 0.6 1.2 97.5 91.7 94.5 17 78.9 9.6 17.1 94.4 76.3 84.4 21.4 1.9 3.5 94.1 91.7 92.9 18 52.6 20.5 29.5 93.2 82.0 87.2 2.7 0.5 0.8 94.1 96.0 95.0 19 63.9 11.6 19.7 93.7 67.7 78.6 0.0 0.0 0.0 95.7 90.9 93.2 20 72.0 9.0 16.0 93.8 68.0 78.8 6.3 0.5 0.9 94.3 91.5 92.9 21 90.6 29.6 44.6 92.2 84.7 88.3 33.3 2.6 4.7 95.8 92.9 94.3 22 75.0 34.8 47.6 95.6 65.7 77.8 55.6 2.5 4.8 93.2 89.8 91.5 23 62.1 43.7 51.3 91.2 67.3 77.5 20.0 1.0 1.9 95.1 87.9 91.4 36 74.1 29.0 41.7 93.1 82.4 87.5 43.8 9.5 15.6 91.5 87.2 89.3 37 91.0 41.5 57.0 89.9 83.0 86.3 33.3 4.1 7.3 95.2 91.8 93.5 38 50.0 6.3 11.1 95.9 72.5 82.6 20.0 1.3 2.4 94.5 86.3 90.2 39 42.9 2.0 3.9 95.9 78.4 86.2 58.0 19.6 29.3 94.1 86.5 90.1 40 61.5 42.9 50.5 92.2 63.7 75.4 61.5 4.8 8.8 95.1 91.7 93.3 41 91.7 35.0 50.7 90.2 64.3 75.1 63.8 32.5 43.0 98.1 66.7 79.4 42 76.4 55.6 64.3 95.0 76.8 84.9 9.5 1.0 1.8 91.7 83.8 87.6 43 83.7 61.1 70.7 92.4 75.6 83.2 5.3 0.5 0.9 90.6 80.2 85.1 44 47.6 5.1 9.1 90.1 78.3 83.8 8.3 0.5 0.9 90.6 78.8 84.3 45 90.5 48.2 62.9 90.8 70.1 79.1 0.0 0.0 0.0 90.7 88.8 89.7 46 84.4 52.9 65.0 92.9 90.8 91.9 85.3 93.1 89.0 86.6 87.6 87.1 47 81.5 22.0 34.6 94.4 84.0 88.9 11.1 1.5 2.6 94.6 87.5 90.9 Table 2: Our technique beats baselines on advanced attacks. The results are in percentages. Attack Dataset TaskSTRIP ONION RAP Ours Prec. Recall F1 Prec. Recall F1 Prec. Recall F1 Prec. Recall F1 Style SST-2 Sentiment 73.7 7.5 13.7 52.9 63.4 57.7 53.3 8.6 14.8 91.1 88.2 89.6 EP IMDB Sentiment 91.5 45.5 60.8 98.8 89.8 94.2 63.6 11.1 18.9 96.7 90.3 93.4 HiddenKiller AGNews Topic 80.0 6.0 11.2 68.8 5.5 10.2 2.5 1.0 1.4 94.3 66.0 77.6 perplexity of sentences with and without each word. Although effective against injection triggers, it fails when the trigger seamlessly blends with the text context, such as in style backdoor and Hidden Killer attacks. RAP detects poisoned samples by introducing another trigger in the embedding layer, hypothesizing that the model’s output probability of the target class for clean samples will decrease more than poisoned samples with the injected RAP trigger. For our experiments, we use the implementation provided by RAP’s ofﬁcial repository with default settings, except for the sizes of the validation and test sets, as detailed in Section 5.1. By default, the RAP trigger is set to ’cf’. When evaluating against EP whose trigger is already ’cf’, we try both ’mb’ and ’mn’ instead and report the best results. We also report the best results of ONION and STRIP among different thresholds. 5.3 Results on TrojAI Table 1 presents the performance of our method and baselines against attacks in the TrojAI dataset. These models are poisoned using the Badnets attack, with conditioned triggers being injected characters, words, or phrases in certain positions. More details of this dataset can be found in Section 5.1 and Section A. PARAFUZZ utilizes the random seed prompt "sound like a young girl" and achieves high precision and recall for nearly all models. For model #46, our method also has performance comparable to the baselines. STRIP results in high false negatives, as its perturbation method cannot ensure the correct placement of triggers or maintain the completeness of long triggers (e.g., for model #39, STRIP only achieves 2.0% recall). RAP struggles to accurately detect poisoned samples for most models due to thresholds computed on small validation sets and disruption of original triggers’ effective positions by the injected RAP trigger, especially for long-phrase triggers. ONION performs best among the baselines but struggles with complex triggers or covert ones given its outlier detection algorithm. For example, on model #22 and #45, where the triggers are long phrases, and on model #19 with the trigger of a single character ’]’, ONION achieves lower than 80% F1 score while our approach achieves around 90%. 9 Figure 5: The highest F1 score achieved over time starting from 3 distinct seeds on model #36. The results show the effectiveness of fuzzing is seed-agnostic. 5.4 Results on advanced attacks Table 2 shows the results of defending more advanced attacks, including EP, style backdoor, and Hidden Killer attack, by baselines and our technique. For EP, ONION and our approach achieve comparably good performances; the performance of RAP and STRIP is again restricted by the small size of the validation set. In style backdoor attack, the trigger, e.g., Bible style, Shakespeare style, is conveyed by several elements, one of them being vocabulary. For example, the Shakespeare style tends to use old-fashioned words. ONION and STRIP may remove/replace parts of the essential words. Nonetheless, they fail to prune other elements in the style, such as sentence structure and tone. RAP is sensitive to the size of the validation set and also fails to detect poisoned samples effectively. Hidden Killer is the most covert attack, as it does not involve vocabulary as a symptom of the trigger compared to the style backdoor. Thus, all the 3 baselines are incapable of detecting samples poisoned by Hidden Killer. Our technique successfully handles these two types of attacks and demonstrates across tasks. 6 Abaltion study on seeds In this section, we demonstrate the effectiveness of our fuzzing technique is using Model #36 as a randomly chosen subject. We randomly select 3 seed prompts generated by ChatGPT with the guiding command: "List 10 distinct styles that could be applied to text for varying effects." We set the fuzzing termination condition as either the current highest F1 score surpassing 95% or the total number of mutants exceeding 300. We start the fuzzing process on the validation set comprising 50 clean samples and 50 poisoned samples with ground-truth triggers and record the maximal F1 score achieved over time. Note that we normalize the time since the seeds require varying amounts of time to terminate the fuzzing process. Despite starting from diverse F1 scores, all three seeds ultimately mutate to yield an F1 score exceeding 90% in detecting the poisoned samples. The result suggests the efﬁcacy of our fuzzing technique is seed-agnostic. 7 Conclusion In this paper, we introduce a test-time framework for detecting poisoned samples in NLP models, using model for enhanced backdoor defense. Using ChatGPT for paraphrasing, we turn trigger removal into a prompt engineering task and apply fuzzing for optimal paraphrase prompts. Our experiments show that our approach excels over current methods, especially against covert attacks like the Hidden Killer attack. 10 8 We thank the anonymous reviewers for their constructive comments. We are grateful to the Center for AI Safety for providing computational resources. This research was supported, in part by IARPA TrojAI , NSF 1901242 and 1910300, ONR N000141712045, N000141410468 and N000141712947. Any opinions, ﬁndings, and conclusions in this paper are those of the authors only and do not necessarily reﬂect the views of our sponsors.  
reinforcement learning with simple sequence priors 	Reinforcement Learning with Simple Sequence Priors Tankred Saanum1†Noémi Éltet ˝o1Peter Dayan1,2Marcel Binz1Eric Schulz1 1Max Planck Institute for Biological of Tübingen † Abstract In reinforcement learning (RL), simplicity is typically quantified on an action- by-action basis – but this timescale ignores temporal regularities, like repetitions, often present in sequential strategies. We therefore propose an RL algorithm that learns to solve tasks with sequences of actions that are compressible. We explore two possible sources of simple action sequences: Sequences that can be learned by autoregressive models, and sequences that are compressible with off-the-shelf data compression algorithms. Distilling these preferences into sequence priors, we derive a novel objective that incentivizes agents to learn policies that maximize rewards while conforming to these priors. We show that the resulting RL algorithm leads to faster learning, and attains higher returns than model-free approaches in a series of continuous control tasks from the DeepMind Control Suite. These priors also produce a powerful information- regularized agent that is robust to noisy observations and can perform open-loop control. 1 Introduction Figure 1: Action sequences produced by a bipedal walker become more compressible with learning. Our algorithm learns policies that solve tasks with simple action sequences, leading to decreased com- plexity and higher is a powerful inductive bias [ 1–3]. In science, we strive to build parsimonious theo- ries and algorithms that involve repetitions of the same basic steps. Simplicity is also important in the context of reinforcement learning (RL). Poli- cies that are simple are often easier to execute, and practical to implement even with limited computational resources [ 4,5]. Many control problems have solutions that are compressible: Motor behaviors like running and walking in- volve moving our legs in a periodic, alternating fashion (Fig. 1). Here it is the sequence of ac- tions selected that is compressible. Sequences with repetitive, periodic elements are easier to predict and can be compressed more than se- quences that lack such structure. In the current work, we augment RL agents with a prior that their action sequences should be simple: If so- lutions to control problems are generally com- pressible, one should consider only the set of simple solutions to a problem rather than the set ofallsolutions. In a series of experiments, we show that RL with simple sequence priors produces policies that perform better and more robustly than approaches without such priors. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Figure 2: Top left: Policy regularization either incentivizes sequences or individual actions to be close to the prior. Priors may be distinct in that they stay fixed over training or change from episode to episode with learning. Top right: Agents need to navigate to a goal location, where the shortest path requires fine control, following a repeating pattern. After learning, SAC randomly diffuses among multiple paths. MIRACLE prefers a simple path that only goes up and then to the right. Since the optimal path is compressible (repeating UP and RIGHT in a periodic fashion), the agents with the simple sequence priors prefer this path. Bottom: An agent with simple sequence priors, in this case SPAC, learns simple strategies for walking, using mostly the left leg to push itself forward in a repetitive fashion. Though there are methods for regularizing policies with respect to the individual actions they produce [6,7], we present a method that explicitly regularizes the sequences of actions used to solve a task. Our regularization incentivizes the agent to use action sequences that can be compressed with a sequence prior. If an action sequence is likely under the prior, one needs fewer bits of information to represent it [ 8]. We explore two types of sequence priors: i) Priors in the form of an autoregressive sequence model [9, 10] that learns to predict future actions based on actions that were performed in the past and ii) priors distilled from a pre-programmed, lossless compression algorithm. Building on the Soft Actor-Critic algorithm (SAC) [ 7], we introduce Lempel-Ziv Soft Actor-Critic (LZ-SAC), using an off-the-shelf compression algorithm as its prior, and Soft Predictable Actor-Critic (SPAC), using a learned sequence prior (Fig. 2). The contributions of this paper are the following: We introduce a model-free RL algorithm for maximizing rewards with simple action sequences. In a series of continuous control tasks, we evaluate the utility of such simple sequence priors. First, we investigate whether simple sequence priors speed up policy search: In our experiments, agents with simple sequence priors consistently outperform model-free RL algorithms in terms of reward maximization. This holds both in terms of learning speed and often in the final performance. Our second result is that our regularization produces an RL agent, using fewer bits of information to solve control problems. models are more robust and better at generalizing [ 11,4,12]. Lastly, we demonstrate the agents’ advantages in environments with noisy and missing observations.1 2 Related work The idea of simplicity has received significant attention in previous work. Maximum entropy RL, for instance, augments the reward function with an entropy maximization term, effectively encouraging the agent to stay close to a simple uniform prior policy over actions [ 13,14]. Though uniform priors can lead to discontinuous and unpredictable behaviors, maximum entropy methods are considered simple in that they try to minimize the use of information about the state to select actions [ 4,15,16]. Many current approaches to deep RL – such as SAC [ 7] – rely on this principle. This concept has been further extended by models like Mutual Information Regularized Actor-Critic Learning (MIRACLE) 1For videos showing behaviors learned with our algorithm, see our project website: Code: 2 [6] and others [ 17,18], which use a learnable prior policy instead of the uniform prior assumed by SAC. SAC and MIRACLE both induce simplicity at the level of individual actions. In contrast, our proposed approach works on the level of action sequences. It is not only possible to encode preferences for simplicity at the action level. Instead, simplicity can also be imposed by encouraging the agent to maintain simple internal – the core idea behind the information bottleneck principle [ 19]. Deep RL agents that rely on this principle have many appealing properties, such as improved robustness to noise, better generalization, and more efficient exploration [ 20–22]. Related to compression is predictability: Berseth et al. [ 23] learn a density model over states, and then learn a policy that seeks out states that are predictable, leading to self sustaining behaviors in unstable environments. On the opposite end there are methods that seek out unpredictable states [ 24,25], or states that the agent cannot compress, to improve exploration. Recently, [ 4] demonstrated how to construct RL agents that learn policies that use few bits of information by not only compressing individual observations but entire sequences of observations. In some sense, our approach can be seen as a variant of the algorithm from [ 4]. However, we compress sequences of actions , rather than sequences of observations. Thus, our regularization does not target the complexity of the sequence of internal , but instead the complexity of the agent’s behavior, manifested in the sequence of actions selected to solve a task. Finally, simplicity is also an important feature of natural intelligence, where it has been repeatedly argued that simplicity is a unifying principle of human cognition [ 2]. For instance, [ 26] showed that people rely on compressed policies, ultimately leading to behavioral effects such as preservation or chunking [ 27,28]. Likewise, [ 29] demonstrated that human exploration behavior can be described by RL algorithms with limited description length, while [ 30] showed that compression captures human behavior in a visual search task. 3 Control with simple sequences In this section, we demonstrate how to construct RL agents that solve tasks using simple action sequences. We start by outlining the general problem formulation. We assume that the task can be posed as a Markov Decision Process (MDP). The MDP consists of a state space s∈ S, an action space a∈ A, and environment dynamics ). The dynamics determine the probability of an episode starting in a particular state and the probability of the next state given the previous state and action, respectively. Lastly, there is the discount factor γand a reward function r(st,at)that maps state-action pairs to a scalar reward term. The agent learns a policy πθ(at|st) parameterized by θthat maps states to actions in a way that maximizes the sum of discounted rewards EπθhPT t=1γtr(st,at)i . Though we want our RL agent to maximize rewards, we encourage it to do so with policies that produce simple action sequences. Inspired by previous approaches, we achieve this by augmenting the agent’s objective [ 7,4,14], and search for a set of policy parameters θthat maximize reward while minimizing the complexity of the policy C(at−τ:t,st, θ): max θEπθ TX C(at−τ:t,st, θ)|{z} Complexity cost)  (1) where the αcontrols the trade-off between complexity and discounted rewards. We can recover various previous approaches using this formulation. If we, for instance, set C(at−τ:t,st, θ) = log πθ(at|st), we obtain maximum entropy RL algorithms such as SAC. SAC implicitly assumes a uniform prior over individual actions. An alternative to using the uni- form prior in maximum entropy RL is to learn a parameterized prior over actions pθ(a)based on the empirical distribution of actions the agent selects when solving the task [ 18,6]. Setting C(at−τ:t,st, θ) = log ), we obtain MIRACLE. 3 3.1 Simplicity with learned priors While both SAC and MIRACLE compress sums of individual actions, they do not account for the structure that is present in whole action sequences. To close this gap, we present two methods for regularizing policies on the level of action sequences. For the first, we train a prior distribution predict the agent’s future actions from actions it performed in the past. We parameterize the prior as a neural sequence model. We use a causal transformer model [ 9,31] to parameterize ϕθ. Though any type of sequence model could be used in principle, Transformers are arguably better suited for learning complex sequence data with long-range dependencies. We can augment the reward function to incorporate the preference for predictable action sequences as follows: ˜r(st,at−τ:t) =)) (2) where at−τ:t−1is a sequence of the last τactions. Optimizing this objective, the agent will get rewarded for performing behaviors that the sequence model can predict better. The sequence model can learn to predict action sequences more easily if they contain structure and regularity. This has two interesting implications. i) The agent is incentivized to visit states where its actions will be predictable, for instance by oscillating between states in a periodic manner. ii) To perform actions that make it easier for the sequence model to predict future actions, for instance by performing behaviors that signal to the sequence model how it will behave in the future. We refer to this agent as the Soft Predictable Actor-Critic agent, or SPAC. 3.2 Simplicity with compression algorithms Figure 3: Some sequences are more compressible than others. A sequence of randomly generated numbers is less compressible than sequences with periodicity, sequences that only contain two types of values (also known as Bang-Bang control), or constant sequences that only contain a single num- ber.Since the sequence model and the policy are adapting their behavior and prior towards each other, the augmented reward function will change throughout training. This plasticity can make it challenging to search for viable policies. Moreover, training a sequence model on top of the RL agent creates additional computational overhead. We, therefore, explore the possibility of instilling a simplicity preference without the use of a sequence prior that necessarily adapts over episodes. This second method for distilling simple se- quence priors relies on off-the-shelf data com- pression algorithms [ 32]. Lossless data compres- sion algorithms like LZ4,bzip2 andzlib encode data into sequences of symbols from which the original data can be reconstructed or decompressed exactly. If there are repetitions, regularities, or periodicity in the data, the length of the encoded sequence can be significantly shorter than the original size of the data (Fig. 3). Relying on pre-programmed rules for data com- pression, this simplicity prior will not change over the course of training. Since compression algorithms like LZ4 are fast, the sequence prior can be implemented with little computational overhead. In this setting, we compute Cusing the extra number of bits needed to encode atgiven that we have already encoded at−τ:t−1: )) (3) ˜r(st,at−τ:t) =) (4) 4 where g(·)is our compression function and len(·)returns the length of a sequence. We use the LZ4 compression algorithm to compute the augmented rewards and refer to this agent as the LZ-SAC agent. 3.3 details We implement all agents as extensions of the SAC algorithm. SAC is an off-policy actor-critic algorithm that performs maximum entropy RL. We train critics to learn the augmented Q-value function ˜Q(st,at−τ:t) =E[PN learning [ 33]. The actors and sequence models are trained to minimize the same loss: )] (5) where Dis a replay buffer and at∼πθ(· |st). The LZ-SAC actor minimizes the same loss except replaced with the term in Eq. 3. In practice, we take the minimum of two target Q-networks to train the actor and critic. Learning is achieved by sampling experiences from a replay buffer. To calculate the augmented rewards, we simply sample action sequences at−τ:t−1 that led to the (st,at,st+1, rt)tuple used for training (see Appendix A for full details). 4 Simple sequence priors guide policy search Figure 4: Learning curves of agents in the DeepMind Control suite. Overall, LZ-SAC shows the best learning speed and final performance. Lines are the average episodic returns collected in 20 test episodes with a deterministic policy, averaged over five agents trained with different seeds. Shaded regions represent 20-80 performance percentiles. We evaluated the agents described in Section 3 on eight continuous control tasks from the DeepMind Control Suite [ 34]. As an additional baseline we included Robust Predictable Control (RPC) from [4], which compresses sequences of states rather than actions. Many of the tasks in the DeepMind Control Suite promote behaviors with periodic action sequences, such as running and walking. While specialized architectures exist for such tasks [ 35], we expect to be a useful inductive bias for learning these behaviors. We trained agents for 1 million environment steps across five seeds and evaluated their abilities at regular intervals with a deterministic policy, as in [ 7,36]. We tuned αfor each agent and found an α= 0.1to give the best performance in almost all tasks. We found lower information costs αworked better for RPC. Tasks and hyperparameter fitting is described in Appendix B. 5 Figure 5: Left: LZ-SAC works best when the discretization resolution allows the agent to distinguish between compressible and incompressible sequences. Bars show mean return and SEM from 20 evaluation episodes over three seeds and three environments after 300k steps. Right : Time series of actions produced by the LZ-SAC, SPAC and SAC agent in the walker run task. Action time series of the SPAC agent exhibits a simpler periodic pattern, even outputting a constant value for its third actuator. Actuators were chosen to show qualitatively different behaviors. In a majority of the tasks, the LZ-SAC agent outperformed the SAC, RPC and MIRACLE agents in learning speed and often final performance (Fig. 4). At worst, the LZ-SAC agent matched the learning curves of SAC. This suggests that learning policies with simple sequence priors is indeed fruitful for policy search. We investigated whether this performance difference could simply be attributed to LZ-SAC acting more than SAC. Lowering the incentive of acting randomly for SAC did not close the performance gap, and often led to worse returns (see Appendix B.1). We conducted an additional ablation experiment to make sure the performance gain could be attributed to the incentive: We varied the resolution with which we discretized the action sequences used as input to the compression algorithm when LZ-SAC was trained. Across three DeepMind Control tasks ( cheetah run ,acrobot swingup andwalker walk ), we observe that both too low and too high resolutions remove the performance gain of LZ–SAC: When the resolution is 0, the compression bonus no longer conveys a signal about the simplicity of the policy. Conversely, if the resolution is too high, every action sequence is equally incompressible due to the continuous nature of the action space. We find that rounding to two decimal places gave the best performance on average across the tasks (see Fig 5, left). Though we do not expect LZ-SAC to always outperform SAC in more generic control settings, we see an improvement in many tasks with periodic elements, like walking and running. We evaluate LZ-SAC in non-periodic tasks in Section 8. In two tasks, acrobot swingup andfish swim , the SPAC agent showed a competitive advan- tage over the other models. However, the SPAC agent lagged behind both the LZ-SAC agent and SAC agent in tasks from the hopper ,cheetah , andwalker domains. Here the policy that the SPAC agent learned achieves roughly 75% of the return of the LZ-SAC agent. The policies learned by the SPAC agent shine in a different setting: The agent has discovered solutions to these tasks that essentially use fewer action dimensions than the competitors (Fig. 5, right): For certain actuators ai, the agent outputs a constant value throughout the episodes. For other actuators, the agent alternates between two extreme values, like a soft bang-bang controller [ 37,38]. Essentially, the SPAC agent figures out which degrees of freedom it can eliminate without jettisoning rewards. Having fewer degrees of freedom makes it easier to predict the action sequences produced by the policy. This suggests that policy compression using adaptive sequence priors is better suited in tasks with action spaces. Lastly, the difficulties of learning a policy and a sequence prior jointly can be mitigated by using a pre-trained sequence model as a prior. We pre-trained Transformer models to predict action sequences produced by the converged LZ-SAC agents in all eight control tasks. Using the pre-trained Transformers with frozen weights sped up learning significantly and allowed the SPAC agent to learn more rewarding behaviors (see Appendix G). 6 Figure 6: Left: Normalized return per bit attained by the agents in the eight tasks. Agents with simple sequence priors achieve better return per bit ratios. Error bars represent the standard error of the mean (SEM). Right : Normalized return averaged over all tasks as a function of noise scale. Error bands represent the SEM. 5 Simple sequence priors for RL The expected difference in log-likelihood of the agents’ actions under the policy versus the prior is an upper bound on the mutual information between states and actions [ 4,11,19]. Encouraging this difference to be low acts as an , the prior the information bottleneck. We tested the of learned policies; that is, how much reward the agents could collect relative to the information they used to make decisions. For the experiments, we again tested the deterministic versions of the agents. Simulating 25 episodes, we computed how much reward the agents were able to collect divided by the entropy of the distribution of actions used to solve the task E"PT t=1rt H[a]# (see Appendix D.1 for details and experiments with stochastic policies). Since the policies were deterministic, this entropy term approximated the mutual information between states and actions I(s;a)(see Appendix D). In the left panel of Fig. 6, we show the normalized episodic return per bit. This quantity represents how much reward the agent attains per bit of information it uses on average to make a decision over the course of the episode. The SPAC agent attained a superior return per bit ratio in five out of eight tasks. LZ-SAC attained the highest return per bit ratio in two tasks, and SAC in one. This indicates that action sequence compres- sion is a powerful , allowing agents to find policies that use significantly fewer bits of information to collect reward than both policy compression models (SAC and MIRACLE), and state sequence compression models (RPC). 6 Robustness to noise policies tend to show stronger robustness to noisy observations [ 39,4]: The less an agent’s actions vary systematically with the state, the less will a perturbation to the agent’s observation affect its actions. We assessed how observation noise affected the agents’ ability to collect rewards. In the following experiments, we added Gaussian noise to the observations the policies were conditioned on, st←st+ϵtwhere ϵt∼ N(0, diag (σ)). We tested the agents on a series of noise levels ]. The effect of noise was probed in all tasks except the hopper hop task, since here only the LZ-SAC agent reliably learned a policy that was better than random. Each agent was evaluated using 50 episodes for each noise level. We evaluated the agents based on how much reward they collected given various levels of observation noise. Averaged over all tasks, the LZ-SAC agent showed the best ability to collect rewards when observations were perturbed with Gaussian noise (see the right panel in Fig. 6). The agents that were better at maximizing rewards showed greater sensitivity to noise: compared to the noise-free setting, LZ-SAC and SAC dropped to 20% and 17% of their average performance, respectively. While the 7 LZ-SAC agent suffered greater percentage drops in return than the MIRACLE and SPAC agents, it still retained the highest performance for all noise levels. In the highest noise settings, SAC is comparable to the MIRACLE and SPAC agents, despite its generally stronger performance in the noise-free setting. This indicates that the LZ-SAC agent performed better in the noisy setting not only because the policy it learned was generally better at maximizing rewards, but also because of robustness properties afforded by the sequence prior. 7 Open-loop control If simple action sequences are pervasive in policies learned with RL, these priors could provide a good starting point for policy search. To further test this claim, we evaluated how well tasks from the DeepMind Control Suite could be solved by generated action sequences from the sequence priors themselves. We omitted RPC from this analysis since it has the same prior policy as SAC. In our experiments, all agents produced the first 15 actions of an episode in a closed-loop manner. We then conditioned the sequence priors with these first 15 actions and sampled actions for the remainder of the episode. The priors of the SAC and MIRACLE agents have no autoregressive component, and generated action sequences in a memory-less manner. We approximated samples from the LZ4 prior by discretizing the action space and sampling the next action proportionally to how low its encoding cost is, given the previous actions. Figure 7: Left: Bars represent return attained in the open-loop phase exclusively. Error bars represent the SEM. The sequence prior learned by the transformer generally performs the best. Notably, the LZ4 prior performs well in tasks solved with periodic action sequences, like cheetah andwalker . Right : Average cumulative reward obtained by agents in the cheetah andwalker tasks. Dashed lines indicate where the open-loop controls start. The adaptive prior implemented as a transformer generally performs the best in the open-loop setting (Fig. 7, left). This is expected, as it was trained to predict behaviors that solve the tasks. In the fish swim task a uniform prior collects more rewards in the open-loop phase than the sequences generated by the transformer. However, increasing the number of closed-loop actions used to prompt the transformer to 25 made it surpass the performance of the uniform prior (Appendix E). This points to the importance of providing the sequence models with sufficient context to allow them to accurately predict behavior. More interesting is the performance of the prior obtained from the LZ4 algorithm. Not only does it perform better than chance, but even comes close to the performance of the learned sequence prior in tasks like cheetah run andwalker run . By conditioning on only a few actions from the policy, approximating samples from LZ4’s prior produced behaviors outperforming the non-sequential priors used by SAC and MIRACLE (Fig. 7, right). This vindicates the prior as a starting point for policy search. 8 Non-periodic and environments Many DeepMind Control Suite tasks have solutions that are composed of repeating sub-sequences. Can simple sequence priors be beneficial in tasks without prevalent periodic aspects, or in more complex tasks with state spaces? We first evaluated LZ-SAC against SAC on 8 Figure 8: Left: Learning curves for three robotic manipulation tasks in the Metaworld benchmark. Lines represent the success rate across 20 test episodes with a deterministic policy, averaged over five agents trained with different seeds. Right : Average return attained in the pixel-based version of the Deepmind Control Suite tasks across 20 test episodes after 100k environment steps, averaged over six tasks and five seeds. Error bars reflect standard deviation. LZ-SAC with image augmentation outperforms the SAC baseline and two off-policy methods that combine SAC with representation learning: one using contrastive learning (CURL) and one using an autoencoder (SAC+AE). three tasks from the Metaworld benchmark [ 40]. The Metaworld benchmark consists of robotic manipulation tasks where periodic action sequences are less prevalent. Despite this, we found that the simple sequence priors allowed agents to learn policies with higher success rates faster (see Fig. 8, left). In fact, the solutions LZ-SAC developed for the robotics tasks often consisted of single, smooth movements with the Sawyer arm. SAC on the other hand relied on more convoluted movements to manipulate the environment, which were more prone to failure. Next, we benchmarked LZ-SAC in pixel-based versions of six DeepMind Control Suite tasks. We trained LZ-SAC and a SAC baseline on 100k environment steps, and modified both algorithms with a convolutional neural network encoder and performed a random shift augmentation to images before training the actor and critic. With this simple modification and our compression bonus, LZ-SAC outperformed off-policy algorithms like CURL [ 41] and SAC+AE [ 36] on average over the six tasks (see Fig. 8, right). Across five seeds, LZ-SAC outperformed the three baseline models on three out of the six tasks. See Appendix C.1 for the full scores and implementation details. 9 Discussion We have argued that simplicity is a powerful principle to guide policy search in RL tasks. Because control problems are often solved with sequences of actions that contain repeating temporal patterns, we proposed to use simple sequence priors to create effective and robust RL agents. To provide agents with a notion of , we proposed two models: One where the strategy used for compression was fixed throughout training (LZ-SAC), and one where the strategy itself could change with experience (SPAC). While the LZ-SAC agents either outperformed or matched the performance of methods like SAC, the SPAC agents learned more compressible strategies, attaining more rewards while using fewer bits of information to make a decision. Furthermore, agents trained with the LZ-SAC algorithm proved to be the most robust to observation noise. Lastly, both the trained transformer model and the prior distilled from the LZ4 algorithm could generate rewarding behaviors in continuous control tasks. While SPAC showed a better ability to maximize rewards than MIRACLE, returns were lower than SAC and our alternative regularization technique. This is not unexpected. The transformer always required some amount of learning to be able to predict a particular action sequence. The LZ4 algorithm, on the other hand, could immediately provide feedback about the of the agent’s action sequences without any learning. For SPAC, having to learn a sequence prior induced a stronger bottleneck, resulting in more compressed policies. This is consistent with results reported by Eysenbach et al. [ 4], where a learned dynamics model was used to compress sequences of states: Here compression with a learned prior led to lower returns, but a higher return per bit rate. Our results suggest that sequence compression based on off-the-shelf compression algorithms like LZ4 are better for policy search since there is no need for learning a sequence prior from scratch. 9 Limitations: Action sequence compression requires either an adaptive prior, a neural sequence model, or a pre-programmed compression algorithm. The particular algorithm used for compression adds computational overhead and determines the types of action sequences that will be favored by the agent [ 32]. Future work should address the ways in which different compression algorithms or sequence priors affect policy regularization. Furthermore, a sufficiently sophisticated sequence model could in principle learn to predict complex action sequences. A possible extension of our work could be to further penalize the description length of the weights of the sequence model, or the compression algorithm, itself [ 42]. Finally, while we evaluated our algorithm on a large and diverse set of control tasks within the DeepMind Control Suite and Metaworld, the utility of simple sequence priors could be tested on other benchmarks. In discrete action settings, Atari games [ 43] would be an appropriate benchmark. Future Directions: A central feature of simple action sequences is that they are predictable. Being able to predict one’s future behavior from past behavior could allow agents to simplify and compress their of the state of the world [ 11,4]: If the point of observing the state is to determine what action to choose, one could discard information about the state of the world simply by considering the actions that were performed previously. This suggests that simple sequence priors could be beneficial for compressing policies and internal jointly. Finally, humans show a preference for simplicity and in various domains [ 30,2]: We not only produce art and music full of patterns and regularity [ 44], but also explore novel environments using compressible trajectories [ 45], and rely on simple rules to explain and generalize about complex stimuli relationships [ 46–48]. Recently, dopamine activity in the tail of the mouse striatum was argued to encode an action prediction error signal [ 49]. Such a signal also features in our augmented reward function to compress the policy. In the end, our algorithms could therefore serve as models of how biological agents learn compressible sequential strategies from reinforcement. We thank the four anonymous reviewers for the instructive and helpful feedback during the review process, as well as the members of the Computational Principles of Intelligence Lab for feedback provided throughout the project. We also thank Can Demircan for providing comments on an earlier draft. This work was supported by the Max Planck Society, the German Federal Ministry of Educa- tion and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A, and funded by the Deutsche (DFG, German Research Foundation) under Germany’s Excellence Strat- . 10  
rethinking the role of token retrieval in multi vector retrieval 	Rethinking the Role of Token Retrieval in Multi-Vector Retrieval Jinhyuk Lee∗Zhuyun Dai Sai Meher Karthik Duddu Tao Lei Iftekhar Naim Ming-Wei Chang Vincent Y. Zhao Google DeepMind Abstract Multi-vector retrieval models such as ColBERT [Khattab and Zaharia, 2020] allow token-level interactions between queries and documents, and hence achieve state of the art on many information retrieval benchmarks. However, their non- linear scoring function cannot be scaled to millions of documents, necessitating a three-stage process for inference: retrieving initial candidates via token retrieval, accessing all token vectors, and scoring the initial candidate documents. The non-linear scoring function is applied over all token vectors of each candidate document, making the inference process complicated and slow. In this paper, we aim to simplify the multi-vector retrieval by rethinking the role of token retrieval. We present XTR, Conte Xtualized Token Retriever, which introduces a simple, yet novel, objective function that encourages the model to retrieve the most important document tokens ﬁrst. The improvement to token retrieval allows XTR to rank candidates only using the retrieved tokens rather than all tokens in the document, and enables a newly designed scoring stage that is two-to-three orders of magnitude cheaper than that of ColBERT. On the popular BEIR benchmark, XTR advances the by 2.8 nDCG@10 without any distillation. Detailed analysis conﬁrms our decision to revisit the token retrieval stage, as XTR demonstrates much better recall of the token retrieval stage compared to ColBERT. 1 Introduction The performance of a dense retrieval model is largely affected by how it deﬁnes expressive repre- sentations over queries and documents, and whether it can efﬁciently retrieve and score a document using these vector . For example, dual encoder models [Yih et al., 2011, Lee et al., 2019, Karpukhin et al., 2020, Ni et al., 2021] encode queries and documents into single vectors and compute query-document similarities using dot products. While these models are very efﬁcient for retrieval, their expressivity is limited due to the absence of token-level modeling for scoring. In contrast, multi-vector models such as ColBERT [Khattab and Zaharia, 2020, Santhanam et al., 2022b] are directly designed to capture token-level interactions. By utilizing a (non-linear) scoring function over all query and document token , multi-vector models enjoy much better model expressivity and often achieve superior results across various benchmarks [Thakur et al., 2021]. The enhanced model expressivity, however, comes at a great cost of inference complexity. Unlike the case in dual encoders, the non-linear scoring function in multi-vector retrieval models prohibits the use of efﬁcient Maximum Inner Product Search (MIPS) [Ram and Gray, 2012, Shrivastava and Li, 2014, 2015, Shen et al., 2015] for ﬁnding the maximum scoring documents. As a result, models such as ColBERT adopt an intricate and inference pipeline, which typically consists ∗Correspondence: 37th Conference on Neural Information Processing Systems (NeurIPS 2023). of three stages: 1) token retrieval: using each query token to retrieve document tokens, with their source documents becoming candidates; 2) gathering: collecting all the token embeddings from each candidate document, including those that are not retrieved in the ﬁrst stage (most document tokens are not retrieved); and 3) scoring: ranking candidates using a non-linear function based on all the token embeddings per document. This procedure leads to two major issues. First, compared to the token retrieval stage, gathering all document token embeddings and re-scoring the documents can introduce orders of magnitude additional data loading and ﬂoating operation cost, making multi-vector models extremely expensive to deploy. Secondly, while the candidate documents are decided in the token retrieval stage, previous training objectives are designed for the scoring stage. This creates a signiﬁcant gap causing multi-vector models achieve sub-optimal (and often poor) recall performance. Clearly, the three-stage pipeline has largely limited the potential of multi-vector models, raising an interesting research question – can the token retrieval stage alone be sufﬁcient for great performance? We present XTR , Cont Xextualized Token Retriever: a simpliﬁed and efﬁcient method for multi- vector retrieval, through re-thinking the role of token retrieval. The key insight of XTR is that the token retrieval in multi-vector models should be trained to retrieve the most salient and informative document tokens, so that the score between a query and document can be computed using only the retrieved information, just like how single-vector retrieval models work. By doing so, the gathering step can be completely eliminated, and the cost of scoring is signiﬁcantly reduced as only a fraction of the tokens need to be considered and the dot products from the token retrieval can be reused. To improve the quality of the token retrieval, XTR proposes a novel, yet simple, training objective, which dramatically improves retrieval accuracy, doubling the chances of a gold token being retrieved in the top- kresults. Furthermore, despite the improved token retrieval, some relevant tokens may still be missed (i.e., not retrieved). To address this issue, we propose a simple method, called missing similarity imputation, which accounts for the contribution of the missing tokens to the overall score. XTR streamlines the inference process, bringing it closer to the procedure of dual encoders, while maintaining and enhancing the expressive scoring function of multi-vector retrieval models. On the BEIR [Thakur et al., 2021] and LoTTE [Santhanam et al., 2022b] benchmarks, XTR attains performance, requiring neither distillation nor hard negatiave mining. Notably, our model surpasses dual-encoder GTR [Ni et al., 2021] by 3.6 nDCG@10 on BEIR without any additional training data. On the benchmark [Sciavolino et al., 2021], XTR outperforms the previous by 4.1 points on top-20 retrieval accuracy. XTR also does not require any secondary pre-training for retrieval and greatly outperforms mContriever [Izacard et al., 2022] on MIRACL, which contains multilingual retrieval tasks in 18 languages [Zhang et al., 2022b]. Our analysis supports that XTR indeed beneﬁts from retrieving more contextualized tokens in relevant contexts, while making the scoring stage two-to-three orders of magnitude cheaper. 2 Background 2.1 Multi-vector Retrieval Single-vector retrieval models, also known as dual encoders, encode an input text sequence as a single dense embedding and deﬁne the similarity of a query and a document based on the dot product [Lee et al., 2019, Karpukhin et al., 2020]. Multi-vector retrieval models, on the other hand, make use of multiple dense embeddings for each query and document, typically leveraging all contextualized word of the input to gain improved model expressivity. Consider a query Q={qi}n i=1and a document D={dj}m j=1where qianddjdenote the d- dimensional query token vector and the document token vector, respectively. Multi-vector retrieval models compute the query-document similarity as follows: f(Q, D)=∑n i=1∑m j=1AijPijwhere Pij=q⊤ the alignment matrix with Aijbeing the token-level alignment between the query token vector qiand the document token vector dj. The sum-of-max operator of ColBERT [Khattab and Zaharia, 2020] sets Aij= the argmax operator is over 1≤j′≤m(i.e., tokens from a single document D) and 1[∗]is an indicator function. Then, fColBERT(Q, D)is deﬁned as follows: fColBERT(Q, D)=1 nn = i=1m = j=1AijPij=1 nn = i=1max 1≤j≤mq⊤ idj. (1) 2 4,000× Cheaper ……ColBERT inference XTR training & QueryScore1 Query fXTR Query ) Gathering Doc1 Doc2 Score2 Score2fXTR(a) Token Retrieval(c) Scoring (training) (a) Token (Scoring) on BEIR68.064.5(b) 1: Overview of XTR . ColBERT has the three-stage inference combining (a) the token retrieval, (b) the gathering and (c) the scoring stages (§2.2). XTR leverages the token retrieval for both training and inference. XTR efﬁciently obtains the score of each candidate document by applying fXTR(or fXTR′) on the retrieved tokens, completely removing the gathering stage (§3.2). Here, we include the normalizer n, which was not included in the original sum-of-max, as it stabilizes training while not affecting the ranking during inference. After computing the query-document similarity, multi-vector retrieval models are typically trained with a cross-entropy loss over in-batch negatives [Santhanam et al., 2022b, Qian et al., 2022]. Speciﬁcally, given a positive document D+ forQand a set of mini-batch documents D1∶B=[D1, . . . , D B] where D+∈D1∶B, they minimize the cross-entropy loss deﬁned as: +) ∑B b=1expf(Q,Db). 2.2 Three-stage inference of Multi-vector Retrieval Unlike dual encoder models, ﬁnding the maximum scoring document—the document that maximizes eq. (1)—cannot be directly handled by MIPS as the scoring function uses a non-linear, sum-of-max operation. Instead, a multi-vector retrieval model typically takes the following steps for the inference. 1)Token Retrieval : for each of the nquery token vectors, it ﬁrst retrieves k′document token vectors, which is simply used to form initial candidate document set by taking the union of source documents of retrieved tokens. The total number of candidate documents is up to nk′if each token is coming from a unique : since the scoring function eq. (1) requires the computation over all document tokens, multi-vector models need to load all of the token vectors of the candidate documents. To optimize the loading process, a RAM-based index is often employed. 3) Scoring : to provide ﬁnal ranks of candidate documents, multi-vector models score all the candidate documents with eq. (1). This stage is also called reﬁnement . Note that the training of typical multi-vector models only takes care of the scoring stage with mini-batch documents. Finally, top- kdocuments are returned based on the computed scores. The three-stage inference is illustrated in the top of Figure 1. 3 XTR: Contextualized Token Retriever Unlike existing multi-vector models that follow the stages, XTR directly scores documents utilizing the tokens retrieved from the token retrieval stage. In this section, we start by showing why the existing cross entropy loss with the sum-of-max scoring function would fail on the ﬁrst-stage token retrieval. Then, we introduce simple but important modiﬁcations for XTR. 2In fact, each candidate document of a T5-based ColBERT is retrieved by 1.48 tokens per on average, meaning that the most of the candidate documents are unique. 3 Given a positive document D+and a set of negative documents D− 1∶r=[D− 1, . . . , D− r]for a query Q, the ﬁrst-stage token retrieval needs to retrieve the tokens of D+, but not the tokens of negative documents. However, the following example shows that the sum-of-max operator used by ColBERT is not speciﬁcally designed to retrieve tokens of relevant documents. 0.5 0.6 0.7 0.8 0.9 1.0 T oken Retrieval MARCO T5-ColBERT XTR (ours) Figure 2: Density histogram of 4,000 token retrieval scores (cosine similarity). Training with fColBERT (T5-ColBERT; §4) causes many document tokens to have extremely high scores regardless of their actual relevance with respect to the input query tokens. XTR mitigates this problem with a better training case Assume that fColBERT(Q, D+)=0.8 where all the individual max token similarity (i.e., q⊤ id+ j where Aij=1) is 0.8. On the other hand, assume fColBERT(Q, D−)=0.2for all D−∈D− 1∶rwhere each D− has a highly peaked token similarity greater than 0.8 but others close to zero (i.e., there exists q⊤ id− j>0.8where Aij=1while other q⊤ id− j→0). Since the sum-of-max operator only cares about the document-level scores, the cross entropy loss would be close to zero during training.3 However, for each of nquery tokens, if there exists at least one negative document token that has a high token simi- larity greater than 0.8, the token retrieval with top- k′=1 would fail to retrieve any tokens of D+. As a result, multi- vector retrieval model with the sum-of-max operator will not be able to lower the high scores of some negative to- kens. Figure 2 shows that the sum-of-max training causes many document tokens to have unreasonably high scores regardless of their actual relevance to the query tokens. 3.1 In-Batch Token Retrieval To train multi-vector retrieval models to directly retrieve tokens of relevant documents, we simulate the token retrieval stage during training. This can be simply achieved by employing a different alignment strategy ˆA. Speciﬁcally, we set the alignment ˆAij= the top- k operator is applied over 1≤j′≤mB (i.e., tokens from Bmini-batch documents) returning the indices of klargest values. During training, we use a hyperparameter ktrainfor the top- koperator. Then, we simply modify eq. (1) as follows: fXTR(Q, D)=1 Zn = i=1max 1≤j≤mˆAijq⊤ idj. (2) The intuition is that we consider the token similarities within Donly when they are high enough to be retrieved within top- ktrainfrom a mini-batch. Here, we use a normalizer Z=∣{i∣∃j, s.t.ˆAij>0}∣, which is essentially the number of query tokens that retrieved at least one document token of D.4If allˆAij=0, we clip Zto a small number and fXTR(Q, D)becomes 0. As a result, our model cannot assign a high token similarity to negative documents as it blocks tokens of positive documents to be retrieved. With the previous failure case where fColBERT assigned a high score on D+even though it cannot be retrieved, our similarity function incurs a high loss as fXTR(Q, D+)=0during training (since tokens of D+were not retrieved). For training, we use the same cross entropy loss deﬁned in §2.1 with our new scoring function. Note that the training data only contains document-level annotations, but XTR encourages important tokens from positive documents to be retrieved. 3.2 Scoring Documents using Retrieved Tokens During inference, multi-vector retrieval models ﬁrst have a set of candidate documents ˆD1∶Cfrom the token retrieval stage: ∗)}. (3) 3Indeed, our derivative analysis in Appendix A shows that the token-level similarity would not change if the document-level scores are already well discriminated. 4We tried different normalizers such as nand found that Zworks the best while stabilizing the training. 4 ≤≤ 3: Comparison of fColBERT in eq. (1) and fXTR′in eq. (4). Assume that DaandDbwere selected as initial candidate documents from the token retrieval stage. fColBERT loads all token vectors ofDaandDband exhaustively recomputes pairwise token similarity to obtain the max values ( red boxes). On the other hand, fXTR′does not load any token vectors and reuses retrieval scores from the ﬁrst-stage token retrieval. Assume that, with the top-2 token retrieval results, the ﬁrst query token retrieved each max score of DaandDb, but the second query token retrieved two tokens only from Dabut not Db. We impute the missing similarity mforDb(denoted as yellow dashed box) by ﬁnding its upper bound using the top-2 score (denoted as s2) of the second query token (i.e., m≤s2≤s1). Scoring Estimated FLOPs/query Setting fColBERT n2k′(2 ¯md+¯m+1) , fXTR′ n2k′(¯r+1) Table 1: FLOPs comparison of ColBERT and XTR for the scoring stage. XTR only adds minimal complexity for scoring each candidate document. The setting is derived from MS MARCO. Here, top- k′(q∗)is a union of top- k′document tokens (from the entire corpus) based on the inner product scores with each query vector (i.e., q⊤d). Given the nquery token vectors, there are C (≤nk′) candidate documents. Previous methods load the entire token vectors of each document and compute eq. (1) for every query and candidate document pair, which takes per query ( ¯m= average document length). Instead, we propose to score the documents solely using the retrieved token similarity . This signiﬁcantly reduces the computational cost for the scoring stage since re-using the token retrieval scores removes computing redundant inner products and unnecessary (non-max) inner products. Furthermore, the expensive gathering stage (which requires loading all the document token vectors for computing eq. (1)) can be removed completely. Unlike previous work [Macdonald and Tonellotto, 2021] that leverages token retrieval to sort ﬁrst-stage candidate documents before the scoring stage, we aim to directly provide the ﬁnal scores of documents. Missing similarity imputation During inference, we retrieve k′document tokens for each of n query tokens. Assume that each document token belongs to a unique document, providing C=nk′ candidate documents in total. This leaves us with a single token similarity to score each document in the absence of the gathering stage. However, during with eq. (1) or eq. (2)—each positive document has up to n(max) token similarities to average, which mostly converges to nas training proceeds. Hence, during inference, we impute the missing similarity for each query token treating each of candidate documents as if it were positive with ntoken similarities. For every candidate document ˆD, we ﬁrst deﬁne the following scoring function for the inference: fXTR′(Q,ˆD)=1 nn = i=1max 1≤j≤mˆAijq⊤ idj+(1−ˆAij)mi. (4) This is similar to eq. (2), but introduces mi∈R, which estimates the missing similarity for each qi. ˆAis deﬁned similar to the one described in eq. (2) except that it uses k′for the top- koperator. Each qiwould take the missing similarity mias the maximum value if ˆAi∗=0andmi≥0. Importantly, fXTR′removes the need of recomputing any q⊤ idjsince when ˆAij=1we already know the retrieval score from the token retrieval stage, and when ˆAij=0we simply don’t need to compute it as ˆAijq⊤ idj=0. Note that when every ˆAij=1, the equation becomes the sum-of-max operator. On the other hand, when no document tokens of ˆDwere retrieved for qi(i.e.,ˆAi∗=0), we fall back to the imputed score mi, which provides an approximated sum-of-max result. 5 MS AR TO FE CF SF CV NF NQ HQ FQ SD DB QU Avg. One Retriever per Domain GenQ 40.8 49.3 18.2 66.9 17.5 64.4 61.9 31.9 35.8 53.4 30.8 14.3 32.8 83.0 43.1 PTR retriever - 58.8 25.6 76.2 23.5 63.8 70.2 33.7 45.6 61.7 43.0 18.3 34.4 87.5 49.4 One Retriever for All BM25 22.8 31.5 36.7 75.3 21.3 66.5 65.6 32.5 32.9 60.3 23.6 15.8 31.3 78.9 44.0 ColBERT 40.1 23.3 20.2 77.1 18.4 67.1 67.7 30.5 52.4 59.3 31.7 14.5 39.2 85.4 45.1 GTR base 42.0 51.1 21.5 66.0 24.1 60.0 53.9 30.8 49.5 53.5 34.9 14.9 39.2 88.1 45.2 T5-ColBERT base 45.6 28.8 31.1 72.4 18.1 70.4 68.3 34.0 52.2 61.7 33.4 14.1 41.6 82.3 46.8 XTR base 45.0 40.7 31.3 73.7 20.7 71.0 73.6 34.0 53.0 64.7 34.7 14.5 40.9 86.1 49.1 Splade v2♣♦43.3 47.9 27.2 78.6 23.5 69.3 71.0 33.4 52.1 68.4 33.6 15.8 43.5 83.8 49.9 ColBERT v2♣♦- 46.3 26.3 78.5 17.6 69.3 73.8 33.8 56.2 66.7 35.6 15.4 44.6 85.2 49.9 GTR xxl 44.2 54.0 23.3 74.0 26.7 66.2 50.1 34.2 56.8 59.9 46.7 16.1 40.8 89.2 49.1 T5-ColBERT xxl 47.3 33.8 31.0 74.2 19.7 73.1 75.8 35.2 60.5 65.2 43.5 17.1 45.0 86.0 50.8 XTR xxl 46.6 44.2 30.9 77.0 24.5 74.3 78.9 35.3 60.9 66.2 43.8 17.1 44.3 88.1 52.7 LoTTE Search LoTTE Forum Writing Rec. Sci. Tech. Life. Pooled Writing Rec. Sci. Tech. Life. Pooled BM25 60.3 56.5 32.7 41.8 63.8 48.3 64.0 55.4 37.1 39.4 60.6 47.2 ColBERT 74.7 68.5 53.6 61.9 80.2 67.3 71.0 65.6 41.8 48.5 73.0 58.2 GTR base 74.1 65.7 49.8 58.1 82.0 65.0 69.2 62.0 33.7 47.6 72.2 54.9 XTR base 77.0 69.4 54.9 63.2 82.1 69.0 73.9 68.7 42.2 51.9 74.4 60.1 Splade v2♣♦77.1 69.0 55.4 62.4 82.3 68.9 73.0 67.1 43.7 50.8 74.0 60.1 ColBERT v2♣♦80.1 72.3 56.7 66.1 84.7 71.6 76.3 70.8 46.1 53.6 76.9 63.4 GTR xxl 83.9 78.0 60.0 69.5 87.4 76.0 79.5 73.5 43.1 62.6 81.9 66.9 XTR xxl 83.3 79.3 60.8 73.7 89.1 77.3 83.4 78.4 51.8 64.5 83.9 71.2 ♣: cross-encoder distillation ♦: model-based hard negatives Table 2: (top) nDCG@10 on MS MARCO (in-domain) and BEIR (zero-shot). The last column shows the average over 13 BEIR datasets. (bottom) Top-5 retrieval accuracy on LoTTE datasets (zero-shot). In fact, we can ﬁnd the upper bound of the missing similarity. For every token retrieval with qi, the missing similarity of the query token for ˆDwill be upper bounded by its last top- k′score. Speciﬁcally, for each query token qi, we have the following top- k′token similarity during inference: [q⊤ id(1), . . .q⊤ id(k′)]. Here, each d(∗)could come from a different document. Since the missing similarity would have a score less than equal to the score of the last retrieved token, we know that mi≤q⊤ id(k′). With a larger k′, the upper bound becomes tighter. In our experiments, we show that simply choosing mi=q⊤ id(k′)works well especially when a model is trained with fXTR.5While we also tried more complicated imputation methods based on regression, our method was competitive enough despite its simplicity. The imputation process is illustrated in Figure 3. Table 1 shows the estimated FLOPs of ColBERT and XTR (see Appendix B for more details). Due to the differences in hardware and infrastructure, we mainly compared the theoretical FLOPs. XTR reduces the FLOPs at the scoring stage by 4000 ×making multi-vector retrieval more efﬁcient. 4 Experiments Experimental Setting Following Ni et al. [2021], we ﬁne-tune XTR on MS MARCO with a ﬁxed set of hard negatives from RocketQA [Qu et al., 2021]. Then, we test XTR on MS MARCO (MS; in-domain) and zero-shot IR datasets. For the zero-shot evaluation, we use 13 datasets from BEIR [Thakur et al., 2021] (see Appendix C for acronyms), 12 datasets from LoTTE [Santhanam et al., 2022b], and 4 datasets on open-domain QA passage retrieval (EQ: [Sciavolino et al., 2021], NQ, TQA: TriviaQA, SQD: SQuAD). We also train multilingual XTR (mXTR) and evaluate it on MIRACL [Zhang et al., 2022b], which contains retrieval tasks in 18 languages. The performance gap between T5-ColBERT [Qian et al., 2022] and XTR shows the improvement with our methods on a multi-vector retrieval model. For implementation details and baselines, see Appendix C. For the relationship between (e.g., ktrainandk′), see §5.3. 5We found that directly training with fXTR′instead of fXTRfails to converge, which we leave as future work. 6 EQ NQ TQA SQD Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 BM25♣71.4 80.0 62.9 78.3 76.4 83.2 71.1 81.8 DPR multi+ BM25♣73.3 82.6 82.6 88.6 82.6 86.5 75.1 84.4 ART MS MARCO♦75.3 81.9 - - 78.0 84.1 68.4 80.4 GTR base♦73.3 80.6 78.5 86.5 76.2 83.4 65.9 77.6 GTR xxl♦75.3 82.5 83.5 89.8 81.7 86.6 70.4 80.6 DPR multi 56.7 70.0 79.5 86.1 78.9 84.8 52.0 67.7 ColBERT - - 79.1 - 80.3 - 76.5 - XTR base 79.0 85.2 79.3 88.1 80.3 85.5 78.2 85.9 XTR xxl 79.4 85.9 84.9 90.5 83.3 87.1 81.1 87.6 ♣: sparse component ♦: retrieval pre-training Table 3: Zero-shot passage retrieval accuracy on open-domain question answering datasets. In-domain performances are underlined and all the other performances are based on the zero-shot evaluation. For , we report macro-averaged performances over different relations. ar bn en es fa ﬁ fr hi id ja ko ru sw te th zh de yo Avg. BM25 48.1 50.8 35.1 31.9 33.3 55.1 18.3 45.8 44.9 36.9 41.9 33.4 38.3 49.4 48.4 18.0 - - - mDPR 49.9 44.3 39.4 47.8 48.0 47.2 43.5 38.3 27.2 43.9 41.9 40.7 29.9 35.6 35.8 51.2 - - - BM25 + mDPR 67.3 65.4 54.9 64.1 59.4 67.2 52.3 61.6 44.3 57.6 60.9 53.2 44.6 60.2 59.9 52.6 - - - Trained on English MS MARCO mContriever (en) 55.3 54.2 37.9 34.1 42.6 51.2 31.5 40.6 36.8 38.3 46.2 39.9 44.4 48.7 52.4 27.4 32.9 32.9 41.5 mXTR base(en) 66.1 64.7 49.4 40.5 47.9 62.2 37.5 51.4 46.9 56.8 64.0 49.8 43.0 67.7 69.2 47.2 34.5 40.6 52.2 mXTR xxl(en) 74.1 75.5 56.0 52.4 56.1 75.1 51.4 61.8 52.0 68.7 67.4 61.3 69.7 76.0 76.9 56.9 51.7 60.3 63.5 Trained on English MS MARCO + MIRACL (16 languages) mContriever 64.6 66.4 41.2 40.3 46.3 61.9 42.9 41.9 44.6 55.6 55.4 48.1 65.3 77.6 69.3 45.9 39.6 41.9 52.7 mXTR base 73.0 73.9 46.1 42.6 51.0 70.5 39.3 51.3 54.2 62.3 67.7 54.5 69.7 80.7 76.1 51.4 36.1 46.8 58.2 mXTR xxl 77.8 78.4 52.5 48.9 56.0 76.0 52.9 61.5 54.9 73.4 68.5 66.2 79.4 84.3 80.7 58.9 52.8 62.4 65.9 Table 4: nDCG@10 on 18 multilingual retrieval tasks from MIRACL. Each row shows the perfor- mance of a single multilingual retrieval model. The last two surprise languages ( deandyo) are not included in the training dataset of MIRACL. The last column shows the average over 18 languages. 4.1 In-domain Document Retrieval MS MARCO The ﬁrst column of Table 2 (top) shows nDCG@10 on MS MARCO (see Table D.1 for recall@100). XTR outperforms most models and remains competitive with T5-ColBERT. This is encouraging since XTR signiﬁcantly reduces the cost of the stage. Note that MS MARCO may fail to reﬂect the actual improvement of [Arabzadeh et al., 2022]. 4.2 Zero-shot Document Retrieval BEIR & LoTTE Table 2 (top; except the ﬁrst columns) shows nDCG@10 on BEIR (see Table D.1 for recall@100). XTR xxlachieves the new performances signiﬁcantly outperforming both per-domain models and single model . Simply scaling XTR removes the needs of designing distillation or hard negative mining pipelines [Santhanam et al., 2022b, Formal et al., 2021]. Results on LoTTE (Table 2 bottom) also show that XTR baseis better than ColBERT and competitive with models while XTR xxladvances the . Passage retrieval for open-domain QA Table 3 shows results on four open-domain QA datasets. While previous work often includes sparse retrievers (e.g., BM25) [Chen et al., 2021] or contrastive pre-training [Ram et al., 2022, Sachan et al., 2022a,b] to achieve better performances on EntityQues- tions, XTR simply ﬁne-tuned on MS MARCO achieves the performance. 4.3 Multilingual Document Retrieval MIRACL Since XTR does not need any secondary pre-training, we expect it to be better at multilingual retrieval by better utilizing the multilingual language models. We train a multilingual version of XTR with mT5 [Xue et al., 2021] and test it on multilingual retrieval tasks in 18 languages. Table 4 shows that mXTR greatly outperforms mContriever that uses expensive contrastive pre- training, as well as the hybrid model, BM25 + mDPR. 7 0 200 400 600 800 1000 T oken Rank oken@k)MS MARCO T5-ColBERT-xxl XTR-xxl 0 200 400 600 800 1000 T oken Rank T5-ColBERT-xxl XTR-xxl 0 200 400 600 800 1000 T oken Rank oken@k)ArguAna T5-ColBERT-xxl XTR-xxl 0 200 400 600 800 1000 T oken Rank oken@k)MS MARCO T5-ColBERT-xxl XTR-xxl 0 200 400 600 800 1000 T oken Rank T5-ColBERT-xxl XTR-xxl 0 200 400 600 800 1000 T oken Rank oken@k)ArguAna T5-ColBERT-xxl XTR-xxlFigure 4: (top) Gold token retrieval performances of T5-ColBERT and XTR. We plot the probability of each retrieved document token at rank kcoming from the gold document. (bottom) Lexical token retrieval performances of T5-ColBERT and XTR. We plot the probability of each retrieved document token at rank kbeing lexically identical to its query token. Model Imputation MRR@10 R@1000 T5-ColBERT base None 0.0 0.0 top-k′score 27.7 91.8 XTR base None 22.6 88.7 mi=0 36.2 97.3 mi=0.2 36.4 97.3 top-k′score 37.4 98.0 Table 5: Impact of training objectives and imputation methods comparing T5-ColBERT and XTR . For both models, we apply fXTR′during inference. We report MRR@10 and Recall@1000 on the MS MARCO development set. 102103104105 k MS MARCO (f_ColBERT) (f_XTR ) XTR-baseFigure 5: Recall@100 of XTR and T5-ColBERT with different k′. For T5-ColBERT, we use ei- . 5 Analysis 5.1 Towards Better Token Retrieval Gold token retrieval If the tokens of gold documents are not retrieved at all, multi-vector retrieval models would fail to retrieve the gold documents. Hence, a better token retrieval would contain these gold tokens more often in their top results. In Figure 4 (top), we show the probability of a token at the rank kcoming from the gold documents of a query. To compute the probability for the rank k, we simply count the number of an event where a token at rank kbelongs to the gold document and divide it by the number of tokens at rank k. While this is measuring the precision of the token retrieval, we observed a similar trend for the recall of gold tokens. Compared to T5-ColBERT, XTR retrieves gold tokens with higher probability, even on MS MARCO. This shows that the training objective of XTR encourages it to retrieve tokens from more relevant context. Lexical token retrieval In Figure 4 (bottom), we show the probability of a token at the rank k being the same as its query token (e.g., ‘ insulin ’ retrieving ‘ insulin ’s). T5-ColBERT has very high probability of retrieving the same token across different ranks and datasets. However, it is unclear to what extent the token retrieval stage should behave as sparse retrieval, as it might suffer from the vocabulary mismatch problem. XTR effectively lowers the reliance on the lexical matching while preserving a good amount of lexical precision so that it would achieve a high retrieval accuracy on the entity-centric dataset (§4.2). In fact, Table 6 in Appendix shows that having lower lexical matching doesn’t necessarily mean a lower retrieval quality, but often means better . 5.2 Efﬁcient Scoring In Table 5, we show how we can employ the efﬁcient scoring function fXTR′inXTR with minimal performance losses. We apply fXTR′on both T5-ColBERT and XTR, and show their performances on MS MARCO. With T5-ColBERT, even if we use the top- k′score for the imputation, the performance 8  k MS MARCO XTR-base (k_train = 32) XTR-base (k_train = 64) XTR-base (k_train = 128) XTR-base (k_train = 256) XTR-base (k_train = 320) (f_XTR ) Figure 6: MRR@10 of XTR with different ktrainandk′. For T5-ColBERT, we also use fXTR′ with the top- k′score imputa- tion method for the inference. 0 50 100 150 200 250 300 MS MARCO XTR-base (batch_size = 128) XTR-base (batch_size = 256) XTR-base (batch_size = 320) 0 50 100 150 200 250 300 ArguAna XTR-base (batch_size = 128) XTR-base (batch_size = 256) XTR-base (batch_size = 320)Figure 7: Effect of training XTR with different batch sizes andktrain. For each point of the graph, we train XTR basewith the speciﬁed training batch size (128, 256, 320) and ktrain(32, 64, 128, 256) and evaluate on each dataset (MS MARCO and ArguAna). nDCG@10 of each model is reported. is much worse than the original sum-of-max scoring. With XTR , the performance greatly improves as it has better token retrieval. Figure 5 shows how Recall@100 improves with larger k′’s as it provides more exact upper bound for the missing similarity imputation. Table D.2 shows that even if we use smaller k′, XTR still maintains high performances on BEIR. 5.3 Relationship between ktrainvs.k′In Figure 6, we show MRR@10 of XTR trained with different ktrainand evaluated with different k′on the MS MARCO development set. While all variants of XTR prefer larger k′, ones trained with smaller ktrainshow higher performances than others under small k′settings. XTR with larger ktrainexhibits better performances than ones with smaller larger. Training batch size vs. ktrain In Figure 7, we show the relationship between the training batch size and ktrainduring training XTR . In this experiment, we use k′=40,000. While it is evident that XTR mostly favors large training batch sizes, the optimal top- ktraincan be different for different datasets. While most datasets including MS MARCO favored a large enough ktrain, ArguAna prefers smaller ktrain. We hypothesize that this is due to the longer query length in ArguAna, which makes multi-vector models fall short compared to dual-encoders (see GTR vs. T5-ColBERT in Table 2). 5.4 Qualitative Analysis Table 6 shows a prediction sample from MS MARCO. For T5-ColBERT, all of the top retrieved tokens are exact lexical matches. Surprisingly, none of the retrieved passages are about the query, demonstrating T5-ColBERT’s failure to retrieve tokens from the correct context. In contrast, XTR retrieves fewer exact lexically matching tokens, but the contexts of the retrieved tokens are much more related to the query. This example explains the lower lexical token retrieval probability of XTR compared to T5-ColBERT in Figure 4 (bottom), but higher gold token retrieval performance in Figure 4 (top). For more qualitative examples, please see Appendix E. 6 Related Work One of the main limitations of dense retrieval models is that encoding the query and document into a single vector constrains the power of the models. Polyencoder [Humeau et al., 2020], MEBERT [Luan et al., 2021], and MVR [Zhang et al., 2022a] propose to use multiple embeddings, instead of one, to represent the query or the document. A more recent approach is token-level multi-vector retrieval, which stores and retrieves with every token embedding. ColBERT [Khattab and Zaharia, 2020] is probably the most renowned model in this family. ALIGNER (i.e. T5- ColBERT) [Qian et al., 2022] extends ColBERT by scaling up the backbone langauge model and studying various strategies for aggregating the token-level alignment scores. These token-level retrieval models show strong effectiveness and out-of-domain generalization ability. Efforts for reducing serving costs of multi-vector models have been mostly focused on the token-level retrieval stage. COIL [Gao et al., 2021] accelerates token-level retrieval by conﬁning retrieval within exact match tokens, sharing the spirit of classic inverted indexing. CITADEL [Li et al., 2022] relaxes COIL with a lexical routing mechanism where a query token vector only retrieves from a subset of 9 T5-ColBERT token retrieval for “what is the usual pay for stock associates at michael? ” Rank Token Context of Token Relevance 1 usual routine passport services: the usual waiting time in logan to get your passport is four (4) to eight (8) weeks for routine 2 usual theusual pay days are the 1st and 16th of each month. for annual educational there is no payroll lag.No 5 usual theusual part xiii tax rate is 25% (unless a tax treaty between canada and your home country reduces the rate).No 50 usual this is where one can challenge the judgment debtor’s claim. one option creditors have is to try and make a deal with the debtor to take less than 25% (the usual amount of a wage levy).No 100 usual theusual maximum inventory is 1 talisman, 26 elemental runes, and 26 pure essence. the ingredients must be brought to an opposing altar ... from the runes being crafted.No XTR token retrieval for “what is the usual pay for stock associates at michael? ” Rank Token Context of Token Relevance 1 usual store manager. 1 salary: the usual salary a store manager receives can be anywhere around $52,000 to $115,000 annually.No 2 usual 1 salary: the usual salary a store manager receives can be anywhere around $52,000 to $115,000 annually. 2 bonuses: publix provide bonuses that could reach up to $40,000.No 5 average average salaries for michaels stores stock associate: $9. michaels stores hourly pay trends based on salaries posted anonymously by michaels stores employees.Yes 50 v i think the a vg starting pay is closer to 30k for asst mgr trainees. it is an hourly position until you are fully trained (40 hours per week).No 100 average average macys salaries. the average salary for macys jobs is $32,000. av- erage macys salaries can vary greatly due to company, location, industry, experience and beneﬁts.No Table 6: Token retrieval example from MS MARCO. Among the top 100 retrieved tokens, 100% of T5-ColBERT tokens are lexically identical as the query token usual while only 8%of XTR tokens are lexically identical. XTR retrieves the relevant passage by retrieving average forusual . document token vectors routed to the same key. PLAID [Santhanam et al., 2022a] optimizes the speed of ColBERT by pruning weaker candidates in the earlier stages of retrieval and using better vector quantization. ColBERT-v2 [Santhanam et al., 2022b] further adopts residual with cluster centroids to improve the efﬁciency of ColBERT. On the other hand, how to accelerate the scoring stage remains under-explored. To the best of our knowledge, XTR is the ﬁrst work to simplify the scoring stage and remove the gathering stage in multi-vector retrieval. 7 Conclusion Multi-vector retrieval leverages query and document token for effective information retrieval. In this paper, we propose XTR that simpliﬁes the existing three-stage inference of multi- vector models by improving the initial token retrieval stage. Speciﬁcally, XTR scores documents solely based on the retrieved tokens, which is also optimized during training with in-batch document tokens. As a result, XTR achieves performances on zero-shot information retrieval benchmarks while greatly reducing the FLOPs of the scoring stage. We further show that our objective function indeed encourages better token retrieval, retrieving more tokens from gold documents, whose contexts are better aligned with the query. Limitations In most of our experiments, XTR was trained on MS MARCO, a large-scale retrieval dataset in English. While our experiments were conducted in a fair setting where most baseline models also utilize MS MARCO, future use cases might need to remove its dependency on MS MARCO due to the license or issue. We believe that LLM-based retrieval dataset generation [Dai et al., 2022] would be able to mitigate the problem in the future. 10  We would like to thank the anonymous reviewers for their helpful feedback. We also thank Nicholas Monath, Raphael Hoffmann, Kelvin Guu, Slav Petrov, and others at Google DeepMind for their helpful comments and discussion. References Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles LA Clarke. Shallow pooling for sparse labels. Information Retrieval Journal , 25(4):365–385, 2022. Xilun Chen, Kushal Lakhotia, Barlas O ˘guz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? arXiv preprint , 2021. Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint , 2022. Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. Splade v2: Sparse lexical and expansion model for information retrieval. arXiv preprint , 2021. Luyu Gao, Zhuyun Dai, and Jamie Callan. COIL: revisit exact lexical match in information retrieval with contextualized inverted list. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 , pages 3030–3042. Association for Computational Linguistics, 2021. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning , pages 3887–3896. PMLR, 2020. Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: Architec- tures and pre-training strategies for fast and accurate multi-sentence scoring. In 8th International Conference on Learning , ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research , 2022. Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick Lewis, Ledell Yu Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. Dense passage retrieval for open-domain question answering. ArXiv , abs/2004.04906, 2020. Omar Khattab and Matei Zaharia. Colbert: Efﬁcient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval , pages 39–48, 2020. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6086–6096, Florence, Italy, July 2019. Association for Computational Linguistics. Minghan Li, Sheng-Chieh Lin, Barlas Oguz, Asish Ghoshal, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. Citadel: Conditional token interaction via dynamic lexical routing for efﬁcient and effective multi-vector retrieval. arXiv preprint , 2022. Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. Sparse, Dense, and Attentional for Text Retrieval. Transactions of the Association for Computational Linguistics , 9:329–345, 04 2021. 11 Craig Macdonald and Nicola Tonellotto. On approximate nearest neighbour selection for multi-stage dense retrieval. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management , pages 3318–3322, 2021. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern’andez ’Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers. In Conference on Empirical Methods in Natural Language Processing , 2021. Yujie Qian, Jinhyuk Lee, Sai Meher Karthik Duddu, Zhuyun Dai, Siddhartha Brahma, Iftekhar Naim, Tao Lei, and Vincent Y Zhao. Multi-vector retrieval as sparse alignment. arXiv preprint , 2022. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 5835–5847, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. The Journal of Machine Learning Research , , 2020. Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. Learning to retrieve passages without supervision. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2687–2700, 2022. Parikshit Ram and Alexander G Gray. Maximum inner-product search using cone trees. In Proceed- ings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 931–939, 2012. Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. Improving passage retrieval with zero-shot question generation. arXiv preprint , 2022a. Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer. Questions are all you need to train a dense passage retriever. arXiv preprint , 2022b. Keshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. Plaid: an efﬁcient engine for late interaction retrieval. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management , pages 1747–1756, 2022a. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2: Effective and efﬁcient retrieval via lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 3715–3734, 2022b. Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. Simple entity-centric questions challenge dense retrievers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6138–6148, 2021. Fumin Shen, Wei Liu, Shaoting Zhang, Yang Yang, and Heng Tao Shen. Learning binary codes for maximum inner product search. In Proceedings of the IEEE International Conference on Computer Vision , pages 4148–4156, 2015. Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips). In Advances in Neural Information Processing Systems , pages 2321–2329, 2014. Anshumali Shrivastava and Ping Li. Improved asymmetric locality sensitive hashing (alsh) for maximum inner product search (mips). In Conference on Uncertainty in Artiﬁcial Intelligence , 2015. 12 Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) , 2021. Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. Gpl: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2345–2360, 2022. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. InProceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 483–498, 2021. Wen-tau Yih, Kristina Toutanova, John C. Platt, and Christopher Meek. Learning discriminative projections for text similarity measures. In Conference on Computational Natural Language Learning , 2011. Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang, and Nan Duan. Multi-view document representation learning for open-domain dense retrieval. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , pages 5990–6000. Association for Computational Linguistics, 2022a. Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David , Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. Making a miracl: Multilingual information retrieval across a continuum of languages. arXiv preprint , 2022b. 13 A Derivatives w.r.t. Similarity Scores Sum-of-max Here, we use a cross-entropy loss LCEwith the sum-of-max operator fColBERT and analyze the derivatives with respect to the token similarity scores. LCE=−logexpf(Q, D+) ∑B b=1expf(Q, D b)=−fColBERT(Q, D+)+logB = , D b) (5) fColBERT(Q, D)=1 nn = i=1m = j=1AijPij=1 nn = i=1Piˆj (6) Here, we denote ˆjas the index of the row-wise maximum value, dependent on each i(i.e.,Aij=1). Given the cross-entropy loss with the sum-of-max operator, we compute the gradient with respect to one of the maximum token similarities P+ iˆjfor a positive document D+∈D1∶B: ∂LCE ∂P+ iˆj=−f(Q, D+) ∂P+ iˆj+1 ∑B b=1expf(Q, D b)∂ ∂P+ iˆjB = b=1expf(Q, D b) =−∂ ∂P+ iˆj1 nn = i=1max 1≤j≤mP+ ij+1 ∑B b=1expf(Q, D b)B = b=1∂ ∂P+ iˆjexpf(Q, D b) =−1 n+1 ∑B b=1expf(Q, D b)B = b=1expf(Q, D b)∂f(Q, D b) ∂P+ iˆj =−1 n+1 nexpf(Q, D+) ∑B b=1expf(Q, D b)=−1 n[1−P(D+∣Q, D 1∶B)]. Similarly, the gradient w.r.t. a maximum token similarity P− iˆjfor a negative document D−∈D1∶Bis computed as follows: ∂LCE ∂P− iˆj=−f(Q, D+) ∂P− iˆj+1 ∑B b=1expf(Q, D b)∂ ∂P− iˆjB = b=1expf(Q, D b) =1 nexpf(Q, D−) ∑B b=1expf(Q, D b)=1 nP(D−∣Q, D 1∶B). Hence, the positive token-level score P+ iˆjwill gradually increase until P(D+∣Q, D 1∶B)→1and the negative token-level score P− iˆjwill decrease until P(D−∣Q, D 1∶B)→0. This shows that the token- level scores are trained based on the document-level scores, which might stagnate the token-level scores. For instance, even if P− iˆjis very high—later causing d− ˆjto be retrieved instead of ones from positive documents—it will not be penalized as long as P(D−∣Q, D 1∶B)is low enough. In-batch token retrieval Compared to the sum-of-max operator, our in-batch sum-of-max fXTR considers the max values only when they are retrieved over other negative tokens in the mini-batch. fXTR(Q, D 1∶B)=1 Zn = i=1m = Zn = i=1Pi¯j Here, we denote ¯jas the index of the row-wise maximum value that is also within the mini-batch qi(i.e., satisﬁes both Aij=1andˆAij=1). If there is no such ¯j, we simply use Pi¯j=0. We also use a normalizer Z, which is the number of non-zero Pi¯j. In this analysis, we assume Z>0since if every Pi¯jis zero, the gradient is undeﬁned. 14 The gradient w.r.t. the maximum token similarity P+ i¯j(non-zero) for a positive document D+∈D1∶B is computed as follows: ∂LCE ∂P+ i¯j=−f(Q, D+) ∂P+ i¯j+1 ∑B b=1expf(Q, D b)∂ ∂P+ i¯jB = b=1expf(Q, D b) =−1 Z+1−expf(Q, D+) ∑B b=1expf(Q, D b) =−1 Z+1−P(D+∣Q, D 1∶B). This is a very similar result compared to the sum-of-max operator except that 1) the gradient is deﬁned only when P+ i¯jis non-zero (i.e. retrieved) and 2) it is dependent on Z+, which means that the gradient will be large whenever there is a small number of retrieved tokens from the positive document. If only a handful of tokens are retrieved for D+, our objective function increases P+ i¯j. For negative similarity score P− i¯j, we have the following: ∂LCE ∂P− i¯j=−f(Q, D+) ∂P− i¯j+1 ∑B b=1expf(Q, D b)∂ ∂P− i¯jB = b=1expf(Q, D b) =−1 Z−−expf(Q, D−) ∑B b=1expf(Q, D b) =1 Z−P(D−∣Q, D 1∶B). Again, it is similar to the sum-of-max result, but it depends on Z−. In this case, even when P(D−∣Q, D 1∶B)is low, if there is a small number of retrieved tokens from D−(i.e., small Z−),P− iˆj will be decreased signiﬁcantly. Note that when Z−is large, Z+naturally becomes smaller as they compete for in-batch token retrieval, which causes positive tokens to have higher scores. B Inference Complexity We compare the complexity of ColBERT and XTR during the scoring stage in terms of FLOPs. We do not measure the complexity for the online query encoding and maximum inner product search (MIPS), which have been extensively studied for both dual encoders and multi-vector retrieval [Santhanam et al., 2022a,b, Guo et al., 2020]. For the scoring stage, both ColBERT and XTR have documents. Here, we assume the worst case nk′where each document token comes from a unique document. For each candidate document, ColBERT loads a set of document vectors of ¯mdﬂoating points ( ¯m=average document length) and computes eq. (1) with the query vectors of ndﬂoating points. Computing eq. (1) per candidate document requires 2n¯mdFLOPs for token-level inner products, n¯mfor ﬁnding the row- wise max, and nfor the ﬁnal average. In total, ColBERT requires n2k′(2 ¯md+¯m+1)FLOPs for the scoring stage. Note that this does not include the latency of loading the points onto the memory, which amounts up to 450MB per query when n=16, k′=1000,¯m=55, d=128. On the other hand, XTR ﬁrst imputes the missing similarity, which is simply done by caching the k′-th token retrieval score for each query token. Then, each of nk′candidate documents requires n¯r FLOPs for ﬁnding row-wise max and nfor the average where ¯ris the average number of retrieved tokens per each candidate document. In total, we have . Table 1 shows the estimated FLOPs of the two models. XTR reduces the FLOPs at the scoring stage by 4000 ×making multi-vector retrieval more efﬁcient and practical. 15 C Implementation Details XTR usesktrainfor retrieving in-batch document tokens. Since we retrieve over mini-batches, the size of mini-batch affects the performance for different ktrain, which is shown in §5.3. In our experiments, we tried each batch size and choose the best model based on their performance on the MS MARCO development set. For inference, XTR usesk′for the token retrieval. We use k′=40,000, which is possible due to the efﬁcient scoring stage of XTR .6We analyze the effect of using different k′’s as well as its relationship to ktrainin §5.3. We initialize XTR from the base and xxl versions of the T5 encoder [Raffel et al., 2020] and provide XTR base and XTR xxl. For multilingual XTR, we initialize XTR from mT5 [Xue et al., 2021]. We ﬁne-tune XTR for 50,000 iterations with the learning rate to 1e-3. Up to 256 chips of TPU v3 accelerator were used depending on the size of the model. We use ScaNN [Guo et al., 2020] for the MIPS during the token retrieval stage. For BEIR, we use 13 datasets (AR: ArguAna. TO: Touché-2020. FE: Fever. CF: Climate-Fever. SF: Scifact. CV: TREC-COVID. NF: NFCorpus. NQ: Natural Questions. HQ: HotpotQA. FQ: FiQA-2018. SD: SCIDOCS. DB: DBPedia. QU: Quora). Baselines There are two main paradigms on training retriever models for the out-of-domain eval- uation. The ﬁrst group trains a single retriever for each dataset (or domain) by generating queries for each out-of-domain corpus. Typically, this approach generates Ndatasets to train Nindepen- dent models for Ndifferent domains. For this approaches, we include GenQ [Thakur et al., 2021], GPL [Wang et al., 2022], and Promptagator [Dai et al., 2022]. The second group builds a single trained on a large-scale IR dataset such as MS MARCO—and directly applies it on the out-of-domain corpora and queries. For this one-retriever- for-all approaches, we present results of retrievers including Splade v2[Formal et al., 2021], ColBERT v2[Santhanam et al., 2022b], and GTR xxl[Ni et al., 2021]. We also show the results of T5-ColBERT xxl[Qian et al., 2022], which is a T5-initialized ColBERT model and shares the same backbone LM and training dataset with XTR . Note that T5-ColBERT uses the heavy scoring stage based on the original sum-of-max. All of our baselines, as well as XTR , are trained on English MS MARCO, unless otherwise stated. 6In fact, XTR still two-to-three orders of magnitude cheaper scoring stage than ColBERT with k′=1,000and T5-ColBERT with k′=4,000. 16 D Additional Results In Table D.1, we show Recall@100 on BEIR. MS AR TO FE CF SF CV NF NQ HQ FQ SD DB QU Avg. One Retriever per Domain GenQ 88.4 97.8 45.1 92.8 45.0 89.3 45.6 28.0 86.2 67.3 61.8 33.2 43.1 98.9 64.2 PTR retriever - 98.9 47.5 94.1 53.1 91.8 55.9 30.6 89.8 74.6 76.5 41.6 46.3 99.6 69.2 One Retriever for All BM25 65.8 94.2 53.8 93.1 43.6 90.8 49.8 25.0 76.0 74.0 53.9 35.6 39.8 97.3 63.6 ColBERT 86.5 91.4 43.9 93.4 44.4 87.8 46.4 25.4 91.2 74.8 60.3 34.4 46.1 98.9 64.5 GTR base 89.8 97.4 44.3 92.3 52.2 87.2 41.1 27.5 89.3 67.6 67.0 34.0 41.8 99.6 64.7 T5-ColBERT base 91.8 76.0 49.9 90.4 46.2 91.3 55.4 27.6 90.5 78.3 63.0 34.2 50.5 97.9 65.5 XTR base 91.0 92.1 50.8 92.5 51.6 90.5 57.3 28.0 91.6 80.7 63.5 34.8 52.0 98.9 68.0 GTR xxl 91.6 98.3 46.6 94.7 55.6 90.0 40.7 30.0 94.6 75.2 78.0 36.6 49.4 99.7 68.4 T5-ColBERT xxl 93.3 81.4 50.1 91.7 49.8 94.6 60.3 29.0 95.5 81.6 72.5 38.5 54.6 99.1 69.1 XTR xxl 93.0 95.6 52.7 93.7 56.2 95.0 62.1 30.7 95.8 82.2 73.0 39.4 54.5 99.3 71.6 Table D.1: Recall@100 on MS-MARCO and BEIR. The last column shows the average over 13 BEIR benchmarks. Compared to GTR, T5-ColBERT only marginally improves the recall. On the other hand, XTR greatly improves the recall showing the importance of having a better token retrieval. In Table D.2, we show nDCG@10 and Recall@100 on BEIR with different k′. k′MS AR TO FE CF SF CV NF NQ HQ FQ SD DB QU Avg. nDCG@10 40,000 45.0 40.7 31.3 73.7 20.7 71.0 73.6 34.0 53.0 64.7 34.7 14.5 40.9 86.1 49.1 1,000 43.2 44.6 29.0 72.1 20.4 71.7 67.5 34.2 49.8 61.3 33.0 15.9 37.0 86.3 47.9 Recall@100 40,000 91.0 92.1 50.8 92.5 51.6 90.5 57.3 28.0 91.6 80.7 63.5 34.8 52.0 98.9 68.0 1,000 88.8 96.4 48.0 92.5 53.3 93.1 48.1 28.6 88.8 78.3 62.5 37.0 47.0 99.1 67.1 Table D.2: nDCG@10 and Recall@100 of XTR baseon MS-MARCO and BEIR with different k′. The last column shows the average over 13 BEIR benchmarks. 17 E Qualitative Analysis In Table 6-E.5, we show token retrieval results from T5-ColBERT and XTR. T5-ColBERT token retrieval for “ lauren london age ?” Rank Token Context of Token Relevance 1 la la ura bush laura lane welch bush (born november 4, 1946) is the wife of the 43rd president of the united states, george w. bush.No 2 la is laura branigan dead? laura branigan died on august 26, 2004 at the age of 47.No 5 la laika death in space. laika died within hours from overheating. her body temperature got way too hot for her to survive. the heat in her spacecraft had risen to 40 degrees celsius (104 degrees fahrenheit).No 50 la singer laura branigan dies at 47 singer laura branigan dies at 47. laura branigan, a pop singer best known for her 1982 platinum hit gloria, has died.No 100 la la uren bacall lauren bacall ( born betty joan perske; september 16, 1924 august)No XTR token retrieval for “ lauren london age ?” Rank Token Context of Token Relevance 1 la lauren london birthday, age, family & biography 33 years, 1 month, 23 days old age lauren london will turn 34 on 05 december, 2018.Yes 2 la la uren london current age 33 years old. lauren london height 5 feet 7 inches (1.5 m/ 157 cm) and her weight 119 lbs (54 kg).Yes 5 la until now, lauren taylor’s age is 28 year old and have gemini constellation. count down 363 days will come next birthday of lauren taylor!No 50 la if dwayne johnson, 43, and his longtime girlfriend, lauren hashian, 31, have a baby, would they have a pebble? the furious 7 star and his bae are reportedly expecting their ﬁrst child together.No 100 la laura bush biography after his defeat, bush returned to is oil business andlaura became a housewife, but soon returned to politics to help her father-in-law, george h.w. bush’s presidential campaign in 1980.No Table E.1: Token retrieval example from MS MARCO for the token “la” in the query “lauren london age” . Among the top 100 retrieved tokens, 100% of T5-ColBERT tokens are lexically identical as the query token laand100% of XTR tokens are also lexically identical. However, top retrieved results from XTR contain the correct entity ( Lauren London ) while those from T5-ColBERT are about wrong entities ( Laura Bush ,Laura Branigan , etc.). 18 T5-ColBERT token retrieval for “ temple university student population ?” Rank Token Context of Token Relevance 1 temple about temple university tuition, cost, ﬁnancial aid, scholarships, and admission ratesNo 2 temple overview the application fee at temple university is $55. it is selective, with an acceptance rate of 61.7 percent and an early acceptance rate of 78 percent.No 5 temple the application fee at temple university is $55. it is selective, with an acceptance rate of 61.7 percent and an early acceptance rate of 78 percent.No 50 temple temple university staff accountants earn $52,000 annually, or $25 per hour, which is 14% higher than the national average for all staff accoun- tants at $45,000 annually and 16% lower than the national salary average for all working americansNo 100 temple browse expedia‘s selection and check out the best hotels close to temple university for the world-class spas and restaurants, or snatch up one of the cheap hotel deals near temple universityNo XTR token retrieval for “ temple university student population ?” Rank Token Context of Token Relevance 1 temple by gender, the school has 18,009 male and 19,476 female students. by race/ethnicity, 20,664 white, 4,466 black, and 3,819 asian students are attending at temple university.Yes 2 temple below tables and charts represent the enrollment statistics including school degree, gender, race/ethnicity, and tranfer-in students at the school. attemple university, 37,485 students are enrolled ....Yes 5 temple temple university the big picture: how many students were on campus in fall 2015? of the 28,886 new freshman applicants, 56% were admitted and 31% of the admitted students enrolled at temple university in fall 2015.Yes 50 temple temple university was founded in 1884 by russell conwell, a yale- educated boston lawyer, orator, and ordained baptist ministerNo 100 temple kaiser said temple ‘s endowment fund is low because the university is late to the idea of fundraising.No Table E.2: Token retrieval example from MS MARCO for the token “temple” in the query “temple university student population?” . Among the top 100 retrieved tokens, 100% of T5-ColBERT tokens are lexically identical as the query token temple and100% of XTR tokens are also lexically identical. However, top retrieved results from XTR are of the correct context ( student population ) while those from T5-ColBERT are off-topic (e.g., tuition ,salary , etc.). 19 T5-ColBERT token retrieval for “aireis expressed in some skin tumors ” Rank Token Context of Token Relevance 1 aire acids: structures, properties, and functions (university science books, sausalito, ca, 2000). humans expressing a defective form of the transcrip- tion factor aire (autoimmune regulator) develop multiorgan autoimmune disease.No 2 aire the primary biochemical defect in apeced is unknown. we have isolated a novel gene, aire, encoding for a putative nuclear protein featuring two phd-type zinc-ﬁnger motifs, suggesting its involvement in regulation.No 5 aire control of central and peripheral tolerance by aire. the negative selection of self-reactive thymocytes depends on the expression of tissue-speciﬁc antigens by medullary thymic epithelial cells.No 50 aire we found that a human patient and mice with defects in aire develop similar lung pathology, demonstrating that the aire-deﬁcient model of autoimmunity is a suitable translational system in which to unravel fundamental mechanisms of ild 100 air cool airinitiates just downstream of the major sense transcript poly(a) site and terminates either early or extends into the ﬂc promoter region.No XTR token retrieval for “aireis expressed in some skin tumors ” Rank Token Context of Token Relevance 1 aire regulation of aire and gene expression in skin tumor keratinocytes expression of the intermediate ﬁlament protein keratin 17 (k17) is robustly upregulated in inﬂammatory skin diseases and in many tumors....Yes 2 aire the thymic transcription factor autoimmune regulator (aire) prevents autoimmunity in part by promoting expression of tissue-speciﬁc self- antigens, which include many cancer antigens. for example, aire- deﬁcient patients are predisposed to vitiligo, an autoimmune disease of melanocytes that is often triggered by efﬁcacious against melanoma.Yes 5 aire aire regulates negative selection of organ-speciﬁc t cells autoimmune syndrome type 1 is a recessive mendelian disorder resulting from mutations in a novel gene, aire, and is characterized by a spectrum of organ-speciﬁc autoimmune diseases.No 50 aire here we demonstrate a novel role for a cd4+3- inducer cell population, previously linked to development of organized secondary lymphoid structures and maintenance of t cell memory in the functional regulation promiscuous gene expression in the thymus.No 100 air this localization is dependent on the presence of sperm in the spermath- eca. after fertilization, air-2 remains associated with chromosomes during each meiotic division.No Table E.3: Token retrieval example from MS MARCO for the token “aire” in the query “aire is expressed in some skin tumors” . Among the top 100 retrieved tokens, 77% of T5-ColBERT tokens are lexically identical as the query token aire and77% of XTR tokens are also lexically identical. Top retrieved results from XTR are relevant to the query (about cancer ,tumor ,skin , and melanocyte ), while those from T5-ColBERT are off-topic. 20 T5-ColBERT for“women with a higher birth weight are more likely to develop breast cancer later in life ” Rank Token Context of Token Relevance 1 later context exposure to cardiovascular risk factors during childhood and adolescence may be associated with the development of atheroscle- rosis later in life.No 2 later n despite the high incidence of febrile seizures, their contribution to the development of epilepsy later in life has remained 5 later prospectively collected data from two intervention studies in adults with severe malaria were analysed focusing on laboratory features on presentation and their association with a later requirement for rrt.No 50 later they did have a limited amount of proteolytic activity and were able to kill s. aureus. with time, the nuclear envelope ruptured, and dna ﬁlled the cytoplasm presumably for later lytic net productionNo 100 late ﬁnally, we address the need for a careful consideration of potential beneﬁts of bisphosphonate therapy and the risk for osteonecrosis of the jaw, a recently recognized late-toxicity of their use.No XTR for“women with a higher birth weight are more likely to develop breast cancer later in life. ” Rank Token Context of Token Relevance 1 later life course breast cancer risk factors and adult breast density (united kingdom) objective to determine whether risk factors in childhood and early adulthood affect later mammographic breast density.Yes 2 later exposure to cardiovascular risk factors during childhood and ado- lescence may be associated with the development of later in life.No 5 subsequent emerging evidence suggests an association between female prenatal experience and her subsequent risk of developing breast cancer.Yes 50 later our nested case–control study of eh progression included 138 cases, who were diagnosed with eh and then with carcinoma (1970–2003) at least 1 year (median, 6.5 years) later , and 241 controls....No 100 during obesity and being overweight during adulthood have been consis- tently linked to increased risk for development of dementia later in life, especially alzheimer’s disease.No Table E.4: Token retrieval example from Scifact for the token “later” in the query “women with a higher birth weight are more likely to develop breast cancer later in life” . Among the top 100 retrieved tokens, 72% of T5-ColBERT tokens are lexically identical as the query token later while only 33% of XTR tokens are lexically identical. Top retrieved results from XTR can retrieves synonyms ( sebsequent ) from relevant context, while those from T5-ColBERT are off-topic. 21 T5-ColBERT for“venules have a thinner or absent smooth layer compared to arterioles. ” Rank Token Context of Token Relevance 1 thinner platelet cd40l is associated with smaller plaques and thinner caps, while p-selectin is associated with smaller core size. conclusions: blood cell activation is signiﬁcantly associated with changes of the carotid wall.No 2 thin the periosteum is a thin, cellular and ﬁbrous tissue that tightly ad- heres to the outer surface of all but the articulated surface of bone and appears to play a pivotal role in driving fracture pain.No 5 thin scoring showed signiﬁcantly (p<0.0001) higher median 5hmc levels in bcn and dcn than in thin ssm, thick ssm, and cmd.No 50 weak subarachnoid haemorrhage (1 ·43 [1 ·25-1·63]), and stable angina (1·41 [1 ·36-1·46]), and weak est for abdominal aortic aneurysm (1 ·08 [1·00-1·17]).No 100 slight the ucp-2 gene expression was widely detected in the whole body with substantial levels in the wat and with slight levels in the skeletal muscle and bat.No XTR for“venules have a thinner or absent smooth layer compared to arterioles. ” Rank Token Context of Token Relevance 1 thinner platelet cd40l is associated with smaller plaques and thinner caps, while p-selectin is associated with smaller core size. conclusions: blood cell activation is signiﬁcantly associated with changes of the carotid wall.No 2 thin the periosteum is a thin, cellular and ﬁbrous tissue that tightly ad- heres to the outer surface of all but the articulated surface of bone and appears to play a pivotal role in driving fracture pain.No 5 thick in dense ﬁbrotic zones, thick ening of the arterial and venous wall with severe luminal narrowing was present in each patient.No 50 small we assessed vasomotor function of the adipose using of small arterioles isolated from different fat 100 particle context circulating concentration of lipoprotein(a) (lp[a]), a large glycoprotein attached to a low-density particle , may be associated with risk of coronary heart disease (chd) and stroke.No Table E.5: Token retrieval example from Scifact for the token “thinner” in the query “vanules have a thinner or absent smooth later compared to arterioles” . Among the top 100 retrieved tokens, only 1%of T5-ColBERT tokens are lexically identical as the query token thinner and only 1%of XTR tokens are also lexically identical. 22 
scientific document retrieval using multi level aspect based queries 	DORIS-MAE: Scientiﬁc Document Retrieval using Multi-level Aspect-based Queries Jianyou Wang⇤Kaicheng Wang⇤Xiaoyue Wang Prudhviraj Naidu Leon Paturi† Laboratory for Emerging Intelligence University of California, San Diego La Jolla, CA 92093 {jiw101, kaw036, xiw027, prnaidu, lbergen, rpaturi} @ ucsd.edu Abstract In scientiﬁc research, the ability to effectively retrieve relevant documents based on complex, multifaceted queries is critical. Existing evaluation datasets for this task are limited, primarily due to the high cost and effort required to annotate resources that effectively represent complex queries. To address this, we propose a novel task, Scientiﬁc DOcument Retrieval using Multi-level Aspect-based qu Eries (DORIS- MAE), which is designed to handle the complex nature of user queries in scientiﬁc research. We developed a benchmark dataset within the ﬁeld of computer science, consisting of 100 human-authored complex query cases. For each complex query, we assembled a collection of 100 relevant documents and produced annotated relevance scores for ranking them. Recognizing the signiﬁcant labor of expert annotation, we also introduce Anno-GPT, a scalable framework for validating the performance of Large Language Models (LLMs) on expert-level dataset annotation tasks. LLM annotation of the DORIS-MAE dataset resulted in a 500x reduction in cost, without compromising quality. Furthermore, due to the multi-tiered structure of these complex queries, the DORIS-MAE dataset can be extended to over 4,000 sub-query test cases without requiring additional annotation. We evaluated 17 recent retrieval methods on DORIS-MAE, observing notable performance drops compared to traditional datasets. This highlights the need for better approaches to handle complex, multifaceted queries in scientiﬁc research. Our dataset and codebase are available at . 1 Introduction Scientists often have complex questions that require thorough exploration within various parts of their ﬁeld (Figure 1). Finding relevant scientiﬁc literature, one of many challenges in this process, can be especially difﬁcult when dealing with multi-faceted queries. These queries typically encompass numerous interconnected topics and require an information retrieval (IR) system capable of recognizing and responding to this level of complexity. ⇤Equal Contribution †Equal Contribution 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks. Figure 1: Example from the DORIS-MAE dataset. Each query is broken down into aspects and sub-aspects. Aspects are semantically distinct components of the query, and sub-aspects are minimal requirements that can be extracted from the aspects. Information retrieval, especially query-based document retrieval [ 78,3,34], is integral to many applications, from search engines [ 9,13,29] and content [ 58,31,41], to open- domain question answering (QA) [ 82,33,15]. A persistent challenge, however, is the low accuracy in processing complex and multi-intent user queries. Despite advanced search engines using semantic understanding and user behavior data in addition to keyword matching [ 13], these systems still fall short when dealing with complex, multi-intent queries. Neural Information Retrieval (NIR) models [ ] are primarily trained on relatively simple queries [ 57,17,64, 61,87]. Moreover, MacAvaney et al. [49] creates a variety of diagnostic probes, revealing NIR models’ instability when processing textual inputs, potentially because they do not comprehend the deeper semantics of text. These limitations can lead to inadequate performance with more complex queries. While open-domain QA models like ChatGPT [ 11] have shown signiﬁcant capability, they frequently produce incorrect or even fabricated responses [ 4,46,50,39,89], and are prohibitively expensive if directly applied to an entire corpus of scientiﬁc documents. To address these challenges, we introduce a novel task, Scientiﬁc Document Retrieval for Multi-level Aspect-based qu Eries (DORIS-MAE). DORIS-MAE extends query-based and example-based IR paradigms [ 20,44], aiming to give users more control in formulating queries using natural language. In order to advance research in this area, we present the DORIS-MAE dataset, comprising 100 unique complex queries in the computer science domain, paired with ranked pools of relevant CS article abstracts. Each query is organized into a hierarchical structure of , which aids annotation. A distinguishing feature of the DORIS-MAE dataset is its aspect-based hierarchical structure shown in Figure 1. This feature aids automation of the annotation process, expands the test case volume without additional annotation, and opens up exploration into the usefulness of aspect information for retrieval methods. Furthermore, we introduce Anno-GPT, a pipeline for validating Large Language Model (LLM) annotations in a statistically sound manner. Our tests reveal that LLM annotations achieve quality comparable to those produced by human annotators, but with considerable savings in both time and cost. Additionally, the design of our pipeline lends itself to easy adaptation for different domains. In our experiments, we evaluated 17 IR/NIR (Information Information Retrieval) methods using the DORIS-MAE dataset. The methods have worse performance on DORIS-MAE compared to traditional document retrieval datasets, highlighting the complexity of DORIS-MAE, and the need for more sophisticated retrieval methods. Our main contributions are three-fold. First, by formulating the DORIS-MAE task we are shedding a new light on tackling complex, multi-faceted queries during scientiﬁc research. Second, we propose Anno-GPT , a procedure for rigorously evaluating the ability of LLMs to replace human experts for challenging annotation tasks. Third, we demonstrate the value of breaking complex queries down 2 to an Multi -level Aspect -based hierarchical structure, for both annotation accuracy and potential improvements in retrieval methods. 2 Related Work A range of methods have been developed for document retrieval and re-ranking. Classic retrieval methods like TF-IDF [ 67] and BM25 [ 72] utilize keyword matching between queries and documents, and fall short when the necessary key phrases are not known to users. To address this, researchers have applied deep learning techniques to develop NIR models, including RocketQA-v2 [ 63], ColBERT-v2 [66], SimLM [ 75], and SPLADE-v2 [ 23]. These models generate latent vector for queries and documents, which are effective for many retrieval tasks, but may not be able to handle textual inputs that are outside of their training distributions [49]. Other models such as SPECTER [ 18] and aspect-based ASPIRE [ 54] focus on calculating document- level similarity. These models are designed to retrieve given an existing paper as input, which makes them less suited for open-ended queries. Other retrieval models, such as text-embedding models (ada-002 by OpenAI [ 28] and E5-large-v2 [ 74]) and model Sentence-BERT [62], all face similar limitations as NIR methods in that they struggle to simultaneously represent multiple aspects of a query. Despite these challenges, they tend to perform better on our dataset than specialized NIR models for efﬁcient dense passage retrieval, indicating their potential for complex tasks like DORIS-MAE. Large-scale traditional IR datasets [ 57,86,40,66,69] primarily contain simple, web-based queries. Models performing well on these datasets often struggle on DORIS-MAE. In contrast, more special- ized IR datasets [ 56,14,53] require human expert annotation, and consequently are more limited in their scale. Mysore et al. [53]and Chan et al. [14]introduce the concept of "aspect" in document retrieval tasks. These datasets provide pre-deﬁned categories for aspects such as "background" or "method". DORIS-MAE extends this work by allowing for open-ended aspects based on the context of the queries. In Recommendation (NDR) research [ 6,2,55], user queries are descriptions that capture a range of users’ needs. These queries are related to everyday tasks, such as ﬁnding restaurants and entertainment. Progress in LLMs [ 11] and [ 24,45,59,79,65] has made it feasible to leverage LLMs such as ChatGPT for annotating NLP tasks [ 90,21], even outperforming crowd-workers in some cases [ 60,70,27]. However, these tasks do not necessitate domain-speciﬁc knowledge, and the annotations produced may not measure up to expert annotations. Faggioli et al. [22], MacAvaney and Soldaini [48]explored the notion of using LLMs to assist human in relevance judgements, and evaluated on TREC-DL datasets [ 19], which have single-faceted queries. Our work seeks to extend these efforts to new annotation tasks requiring domain expertise, while introducing a separate pipeline stage and a hypothesis testing stage. 3 Dataset Description The DORIS-MAE task uses a dataset of 100 complex, human-written queries, each containing between 95 to 226 words. Each complex query is broken down into a hierarchy of aspects and sub- aspects, with aspects representing signiﬁcant semantic components of the original query, typically a sentence or a few sentences. Sub-aspects further decompose an aspect into simpler, veriﬁable requirements. Both aspects and sub-aspects are generally one sentence long, though their semantic complexity varies. A complex query can have up to 9 aspects, and each aspect can further contain up to 6 sub-aspects. Figure 1 shows an example. For each complex query Q, we created a pool Pof approximately 100 potentially relevant scientiﬁc abstracts to evaluate the re-ranking performance of various methods. Within the context of Qand its P, the collection of aspects and sub-aspects is together denoted as (with slight abuse of notation) Q:={ai}. For any aspect or sub-aspect ai2Q, and any paper abstract pj2P, they form a question pair (ai,pj). We generated a total of 165,144 question pairs from the 100 queries, together denoted as D:={(ai,pj)}. To compute the relevance of a paper pjfor a query Q, we ﬁrst break the query down into its aspects. We then compute the relevance score S(ai,pj), which measures the relevance of paper pjfor aspect 3 ai. The relevance score S(pj|Q)for the query Qis the sum of its aspect relevance scores. An average scoreS(pj|Q) |Q| 1indicates pjis a relevant abstract for Q. S(pj|Q): =X ai2QS(ai,pj) (1) Thus, for each complex query and candidate pool, we provide a complete ranking for the list of abstracts. Additionally, we can choose a combination of aspects within a complex query Qand concatenate the corresponding sentences to form a sub-query q⇢Q, and can calculate the relevance score between a paper abstract pjand a sub-query q. S(pj|q): =X ai2qS(ai,pj) (2) Hence, for any sub-query q⇢Qand the candidate pool PofQ, we can also provide a complete ground-truth ranking order for P. This allows the dataset to extend to over 4000 sub-query test cases at no additional annotation cost. Candidate pool abstracts are taken from a database of 360,000 computer science papers between 2011-2021 sourced from arXiv3. We complemented each arXiv paper with its corresponding citation information by cross-matching it on Semantic Scholar [37]. 3.1 Query Formation The DORIS-MAE task aims to mirror real-world scenarios where a researcher has an incomplete concept for a research project and needs to explore the breadth of existing research to establish a solid starting point. The 100 complex queries in our dataset simulate this scenario. Each query is based on one or more existing research papers. We randomly selected 140 papers from AI, NLP, ML, and CV categories on arXiv. We examined each selected paper’s motivation, background, related work, methodology, and experimental results. Using this information, we reverse- engineered a complex query designed to reﬂect the early thought process of the paper’s authors. Overall, DORIS-MAE contains 80 queries derived from single paper abstracts, with 20 queries from each of AI, NLP, ML and CV . We created an additional 20 composite queries, each integrating ideas from 2-3 abstracts. These composite queries are designed to simulate more and unexplored research ideas than the other 80 queries. 3.2 Decomposing Queries to Aspects In this section, we discuss our process of decomposing queries into a list of aspects and further breaking down aspects into sub-aspects. The guiding principles for determining aspects are as follows: (i)Each aspect must correspond to a prominent and semantically meaningful component in the query (refer to Figure 1 for this correspondence). (ii)Each aspect should contain sufﬁcient context to make sense independently, eliminating potential ambiguities. (iii) Each aspect must be semantically distinct and unrelated to others, ensuring their contents are disjoint and do not overlap. Some aspects may fall into broader facets such as background ,method , orresult as deﬁned in [ 53]. Regarding decomposing aspects to sub-aspects, our criteria are: (i)Each sub-aspect should not contain more information than its parent aspect. (ii)Each sub-aspect should represent a semantic segment of its parent aspect (iii) Different sub-aspects may overlap semantically but each should pose a distinct question. (iv)Each sub-aspect should be as simple as possible. / 4 Given the inherent difﬁculty in recalling the full details of a query while ranking a large candidate pool, the aspect-based hierarchical structure of our complex queries provides a systematic, efﬁcient, and interpretable approach for annotation. This leads to a more precise ranking of the candidate pool. As each aspect corresponds to a speciﬁc part of the original query Q, combining several aspects is equivalent to concatenating their corresponding parts in the query, forming semantically coherent sub-queries q⇢Q. 3.3 Candidate Pooling To create a candidate pool Pof approximately 100 paper abstracts for each query Q, we used a variety of IR and NIR methods, similar to the approach in [ 53]. More speciﬁcally, we utilized popular IR search algorithms TF-IDF [ 67] and BM25 [ 72] at different granularities (i.e., sentence and for each query) to retrieve around 80 paper abstracts. We then employed OpenAI’s text embedding model, ada-002 [ 28], to extract 20 more paper abstracts. We also use citation signals from semantic scholar [ 37] to included any papers that directly cite or are cited by any of the original papers used to create that speciﬁc query. Lastly, to prevent bias towards retrieval methods, we excluded from Pany papers that authors referenced during query Q’s formulation. See Appendix C.6 for a sensitivity analysis of our candidate pool construction procedure. 4 Anno-GPT Framework We propose Anno-GPT, a framework for developing a statistically sound annotation pipeline. We use to annotate all 165,144 question pairs aiis an of a query, and pjis an abstract in the query’s candidate pool. This strategy minimizes human annotation efforts. The key to this approach lies in breaking down complex queries into simpler aspects and sub-aspects, ensuring an objective and manageable annotation task. Without this structured approach, we found that evaluating an abstract’s relevance to a complex query was challenging, due to variability in how partial relevance was assessed. However, the scenario changes signiﬁcantly when dealing with question pairs (ai,pj), which only look at a single aspect or sub- aspect of the query. In such cases, assigning a coarse-scale relevance score between 0-2 becomes feasible, maintaining a reasonable degree of objectivity. The performance of the annotation pipeline may be inﬂuenced by several factors: the procedure for breaking down queries into aspects, the criteria used for scoring query relevance, and the LLM prompt selection [ 24,45,59,79,65]. In order to avoid overﬁtting in this pipeline, our methodology comprises two distinct development and testing stages. The development stage involves optimizing all stages of the pipeline, and using feedback and observed outcomes to iteratively reﬁne this strategy. The testing stage uses a prespeciﬁed hypothesis test. The null hypothesis is that there is no signiﬁcant difference between the agreement levels of ChatGPT and humans, and those among humans themselves. After the pipeline has been optimized, we evaluate it on a test set. We compute the difference between ChatGPT-human agreement and human-human agreement. If this difference is sufﬁciently close to 0, with a small conﬁdence interval, we can use the optimized prompt ⇡to annotate the full dataset. Below we give a summary of the Anno-GPT framework: iConstruct question pairs, D:={(ai,pj)}. iiSelect development set Sdev⇢D, and use human annotators to score Sdev. iiiSelect test set Stest⇢D\ Sdevbased on desired power, and use human annotators to score Stest. ivOptimize prompting strategy ⇡and other on Sdev. Fix ⇡. vEvaluate ﬁxed ⇡onStest. viIf satisfactory performance is achieved on Stest, proceed to use ⇡to annotate the entire dataset D. viiOtherwise, repeat steps iii, iv, v for Sdev Sdev[Stestand new Stest. It is important to note that automated annotation for DORIS-MAE only used a single develop- ment/testing cycle, and therefore did not go into step vii. Therefore, there was no risk of inﬂated estimates of annotation accuracy due to multiple comparisons [ 30,83,1]. If multiple cycles are necessary, then the new Stestmust be sufﬁciently large to avoid these problems. 5 4.1 Annotation Guidelines Our team of annotators consists of three graduate students in computer science, all with at least two years of research experience in NLP, CV, ML, and AI. Annotators, both human and ChatGPT, are asked to score each question pair using a 3-point grading scale (0-2): •Level 0: The abstract is unrelated or does not provide any help to the key components of the aspect or sub-aspect. •Level 1: The abstract answers some (or all) key components (either explicitly or within one natural inferential step), but at least one key component is not answered explicitly. •Level 2: The abstract directly answers all the key components of the aspect or sub-aspect. We decided to include both direct and indirect coverage under Level 1, acknowledging that distin- guishing between these cases could be challenging and potentially subjective. The detailed guidelines for human annotation can be found in Appendix B. 4.2 Optimization of Annotation Pipeline For the development stage, two annotators independently annotated a randomly selected set Sdevof 90 question pairs from the complete set Dof 165,144 pairs. This annotated development set then served as the basis for reﬁning the prompting strategies for ChatGPT. The quality of annotations was evaluated using three metrics: macro F1 score, exact accuracy (agreement), and Spearman’s rank correlation coefﬁcient (Spearman’s ⇢). These metrics measure the agreement level between annotators, and have been used successfully in similar tasks [ 53]. After satisfactory agreement levels were achieved between ChatGPT and human annotations on the development set, we transitioned to the hypothesis testing stage, with all three annotators involved. In this stage, we employed the ﬁxed ﬁnalized prompting strategy, ensuring no overﬁtting or leakage from the test set. Our prompting strategy development involved experimenting with recent methodologies such as few-shot in-context learning (ICL) [ 16,11,80,51,91] and chain of thought (CoT) [ 38,81,77,32]. We found that the CoT approach offered the most robust and optimal results for the task of annotating question pairs (ai,dj)2D. A comprehensive description of the prompt engineering process can be found in Appendix B. 4.3 Annotation Evaluations Hypothesis testing was conducted using a sample of 250 question pairs Stestfrom D, distinct from the development set. The selected pairs were independently annotated by three human annotators. We used bootstrapping to estimate the 95% conﬁdence intervals for the macro F1 score, accuracy (agreement), and Spearman’s ⇢. Though the sampling temperature is ﬁxed at zero, recognizing small randomness introduced by GPU [ 52], the ChatGPT annotations are run twice and the pairwise comparisons with human annotators are averaged across these runs. In addition to these prespeciﬁed analyses, we conducted post-hoc analyses using an adjudication procedure to create a more stable set of human annotations [ 53]. We use majority voting [ 8] between the three human annotators to decide the adjudicated annotation. The results presented in Table 1 show that the rate of ChatGPT-human agreement is within range of human-human agreement. Speciﬁcally, ChatGPT’s performance is comparable to that of human annotators as measured by F1 and exact agreement (accuracy). The average agreement level for ChatGPT is numerically lower than average human agreement level as measured by Spearman’s ⇢. We note that the lowest Spearman’s ⇢among two humans is 46.51%, which is comparable to the average ⇢= 46.61%, suggesting ChatGPT’s performance is still within the range of human-level agreement. All p-values are larger than the ↵=0.05criterion. In a post-hoc analysis, we found that comparing ChatGPT to adjudicated human annotations numeri- cally increased the rate of agreement. This provides qualitative evidence for ChatGPT’s performance relative to that of human experts. We further analyzed instances where ChatGPT’s annotations diverged from those of humans. Interestingly, the nature of these discrepancies was similar to those found between humans, with differences largely revolving around the interpretation of key compo- nents in . For example, ChatGPT occasionally differed from human annotators in 6 determining the importance of a given component. Detailed examples of ChatGPT’s reasoning and a comprehensive error analysis can be found in Appendix B. Table 1: Annotation agreement between humans and ChatGPT. H is human, G is ChatGPT, A is Adjudication, CI is 95% conﬁdence interval. p-values correspond to the null hypothesis that there is no difference between avg. H&H and avg. H&G. Higher p-values indicate less evidence of a difference between ChatGPT and humans. Metrics G&A avg. H&H avg. G&H H&H CI G&H CI p-value F1-score (macro) 64.17 58.33 57.46 (52.33, 63.46) (50.93, 62.79) 0.74 Accuracy 67.40 64.13 62.07 (59.73, 68.80) (57.67, 66.13) 0.41 Spearman’s ⇢ 52.63 54.31 46.61 (46.87, 61.56) (38.67, 54.41) 0.07 4.4 Scalability of Annotations The hypothesis testing results support the use of ChatGPT for annotation. At deployment, the pipeline annotated all 165,144 aspect-paper pairs within a span of 24 hours, at a cost under $150. By contrast, human experts typically require approximately 4 minutes per question pair, resulting in an estimated 11,146 hours to annotate the entire dataset. The deployment resulted in a time reduction by a factor of 500, and a cost reduction by a factor of 1,000, without sacriﬁcing annotation quality. Upon completion of the annotation process, we utilized Equations 1 and 2 to compile the results and compute the ﬁnal rankings for both full-query and sub-query test cases. Anno-GPT could potentially utilize any LLM to replace ChatGPT and can be adapted for other expert-level tasks, given the availability of a small set of domain expert annotations for validation. 5 Retrieval Results This section presents the results of testing 17 models discussed in Section 2 on the DORIS-MAE dataset. When available, we trained the model on our CS corpus and denoted the best version as trained in domain (ID), see full training details in Appendix D.1. To contextualize their performance on DORIS-MAE, we compare the results with these models’ previously reported performances on various IR datasets, including MS MARCO [57], LoTTE [66], NQ [40], and Wiki-QA [86]. 5.1 DORIS-MAE Benchmarking Results In our benchmarking process for DORIS-MAE, we use complex queries as inputs to these models. We employ a variety of metrics common in the IR/NIR literature for the evaluation, including R@5, R@20, R-Precision (RP), NDCGexp 10%, MRR@10, and MAP. For fairness, we adopted an alternative approach for models like RocketQA-v2 [ 63] and ColBERT-v2 [ 66] that were not designed to handle long queries. For these cases, we allow the models to process the input as either a single text string or , and report the maximum performance achieved. Uniquely among the models that we consider, the ASPIRE models () are designed to handle multi-aspect queries. For brevity, we only report the higher number among these two options for models in Table 2. For more detailed results, refer to Appendix C. To better interpret the results, we compare against a random ranking baseline. In general, the models show consistent behavior, with larger and more models (like E5-Large- V2, RocketQA-v2, ada-002, Specter-v2) faring better than the smaller and more specialized ones (like SciBERT [ 5], ColBERT-v2, BM25, TF-IDF). Though the Aspire models were designed for multi-aspect queries, they do not have strong performance on the complex queries in DORIS-MAE. When we compare the DORIS-MAE performance of these models with their reported results on traditional retrieval datasets MS MARCO (in Table 3) and NQ (in Table 4), we observe a signiﬁcant reduction in their performances on DORIS-MAE. We choose metrics for comparison based on what is available in previously published work. The results highlight the challenges posed by DORIS-MAE and suggest gaps in the ability of existing methods to generalize well to complex query retrieval. Finally, in Table 5, we make a comparison with the model performances on specialized retrieval datasets such as CSFCube [ 53], RELISH [ 10], and TRECCOVID [ 73]. The comparison reveals a 7 Table 2: Query level performance on full DORIS-MAE. Standard errors are estimated by bootstrapping. ID means a model is trained in domain. Method R@5 R@20 RP NDCGexp 10%MRR@10 MAP Random 4.41 18.48 16.29 7.31 3.59 19.63 E5-L-v2[74] 16.51 ±2.05 43.77 ±2.14 37.46 ±2.44 25.90 ±2.15 14.85 ±2.73 40.49 ±2.32 RocketQA-v2[63] 15.63 ±1.88 45.41 ±2.43 34.36 ±2.32 30.30 ±2.26 20.87 ±3.12 40.18 ±2.23 ada-002[28] 15.38 ±1.95 42.84 ±2.53 35.81 ±2.67 27.46 ±2.48 19.88 ±3.21 40.37 ±2.55 SimCSE[25] 14.90 ±1.89 42.62 ±2.40 35.27 ±2.34 26.88 ±2.36 21.19 ±3.47 39.02 ±2.35 SPLADE-v2[25] 14.78 ±1.89 40.14 ±2.33 31.65 ±2.38 26.08 ±2.00 17.82 ±2.99 37.23 ±2.26 SPECTER-v2[18] 14.50 ±2.15 43.36 ±2.50 33.41 ±2.33 25.65 ±2.23 17.19 ±2.96 37.12 ±2.10 SPECTER ID 13.32 ±1.76 42.52 ±2.37 31.55 ±2.28 21.27 ±2.03 14.48 ±2.78 36.02 ±2.19 TSAspire[54] 14.26 ±1.80 41.25 ±2.40 33.81 ±2.47 26.63 ±2.05 15.59 ±2.59 37.00 ±2.29 SentBERT[62] 14.09 ±1.88 44.69 ±2.47 33.79 ±2.41 21.88 ±2.07 13.23 ±2.69 37.75 ±2.28 OTAspire[54] 13.34 ±1.56 42.25 ±2.53 33.63 ±2.38 25.52 ±2.29 14.18 ±2.66 36.70 ±2.22 ANCE FirstP[85] 13.21 ±2.02 34.54 ±2.20 30.51 ±2.50 20.30 ±2.02 13.87 ±2.64 34.53 ±2.35 SPLADE-v2[23] 11.80 ±1.86 36.59 ±2.12 29.90 ±2.20 21.35 ±2.12 14.30 ±2.77 33.98 ±2.23 LLAMA[71] 12.74 ±1.82 34.51 ±2.36 28.33 ±2.14 16.65 ±1.68 11.78 ±2.45 31.29 ±1.99 SimLM[75] 12.68 ±1.77 35.67 ±2.49 28.90 ±2.42 18.91 ±1.86 11.29 ±2.44 33.06 ±2.34 BM25[72] 8.47 ±1.80 30.50 ±2.38 21.94 ±2.03 13.23 ±1.97 9.19 ±2.46 25.99 ±1.68 ColBERT-v2[66] 8.45 ±1.46 27.86 ±2.29 22.33 ±2.01 12.57 ±1.71 6.69 ±2.15 25.80 ±1.83 TF-IDF[67] 10.71 ±1.48 29.22 ±2.25 24.79 ±2.06 18.25 ±2.01 12.41 ±2.53 28.77 ±1.81 ERNIE[47] 6.49 ±0.94 22.58 ±1.72 20.18 ±1.82 9.66 ±1.18 3.77 ±1.06 22.71 ±1.65 SciBERT[5] 5.13 ±1.25 17.99 ±1.69 17.13 ±1.88 7.50 ±1.34 3.41 ±1.57 20.34 ±1.64 consistent level of difﬁculty between DORIS-MAE and these completely datasets, indicating that DORIS-MAE presents a similarly challenging retrieval task. Table 3: Comparison with MS MARCO. Stats col- lected from [85, 66, 63, 75, 23]. MS MARCO DORIS Ranking Method MRR@10 MRR@10 ANCE 33.0 13.87 ColBERT-v2 39.7 6.69 RocketQA-v2 41.9 20.87 SimLM 41.1 11.29 SPLADE-v2 36.8 14.30Table 4: Comparison with NQ. Stats collected from [66]. NQ DORIS Ranking Method R@20 R@20 ANCE FirstP 81.9 34.54 BM25 59.1 30.50 RocketQA-v2 83.7 45.41 SimLM 85.2 35.67 Table 5: Comparison with CSFCube, TRECCOVID, and RELISH. Results are from [53, 54]. CSFCube TRECCOVID RELISH DORIS-MAE Ranking Method RP MAP R@20 MAP MAP RP MAP R@20 TSAspire - 40.26 - 26.24 61.29 33.81 37.00 41.25 OTAspire - 40.79 - 30.92 62.57 33.63 36.70 42.25 Specter-v2 18.32 - 52.12 28.24 60.62 33.41 37.12 43.36 TF-IDF 14.59 - 39.69 - - 24.79 28.77 29.22 BM25 13.50 - 42.73 - - 21.94 25.99 30.50 5.2 Additional Experiments Up until now, the hierarchical aspect-based structure that Anno-GPT utilizes has been hidden from all the evaluated models because of inability of existing methods to break down a complex query automatically. Even though these structures are not readily available for real-life retrieval methods, they may still hold value once this query decomposition process can be fully automated. To explore this potential, we conduct an experiment where instead of using the original query, each retrieval 8 method had access to a concatenated string of all aspects within a query, excluding sub-aspects. The results, as illustrated in Table 6, show that Sentence-BERT performs best on four metrics: R@5/20, RP, and MAP. These ﬁndings suggest that the use of aspect information could be potentially beneﬁcial to guide retrieval methods. The hierarchical structure of our dataset can be used to create additional, less complex tasks involving only parts of the query. For instance, by pulling out parts of the query corresponding to 2 Aspects, we are able to generate over 1000 test cases. For this task, we found a signiﬁcant increase in the number of relevant abstracts. After re-evaluating all models for the sub-query DORIS-MAE, we observed model performance consistent with those seen in previous benchmarks, as indicated in Table 7. Comparing Table 7 with Table 2, we observed noticeably higher numbers for metrics such as RP, NDCGexp 10%, MRR and MAP, which are indicators of better model performances on this sub-query task. Since models remain unchanged, this suggest the sub-query task is less challenging than full-query task. This is intuitive since sub-queries are less complex and contain fewer aspects. Overall, the creation of these sub-query tasks underscores the adaptability of our dataset, which could accommodate a range of task complexities under the setting of DORIS-MAE. Table 6: Ranking performance given model access to aspects. Full table is in Appendix C Method R@5 R@20 RP NDCGexp 10%MRR@10 MAP ada-002 14.09 42.23 33.56 26.54 20.20 37.62 SentBERT 17.73 45.34 35.67 25.00 15.52 39.87 RocketQA-v2 13.83 43.81 32.59 27.45 16.08 37.90 Table 7: Ranking performance on sub-query (2 Aspects) task. Full table is in Appendix C Method R@5 R@20 RP NDCGexp 10%MRR@10 MAP ada-002 13.49 40.24 47.35 39.00 24.33 51.67 SentBERT 12.15 36.71 45.08 34.78 20.71 48.96 RocketQA-v2 12.79 39.19 46.47 38.78 23.72 50.81 5.3 Supervised Learning on DORIS-MAE To assess the utility of our dataset for training IR models, we conducted an experiment where we allocated 40 queries for training and the remaining 60 for testing. Using supervised contrastive learning (SCL), we optimized a margin-based triplet loss as presented in Equation 3. Each triplet, represented as (A, P, N ), consisted of a query and two abstracts. The higher-ranked abstract in the training data served as the positive instance (i.e., P), with the other functioning as the negative instance (i.e., N). Given each query’s candidate pool size of  100, we derived multiple triplets. This process yielded 3,000 triplets from the designated 40 training queries. Subsequently, we ﬁne-tuned an E5-L-v2 model, a RoBERTa-based text embedding variant with 355 million parameters, over a single epoch. L(A, P, N ) = max < A, N > ||A|| · ||N|| < A, P > ||A|| · ||P||+m,0! A, P, N 2Rn,m> 0 (3) When evaluating the model on the 60 test queries, we noted a marked improvement across all metrics with the SCL-trained model in contrast to the pre-trained baseline. This comparison can be found in Table 8. These positive outcomes, achieved using supervised contrastive learning on DORIS-MAE, underscore the utility of our train/test split for model ﬁne-tuning. Note that Table 2 reports models performance on the full DORIS-MAE dataset. We also report models performance on our proposed test set of 60 queries in Appendix C.4. 6 Conclusion and Future Work This paper introduces a novel task, Scientiﬁc DOcument Retrieval using Multi-level Aspect-based quEries (DORIS-MAE), aimed at modeling the process of information retrieval in the context of 9 Table 8: Comparison of SCL vs pretrained. Standard errors are estimated by bootstrapping. Method R@5 R@20 RP NDCGexp 10%MRR@10 MAP SCL-trained E5-v2 19.57 ±2.33 52.45 ±3.17 44.47 ±3.11 34.67 ±3.17 23.16 ±4.28 49.15 ±3.14 pretrained E5-v2 14.70 ±1.72 42.38 ±2.59 38.24 ±2.94 26.31 ±2.94 14.53 ±3.69 40.62 ±2.85 scientiﬁc research. We also present a dataset for DORIS-MAE generated using the Anno-GPT framework. The results show room for improvement in the performance of current retrieval methods when dealing with DORIS-MAE. Future studies may explore modiﬁcations to model architectures and training procedures to better address complex, multifaceted queries. An understanding of how noise in aspect annotation affects the overall task is an interesting point for future investigation, as it can shed light on how errors in the automated annotations may affect the ﬁnal candidate pool ranking in DORIS-MAE. The hierarchical structure of complex queries, as exempliﬁed in DORIS-MAE, is an area that warrants further attention. Future work might include the development of more sophisticated automated query breakdown methods, potentially drawing from advances in question decomposition [ 36,92], modelling [42, 43] and semantic parsing [76, 88, 68]. Limitations : DORIS-MAE currently contains queries and abstracts from the computer science domain. Consequently, models trained on this dataset may not generalize well to other disciplines. An extended, multi-domain version of DORIS-MAE is a logical direction for future work. The task of determining aspect relevance is challenging due to the complexity of the abstracts. Improved annotation guidelines and training (for both humans and models) may address this challenge. Finally, while we harnessed LLMs to streamline the annotation, the generation of queries and their aspect decomposition remains manual. We found the development of a reliable, automated query generation pipeline difﬁcult, but anticipate that advances in LLMs might bridge this gap in the near future. 10  
